# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/22_MERFISH_codenet.ipynb (unless otherwise specified).

__all__ = ['code_net', 'conv_net', 'input_from_df', 'net', 'train_metric_net']

# Cell
from ..imports import *
from .file_io import *
from .emitter_io import *
from .utils import *
from .dataset import *
from .plotting import *
from ..engine.noise import estimate_noise_scale
import shutil
from .visualization import *
import torch.nn as nn
import torch.nn.functional as F
from .predict import *

from omegaconf import open_dict
from hydra import compose, initialize
from .merfish_eval import *

from ..engine.point_process import *
from .output_trafo import *
from .matching import *
from ..engine.gmm_loss import *
from ..engine.microscope import add_pos_noise
from sklearn.utils import resample
import wandb

# Cell
class code_net(nn.Module):

    def __init__(self, n_inputs=53, n_outputs=1):
        super(code_net, self).__init__()

        self.layers = nn.Sequential(
          nn.Linear(n_inputs, 256),
          nn.BatchNorm1d(256),
          nn.ReLU(),
          nn.Linear(256, 128),
          nn.BatchNorm1d(128),
          nn.ReLU(),
          nn.Linear(128, 64),
          nn.BatchNorm1d(64),
          nn.ReLU(),
          nn.Linear(64, 32),
          nn.BatchNorm1d(32),
          nn.ReLU(),
          nn.Linear(32, n_outputs)
        )

    def forward(self, x):

        return self.layers(x)

class conv_net(nn.Module):

    def __init__(self, n_metrics=5, n_chs=16, bn=False):
        super(conv_net, self).__init__()

        self.n_chs = n_chs
        self.n_metrics = n_metrics

        self.int_c1 = nn.Linear(3, 10).cuda()
        self.int_c2 = nn.Linear(10, 10).cuda()
        self.int_c3 = nn.Linear(10, 1).cuda()

        if bn:
            self.layers = nn.Sequential(
              nn.Linear(self.n_metrics + self.n_chs, 256),
              nn.BatchNorm1d(128),
              nn.ReLU(),
              nn.Linear(128, 64),
              nn.BatchNorm1d(64),
              nn.ReLU(),
              nn.Linear(64, 32),
              nn.BatchNorm1d(32),
              nn.ReLU(),
              nn.Linear(32, 1)
            )
        else:
            self.layers = nn.Sequential(
              nn.Linear(self.n_metrics + self.n_chs, 256),
              nn.ReLU(),
              nn.Linear(256, 128),
              nn.ReLU(),
              nn.Linear(128, 64),
              nn.ReLU(),
              nn.Linear(64, 32),
              nn.ReLU(),
              nn.Linear(32, 1)
            )

    def forward(self, x):

        intm_ints_bin = x[:, -3*self.n_chs:].reshape(-1, 3, self.n_chs).permute(0,2,1)
        f_int = F.relu(self.int_c1(intm_ints_bin))
        f_int = F.relu(self.int_c2(f_int))
        f_int = F.relu(self.int_c3(f_int)[:,:,0])

        dense_inp = torch.cat([x[:, :self.n_metrics], f_int], 1)
        return self.layers(dense_inp)

# net = code_net().cuda()
net = conv_net(6, 16).cuda()

def input_from_df(df, codebook):

    n_ch = len(codebook[0])
    input_keys = ['prob', 'z', 'x_sig','y_sig','z_sig','int_ratio'] + [f'int_sig_{i}' for i in range(n_ch)]  + [f'int_{i}' for i in range(n_ch)]
    offsets = [0.75, 50, 20., 20., 15.,2.] + n_ch*[1.] + n_ch*[0.]
    scales = [1., 100., 20., 20., 15.,2.] + n_ch*[2.] + n_ch*[5.]
    inp_arr = df[input_keys].values
    inp_arr = (inp_arr - np.array(offsets))/np.array(scales)

    inp_arr = np.concatenate([inp_arr, codebook[df['code_inds'].values]], 1)

    return torch.tensor(inp_arr, dtype=torch.float32).cuda()

# Cell
def train_metric_net(net, model, decode_dl, post_proc, micro, point_process, cfg):

    bce = torch.nn.BCEWithLogitsLoss()
    opt = torch.optim.AdamW(net.parameters(), lr = 4e-3)
    sched = torch.optim.lr_scheduler.StepLR(opt, step_size=100, gamma=0.5)
    test_csv = pd.read_csv(cfg.test_csv)
    codebook, targets = hydra.utils.instantiate(cfg.codebook)
    model.cuda()
    test_csv['int_ratio'] = sel_int_ch(test_csv, codebook)['int_ratio']

    ignores = [int(a) for a in str(cfg.ignore)]
    zero_out = torch.ones(54).cuda()
    zero_out[ignores] = 0

    for i in tqdm(range(cfg.num_iters)):

        with torch.no_grad():

            ret_dict = next(iter(decode_dl))
            x, local_rate, background = ret_dict['x'], ret_dict['local_rate'], ret_dict['background'],
            zcrop, ycrop, xcrop = ret_dict['crop_z'], ret_dict['crop_y'], ret_dict['crop_x']
            background = background * micro.get_ch_mult()
            if cfg.sim.bg_estimation.shuffle_ch:
                background = background.index_select(1, torch.randperm(background.shape[1]).cuda())

            if cfg.genm.microscope.col_shifts_enabled  :
                zcrop, ycrop, xcrop = ret_dict['crop_z'], ret_dict['crop_y'], ret_dict['crop_x']
                zcrop, ycrop, xcrop = zcrop.flatten(), ycrop.flatten(), xcrop.flatten()
                colshift_crop = get_color_shift_inp(micro.color_shifts, micro.col_shifts_yx, ycrop, xcrop, cfg.sim.random_crop.crop_sz)
            else:
                zcrop, ycrop, xcrop, colshift_crop = None, None, None, None

            local_rate *= cfg.rate_fac
            sim_vars = point_process.sample(local_rate[:,0])

            ch_inp = list(micro.get_single_ch_inputs(*sim_vars[:-1], ycrop=ycrop, xcrop=xcrop))
            cond = sim_vars[-1] < len(codebook)
            cb_cool = torch.repeat_interleave(cond, cond * (cfg.genm.exp_type.n_bits - 1) + 1)
            ch_inp[1][cb_cool], ch_inp[2][cb_cool], ch_inp[3][cb_cool] = add_pos_noise([ch_inp[1][cb_cool], ch_inp[2][cb_cool], ch_inp[3][cb_cool]],
                                                                                       [cfg.genm.pos_noise.pos_noise_xy*0.5, cfg.genm.pos_noise.pos_noise_xy*0.5, cfg.genm.pos_noise.pos_noise_z], cfg.genm.exp_type.n_bits)
            xsim = micro(*ch_inp, add_noise=True)

            if cfg.genm.phasing:

                phasing_inp = list(micro.get_single_ch_inputs(*sim_vars[:4], get_phased_ints(sim_vars[4], micro.ch_cols, micro.psf.n_cols) ,sim_vars[5], ycrop=ycrop, xcrop=xcrop))
                phasing_inp[1:4] = add_pos_noise(phasing_inp[1:4], [cfg.genm.pos_noise.pos_noise_xy, cfg.genm.pos_noise.pos_noise_xy, cfg.genm.pos_noise.pos_noise_z], cfg.genm.exp_type.n_bits, rm_mean=False)
                xsim += micro(*phasing_inp, add_noise=True) * cfg.genm.phasing * torch.rand(xsim.shape, device=xsim.device)

            x = micro.noise(xsim, background, const_theta_sim=cfg.genm.exp_type.const_theta_sim).sample()

            colshift_crop = get_color_shift_inp(micro.color_shifts, micro.col_shifts_yx, ycrop, xcrop, cfg.sim.random_crop.crop_sz)
            net_inp = torch.concat([x,colshift_crop], 1)

            gt_vars = sim_vars[:-2]
            gt_df = sample_to_df(*gt_vars, sim_vars[-1], px_size_zyx=[1.,1.,1.])
            gt_df = gt_df[gt_df['code_inds'] < len(codebook)]

            res_dict = model(net_inp.cuda())
            res_dict = model.tensor_to_dict(res_dict)
            pred_df = post_proc.get_df(res_dict)
            pred_df = pred_df[pred_df['code_inds'] < len(codebook)]
            pred_df['gene'] = targets[pred_df['code_inds']]
    #         pred_df = sel_int_ch(pred_df, codebook)

            perf, matches, _ = matching(px_to_nm(gt_df), pred_df, tolerance=500, print_res=False)

            pred_df.loc[:, 'class'] = 1
            pred_df.loc[pred_df['loc_idx'].isin(matches['loc_idx_pred']), 'class'] = 0
            pred_df['int_ratio'] = zero_int_ch(pred_df, codebook)['int_ratio'].values

        opt.zero_grad()

        ''''''
        if cfg.resample:
            df_majority = pred_df[pred_df['class']==0]
            df_minority = pred_df[pred_df['class']==1]

            df_maj_down = resample(df_majority, replace=True, n_samples=len(df_minority)*2)

            pred_df = pd.concat([df_minority, df_maj_down])
        ''''''

        net_inp = input_from_df(pred_df, codebook)
        net_inp *= zero_out[None]
        net_out = net(net_inp)

        net_tar = torch.tensor(pred_df['class'].values, dtype=torch.float32).cuda()
        loss = bce(net_out, net_tar[:,None])

        test_csv['net_score'] = torch.sigmoid(net(input_from_df(test_csv, codebook)).detach().cpu())

        loss.backward()

        opt.step()
        sched.step()

        wandb.log({'loss': loss.item()}, step=i)
        wandb.log({'N_blanks': test_csv.nsmallest(12555, 'net_score')['class'].sum()}, step=i)

        if i % 20 == 0 and cfg.save_file is not None:
            torch.save(net, cfg.save_file)

    wandb.finish()