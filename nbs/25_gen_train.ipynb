{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp funcs.gen_train_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop simulator learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from decode_fish.imports import *\n",
    "from decode_fish.funcs.evaluation import *\n",
    "from decode_fish.funcs.file_io import *\n",
    "from decode_fish.funcs.emitter_io import *\n",
    "from decode_fish.funcs.utils import *\n",
    "from decode_fish.funcs.dataset import *\n",
    "from decode_fish.funcs.output_trafo import *\n",
    "from decode_fish.funcs.plotting import *\n",
    "from decode_fish.funcs.predict import *\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions as D\n",
    "from torch.utils.data import DataLoader\n",
    "import torch_optimizer\n",
    "from decode_fish.engine.microscope import Microscope, get_roi_filt_inds, extract_psf_roi, mic_inp_apply_inds, add_pos_noise, concat_micro_inp\n",
    "from decode_fish.engine.model import UnetDecodeNoBn\n",
    "from decode_fish.engine.point_process import PointProcessUniform, get_phased_ints\n",
    "from decode_fish.engine.gmm_loss import PointProcessGaussian\n",
    "import shutil\n",
    "import wandb\n",
    "import kornia\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from decode_fish.funcs.merfish_eval import *\n",
    "from decode_fish.funcs.exp_specific import *\n",
    "# from decode_fish.funcs.visualization vimport get_simulation_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def gen_train(cfg,\n",
    "          model, \n",
    "          microscope,\n",
    "          post_proc,\n",
    "          dl, \n",
    "          optim_dict):\n",
    "    \n",
    "    save_dir = Path(cfg.output.save_dir)\n",
    "    model.cuda().train()\n",
    "\n",
    "    # Save initial psf state\n",
    "    torch.save(microscope.psf.state_dict(), str(save_dir) + '/psf_init.pkl' )\n",
    "\n",
    "    # Load codebook \n",
    "    if 'codebook' in cfg:\n",
    "        code_ref, targets = hydra.utils.instantiate(cfg.codebook)\n",
    "\n",
    "    # Controls which genmodel parameters are optimized\n",
    "    for name, p in microscope.named_parameters():\n",
    "        p.requires_grad = cfg.training.mic.par_grads[name]\n",
    "\n",
    "    calc_log_p_x = False\n",
    "    \n",
    "    ret_dict = next(iter(dl))\n",
    "    x, local_rate, background = ret_dict['x'], ret_dict['local_rate'], ret_dict['background'] \n",
    "    zcrop, ycrop, xcrop = ret_dict['crop_z'], ret_dict['crop_y'], ret_dict['crop_x']\n",
    "\n",
    "    background = background * microscope.get_ch_mult().detach()\n",
    "    x = x * microscope.get_ch_mult().detach()\n",
    "\n",
    "    colshift_crop = get_color_shift_inp(microscope.color_shifts, microscope.col_shifts_yx, ycrop, xcrop, cfg.sim.random_crop.crop_sz)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        out_col = []\n",
    "        bg_pred = []\n",
    "        for i in range(len(x)):\n",
    "            out_inp = model.tensor_to_dict(model(torch.concat([x[i:i+1], colshift_crop[i:i+1]], 1)))\n",
    "            out_col.append(post_proc.get_micro_inp(out_inp))\n",
    "            bg_pred.append(out_inp['background'])\n",
    "\n",
    "        bg_pred = torch.concat(bg_pred)\n",
    "        proc_out_inp = concat_micro_inp(out_col, batch_size=1)\n",
    "\n",
    "        ch_out_inp = microscope.get_single_ch_inputs(*proc_out_inp, ycrop=ycrop.flatten(), xcrop=xcrop.flatten())\n",
    "        filt_inds = get_roi_filt_inds(*ch_out_inp[0], microscope.psf.psf_volume.shape, x.shape, slice_rec=cfg.genm.exp_type.slice_rec, min_dist=10)\n",
    "        print(len(ch_out_inp[1]), len(filt_inds))\n",
    "        \n",
    "        ch_out_inp = mic_inp_apply_inds(*ch_out_inp, filt_inds)\n",
    "\n",
    "        rois = extract_psf_roi(ch_out_inp[0], x, torch.tensor([len(ch_out_inp[1]), 1,1,21,21]))\n",
    "        bgs = extract_psf_roi(ch_out_inp[0], bg_pred, torch.tensor([len(ch_out_inp[1]), 1,1,21,21]))\n",
    "        \n",
    "        if cfg.training.mic.edge_diff:\n",
    "        \n",
    "            bg_edges = torch.cat([bgs[:,0,0,0,:], bgs[:,0,0,-1,:], bgs[:,0,0,:,0], bgs[:,0,0,:,-1]], 1)\n",
    "            rois_edges = torch.cat([rois[:,0,0,0,:], rois[:,0,0,-1,:], rois[:,0,0,:,0], rois[:,0,0,:,-1]], 1)\n",
    "            edge_diff = rois_edges.mean(-1) - bg_edges.mean(-1)\n",
    "            rois -= edge_diff[:,None,None,None,None]\n",
    "        \n",
    "        ch_inds = ch_out_inp[0][1]\n",
    "        int_vals = ch_out_inp[-2]\n",
    "        \n",
    "        del(model)\n",
    "        del(out_col)\n",
    "        free_mem()\n",
    "\n",
    "    for batch_idx in range(cfg.training.start_iter, cfg.training.num_iters+1):\n",
    "\n",
    "        optim_dict['optim_mic'].zero_grad()\n",
    "        calc_log_p_x = False\n",
    "\n",
    "#         int_means = torch.ones(cfg.genm.exp_type.n_channels).cuda() * (model.int_dist.int_loc.detach() + model.int_dist.int_conc.detach())\n",
    "#         for i in range(cfg.genm.exp_type.n_channels):\n",
    "#             if i in ch_inds:\n",
    "#                 int_means[i] = int_vals[ch_inds == i].mean()\n",
    "\n",
    "#         int_means = (model.int_dist.int_loc.detach() + model.int_dist.int_conc.detach()) / int_means\n",
    "#         ch_fac_loss = torch.sqrt(torch.mean((microscope.channel_facs - microscope.channel_facs.detach() * int_means)**2))\n",
    "\n",
    "        ch_out_inp = microscope.get_single_ch_inputs(*proc_out_inp, ycrop=ycrop.flatten(), xcrop=xcrop.flatten())\n",
    "        ch_out_inp = mic_inp_apply_inds(*ch_out_inp, filt_inds)\n",
    "        \n",
    "        psf_n, psf_recs = microscope(*ch_out_inp, ret_psfs=True, add_noise=False)  \n",
    "        \n",
    "        mean_diff = 0.\n",
    "        if cfg.training.mic.mean_diff:\n",
    "            mean_diff = rois.mean([1,2,3,4], keepdim=True) - (psf_recs.detach()+bgs).mean([1,2,3,4], keepdim=True)\n",
    "\n",
    "        log_p_x_given_z = -microscope.noise(psf_recs, bgs, const_theta_sim=False, ch_inds=ch_out_inp[0][1]).log_prob((rois-mean_diff).clamp_min_(1.))\n",
    "        if cfg.training.mic.psf_loss_mult:\n",
    "            log_p_x_given_z = log_p_x_given_z*psf_n.detach()\n",
    "        log_p_x_given_z = log_p_x_given_z.mean()\n",
    "        calc_log_p_x = True\n",
    "\n",
    "#         log_p_x_given_z += ch_fac_loss\n",
    "\n",
    "        if cfg.training.mic.norm_reg:\n",
    "            log_p_x_given_z += cfg.training.mic.norm_reg * (microscope.psf.com_loss())\n",
    "\n",
    "        if cfg.training.mic.l1_reg:\n",
    "            log_p_x_given_z += cfg.training.mic.l1_reg * (microscope.psf.l1_diff_norm(microscope.psf_init_vol))  \n",
    "\n",
    "        log_p_x_given_z.backward()\n",
    "        if cfg.training.mic.grad_clip:\n",
    "            torch.nn.utils.clip_grad_norm_(microscope.parameters(), max_norm=cfg.training.mic.grad_clip, norm_type=2)\n",
    "\n",
    "        optim_dict['optim_mic'].step()\n",
    "        optim_dict['sched_mic'].step()\n",
    "\n",
    "        # Logging\n",
    "        if batch_idx % cfg.output.log_interval == 0:\n",
    "\n",
    "#             print(batch_idx, log_p_x_given_z)\n",
    "            wandb.log({'AE Losses/p_x_given_z': log_p_x_given_z.detach().cpu()}, step=batch_idx)\n",
    "            wandb.log({'AE Losses/RMSE(rec)': torch.sqrt((((rois-mean_diff)-(psf_recs+bgs))**2).mean()).detach().cpu()}, step=batch_idx)\n",
    "            wandb.log({'AE Losses/sum(psf)': F.relu(microscope.psf.psf_volume)[0].sum().detach().cpu()}, step=batch_idx)\n",
    "            \n",
    "            torch.save(microscope.state_dict(), save_dir/'microscope.pkl')\n",
    "\n",
    "    wandb.finish()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(f'/groups/turaga/home/speisera/Mackebox/Artur/WorkDB/deepstorm/models/fishcod/MERFISH_MOp/sweep_mop_17/phasing:0.0//train.yaml')\n",
    "cfg.run_name = 'gensweep_mop_1'\n",
    "\n",
    "cfg.output.log_interval = 20\n",
    "cfg.training.bs = 100\n",
    "cfg.genm.microscope.norm='none'\n",
    "\n",
    "cfg.training.mic.psf_loss_mult = False\n",
    "cfg.training.net.enabled = False\n",
    "\n",
    "# cfg.training.mic.par_grads.channel_facs = False\n",
    "# cfg.training.mic.par_grads.channel_shifts = False\n",
    "# cfg.training.mic.par_grads.psf_vol = True\n",
    "cfg.training.mic.par_grads.theta_par = False\n",
    "cfg.training.mic.par_grads.z_facs = False\n",
    "cfg.training.mic.edge_diff = False\n",
    "cfg.training.mic.mean_diff = False\n",
    "\n",
    "cfg.data_path.model_init = '/groups/turaga/home/speisera/Mackebox/Artur/WorkDB/deepstorm//models/fishcod/MERFISH_MOp/sweep_mop_12/num_iters:9995//'\n",
    "cfg.training.num_iters = 5000\n",
    "cfg.training.schedule = None\n",
    "cfg.training.mic.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gensweep_mop_1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = f'../config/experiment/{cfg.run_name}.yaml'\n",
    "OmegaConf.save(cfg, fname)\n",
    "prepend_line(fname, '# @package _global_')\n",
    "cfg.run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.training.mic.par_grads.channel_facs = True\n",
    "cfg.training.mic.par_grads.channel_shifts = True\n",
    "cfg.training.mic.par_grads.theta_par = True\n",
    "cfg.training.mic.par_grads.psf_vol = True\n",
    "cfg.training.mic.par_grads.color_shifts = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf, noise, micro = load_psf_noise_micro(cfg)\n",
    "post_proc = hydra.utils.instantiate(cfg.post_proc_isi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop size larger than volume in at least one dimension. Crop size changed to (1, 56, 56)\n",
      "7 volumes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/groups/turaga/home/speisera/anaconda3/envs/decode_fish_dev2/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "img_3d, decode_dl = get_dataloader(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Microscope(\n",
       "  (psf): LinearInterpolatedPSF(\n",
       "    (forward_nonlin): Identity()\n",
       "  )\n",
       "  (noise): sCMOS()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_offset, inp_scale = get_forward_scaling(img_3d[0])\n",
    "# inp_scale = 300\n",
    "# inp_offset = 100\n",
    "model = hydra.utils.instantiate(cfg.network, inp_scale=inp_scale, inp_offset=inp_offset)\n",
    "\n",
    "psf  .to('cuda')\n",
    "model.to('cuda')\n",
    "micro.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "for param in model.parameters():\n",
    "    params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_dict = {}\n",
    "optim_dict['optim_mic'] = hydra.utils.instantiate(cfg.training.mic.opt, params=micro.parameters())\n",
    "optim_dict['sched_mic'] = hydra.utils.instantiate(cfg.training.mic.sched, optimizer=optim_dict['optim_mic'])\n",
    "    \n",
    "save_dir = Path(cfg.output.save_dir)\n",
    "save_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "loaded state dict contains a parameter group that doesn't match the size of optimizer's group",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m train_state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(Path(cfg\u001b[38;5;241m.\u001b[39mdata_path\u001b[38;5;241m.\u001b[39mmodel_init)\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_state.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m optim_dict:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43moptim_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/decode_fish_dev2/lib/python3.8/site-packages/torch/optim/optimizer.py:146\u001b[0m, in \u001b[0;36mOptimizer.load_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    144\u001b[0m saved_lens \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m saved_groups)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(p_len \u001b[38;5;241m!=\u001b[39m s_len \u001b[38;5;28;01mfor\u001b[39;00m p_len, s_len \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(param_lens, saved_lens)):\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded state dict contains a parameter group \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the size of optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms group\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Update the state\u001b[39;00m\n\u001b[1;32m    150\u001b[0m id_map \u001b[38;5;241m=\u001b[39m {old_id: p \u001b[38;5;28;01mfor\u001b[39;00m old_id, p \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    151\u001b[0m           \u001b[38;5;28mzip\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable((g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m saved_groups)),\n\u001b[1;32m    152\u001b[0m               chain\u001b[38;5;241m.\u001b[39mfrom_iterable((g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups)))}\n",
      "\u001b[0;31mValueError\u001b[0m: loaded state dict contains a parameter group that doesn't match the size of optimizer's group"
     ]
    }
   ],
   "source": [
    "# Resume training from a previous checkpoint\n",
    "if cfg.data_path.micro_init is not None:\n",
    "    micro.load_state_dict(torch.load(cfg.data_path.micro_init), strict=False)\n",
    "    \n",
    "# cfg.data_path.model_init = cfg.output.save_dir\n",
    "# cfg.data_path.model_init = '/groups/turaga/home/speisera/Mackebox/Artur/WorkDB/deepstorm//models/fishcod/MERFISH_MOp/sweep_mop_5b/int_loc:0.1xscale:400xpsf_noise:0.0'\n",
    "\n",
    "if cfg.data_path.model_init is not None:\n",
    "    print('loading')\n",
    "    model = load_model_state(model, Path(cfg.data_path.model_init)/'model.pkl').cuda()\n",
    "#     micro.load_state_dict(torch.load(Path(cfg.data_path.model_init)/'microscope.pkl'), strict=False)\n",
    "    \n",
    "    train_state_dict = torch.load(Path(cfg.data_path.model_init)/'training_state.pkl')\n",
    "    for k in optim_dict:\n",
    "        optim_dict[k].load_state_dict(train_state_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_ref, targets = hydra.utils.instantiate(cfg.codebook)\n",
    "post_proc.codebook = torch.tensor(code_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "_ = wandb.init(project=cfg.output.project, \n",
    "               config=OmegaConf.to_container(cfg, resolve=True),\n",
    "               dir=cfg.output.log_dir,\n",
    "               group=cfg.output.group,\n",
    "               name=cfg.run_name,\n",
    "               mode='disabled'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2292 540\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgen_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmicroscope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmicro\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m     \u001b[49m\u001b[43mpost_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m     \u001b[49m\u001b[43moptim_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mgen_train\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    110\u001b[0m         optim_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msched_mic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_interval\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#             print(batch_idx, log_p_x_given_z)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m             wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAE Losses/p_x_given_z\u001b[39m\u001b[38;5;124m'\u001b[39m: log_p_x_given_z\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()}, step\u001b[38;5;241m=\u001b[39mbatch_idx)\n\u001b[1;32m    117\u001b[0m             wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAE Losses/RMSE(rec)\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39msqrt((((rois\u001b[38;5;241m-\u001b[39mmean_diff)\u001b[38;5;241m-\u001b[39m(psf_recs\u001b[38;5;241m+\u001b[39mbgs))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean())\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()}, step\u001b[38;5;241m=\u001b[39mbatch_idx)\n",
      "File \u001b[0;32m~/anaconda3/envs/decode_fish_dev2/lib/python3.8/site-packages/omegaconf/dictconfig.py:351\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m()\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_DEFAULT_MARKER_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigKeyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[1;32m    354\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigAttributeError\n\u001b[1;32m    355\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/decode_fish_dev2/lib/python3.8/site-packages/omegaconf/dictconfig.py:445\u001b[0m, in \u001b[0;36mDictConfig._get_impl\u001b[0;34m(self, key, default_value)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Node)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_with_default\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_value\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/decode_fish_dev2/lib/python3.8/site-packages/omegaconf/basecontainer.py:65\u001b[0m, in \u001b[0;36mBaseContainer._resolve_with_default\u001b[0;34m(self, key, value, default_value)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default_value\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingMandatoryValue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing mandatory value: $FULL_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m resolved_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_resolve_interpolation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_value(resolved_node)\n",
      "File \u001b[0;32m~/anaconda3/envs/decode_fish_dev2/lib/python3.8/site-packages/omegaconf/base.py:611\u001b[0m, in \u001b[0;36mContainer._maybe_resolve_interpolation\u001b[0;34m(self, parent, key, value, throw_on_resolution_failure, memo)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_resolve_interpolation\u001b[39m(\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    605\u001b[0m     parent: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContainer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m     memo: Optional[Set[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    610\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Node]:\n\u001b[0;32m--> 611\u001b[0m     value_kind \u001b[38;5;241m=\u001b[39m \u001b[43mget_value_kind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value_kind \u001b[38;5;241m!=\u001b[39m ValueKind\u001b[38;5;241m.\u001b[39mINTERPOLATION:\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/anaconda3/envs/decode_fish_dev2/lib/python3.8/site-packages/omegaconf/_utils.py:524\u001b[0m, in \u001b[0;36mget_value_kind\u001b[0;34m(value, strict_interpolation_validation)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ValueKind\u001b[38;5;241m.\u001b[39mINTERPOLATION\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mValueKind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVALUE\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen_train(cfg=cfg,\n",
    "     model=model, \n",
    "     microscope=micro, \n",
    "     post_proc=post_proc,\n",
    "     dl=decode_dl, \n",
    "     optim_dict=optim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_fish_dev2]",
   "language": "python",
   "name": "conda-env-decode_fish_dev2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
