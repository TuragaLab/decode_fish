# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/14_train_ae.ipynb (unless otherwise specified).

__all__ = ['train_ae']

# Cell
from ..imports import *
from .file_io import *
from .emitter_io import *
from .utils import *
from .dataset import *
from .output_trafo import *
from .evaluation import *
from .plotting import *
import torch.nn.functional as F
from torch.optim import AdamW
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.data import DataLoader
from ..engine.microscope import Microscope
from ..engine.model import UnetDecodeNoBn
import shutil
from torch.utils.tensorboard import SummaryWriter
from ..engine.point_process import PointProcessUniform
from ..engine.gmm_loss import PointProcessGaussian
from .train_sl import gt_logger, load_from_gt_dict

# Cell
def train_ae(model,
             dl,
             optim_sl,
             optim_ae,
             sched_sl,
             sched_ae,
             min_int,
             microscope,
             log_interval,
             save_dir,
             log_dir,
             psf=None,
             bl_loss_scale = 0.01,
             p_quantile=0,
             grad_clip=0.01,
             uniform_prior_rate = 0.0001):

    save_dir = Path(save_dir)
    if os.path.isdir(log_dir):
        shutil.rmtree(log_dir)
    writer = SummaryWriter(log_dir)

    if gt_dict is not None:
        gt_img, gt_df = load_from_gt_dict(gt_dict)

    model.cuda().train()

    for batch_idx, (x, local_rate, background) in enumerate(dl):

        """GET SUPVERVISED LOSS"""
        locs_sl, x_os_sl, y_os_sl, z_os_sl, ints_sl, output_shape = PointProcessUniform(local_rate, min_int=min_int).sample()
        xsim = microscope(locs_sl, x_os_sl, y_os_sl, z_os_sl, ints_sl, output_shape)
        xsim_noise = microscope.noise(xsim, background).sample()

        res_sim = model(xsim_noise)
        gmm_loss = -PointProcessGaussian(logits = res_sim['logits'],xyzi_mu=res_sim['xyzi_mu'],xyzi_sigma = res_sim['xyzi_sigma']).log_prob(locs_sl,x_os_sl, y_os_sl, z_os_sl, ints_sl, p_quantile).mean()

        background_loss = F.mse_loss(res_sim['background'], background)

        loss = gmm_loss  + bl_loss_scale * background_loss
        """"""
        loss.backward()

        if grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip, norm_type=2)

        optim_sl.step()
        optim_sl.zero_grad()


        res_img = model(x)

#         q_z_given_x = PointProcessGaussian(logits = res_img['logits'], xyzi_mu=res_img['xyzi_mu'], xyzi_sigma = res_img['xyzi_sigma'])
#         locs_ae, x_os_ae, y_os_ae, z_os_ae, ints_ae, output_shape_ae = q_z_given_x.sample()
#         ae_img = microscope(locs_ae, x_os_ae, y_os_ae, z_os_ae, ints_ae, output_shape_ae)


        locs_ae, x_os_ae, y_os_ae, z_os_ae, ints_ae, output_shape_ae = model_output_to_micro_input(res_img, threshold=0.1)
        ae_img = microscope(locs_ae, x_os_ae, y_os_ae, z_os_ae, ints_ae, output_shape_ae)

        log_p_x_given_z = - microscope.noise(ae_img,res_img['background']).log_prob(x).mean()

#         log_q_z_given_x = q_z_given_x.log_prob(locs_ae, x_os_ae, y_os_ae, z_os_ae, ints_ae, p_quantile).mean()
#         log_p_z = prior.log_prob(locs_ae, output_shape=output_shape).mean()
#         elbo_loss = - (log_p_x_given_z - log_q_z_given_x + log_p_z)
#         elbo_loss.backward()

        log_p_x_given_z.backward()

        if grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(list(micro.parameters()) + list(psf.parameters()), max_norm=grad_clip, norm_type=2)

        optim_ae.step()
        optim_ae.zero_grad()

        #supervised schedular step if necessary
        if sched_sl:
            sched_sl.step()
        #autoencoder schedular
        if sched_ae:
            sched_ae.step()

#         print('RMSE ', np.round(np.sqrt(((x.detach().cpu().numpy()-(res_img['background'] + ae_img).detach().cpu().numpy())**2).mean()),2))
#         print('Log p ', log_p_x_given_z.item())

        writer.add_scalar('SL Losses/gmm_loss', gmm_loss.detach().cpu(), batch_idx)
        writer.add_scalar('SL Losses/bg_loss', background_loss.detach().cpu(), batch_idx)

        writer.add_scalar('AE Losses/p_x_given_z', log_p_x_given_z.detach().cpu(), batch_idx)
#         writer.add_scalar('AE Losses/q_z_given_x', log_q_z_given_x.detach().cpu(), batch_idx)
#         writer.add_scalar('AE Losses/p_z', log_p_z.detach().cpu(), batch_idx)
#         writer.add_scalar('AE Losses/ELBO', elbo_loss.detach().cpu(), batch_idx)

        if batch_idx % log_interval == 0 and writer is not None:

            with torch.no_grad():
                pred_df = model_output_to_df(res_sim, 0.1)
                target_df = sample_to_df(locs_sl, x_os_sl, y_os_sl, z_os_sl, ints_sl)

                if gt_dict is not None:
                    res_gt = model(gt_img[None].cuda())
                    locs_ae, x_os_ae, y_os_ae, z_os_ae, ints_ae, output_shape_ae = model_output_to_micro_input(res_gt, threshold=0.1)
                    ae_img = microscope(locs_ae, x_os_ae, y_os_ae, z_os_ae, ints_ae, output_shape_ae)
                    pred_gt_df = model_output_to_df(res_gt, 0.1, px_size=gt_dict['px_size'])
                    free_mem()

            if writer is not None:
                gt_logger(writer, pred_df, target_df, batch_idx, data_str='Sim. ')
                gt_logger(writer, pred_gt_df, gt_df, batch_idx, data_str='Inp. ')

            sl_fig = sl_plot(x, xsim_noise, background, res_sim)
            plt.show()

            if gt_dict is not None:
                gt_fig = gt_plot(gt_img, pred_gt_df, gt_df, gt_dict['px_size'],ae_img[0]+res_gt['background'][0])
                plt.show()

            if writer is not None:
                writer.add_figure('SL summary', sl_fig, batch_idx)
                writer.add_figure('GT', gt_fig, batch_idx)

            #storingv
            torch.save({'state_dict':model.state_dict(), 'scaling':[model.unet.inp_scale, model.unet.inp_offset]}, str(save_dir) +'/model_ae.pkl')
            torch.save(microscope.state_dict(), str(save_dir) + '/microscope_ae.pkl')
            torch.save(psf.state_dict(), str(save_dir) + '/psf_ae.pkl' )
            torch.save(opt_ae.state_dict(), str(save_dir) + '/opt_ae.pkl' )