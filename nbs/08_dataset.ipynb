{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp funcs.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from decode_fish.imports import *\n",
    "from decode_fish.funcs.file_io import *\n",
    "from typing import Union\n",
    "import skimage.measure\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DecodeDataset:\n",
    "\n",
    "    def __init__(self, path: Union[str, list],\n",
    "                 dataset_tfms: list, \n",
    "                 rate_transform: list, \n",
    "                 bg_transform: list, \n",
    "                 num_iter: int = 5000, \n",
    "                 device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        Basic Dataset\n",
    "\n",
    "        Args:\n",
    "            path (str): [image_path]\n",
    "            dataset_tfms (list): transformation specific to dataset\n",
    "            rate_transform ([type]): transformation for rate generation\n",
    "            bg_transform ([type]): background transformation\n",
    "            num_iter (int, optional):define lenth of dataset. Defaults to 5000.\n",
    "            device (str, optional): device cpu or gpu]. Defaults to 'cpu'.\n",
    "            scale ([type], optional): scaling image to value. Defaults to None.\n",
    "            min_value ([type], optional): minimum value. Defaults to None.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.image_path = glob.glob(path)\n",
    "        self.dataset_tfms = dataset_tfms\n",
    "        self.num_iter = num_iter\n",
    "        self.local_rate_tfms = rate_transform\n",
    "        self.bg_transform = bg_transform\n",
    "        self.device = device\n",
    "        \n",
    "        self.volumes = [load_tiff_image(f) for f in self.image_path]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_iter\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        x = random.choice(self.volumes)\n",
    "        x = self._compose(x, self.dataset_tfms)\n",
    "        local_rate = self.local_rate_tfms(x)\n",
    "        background = self.bg_transform(x)\n",
    "        return x.to(self.device), local_rate.to(self.device), background.to(self.device)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        print (f'{self.__class__.__name__} Summary:')\n",
    "        print (f'Dataset tfms: {len(self.dataset_tfms)}')\n",
    "        for i in self.dataset_tfms:\n",
    "            print (f'\\n-->')\n",
    "            f\"{i}\"\n",
    "        return ''\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compose(x, list_func):\n",
    "        if not list_func: list_func.append(lambda x: x)\n",
    "        for func in list_func:x = func(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_class_signature(self, nms):\n",
    "    \"print class signature\"\n",
    "    mod = inspect.currentframe().f_back.f_locals\n",
    "    for n in nms:\n",
    "        print(f'{n}: {getattr(self,n)}')\n",
    "#export\n",
    "class TransformBase:\n",
    "    '''\n",
    "    All transformations optionally must be inherited from this class for nice\n",
    "    representations and checks if input to given transformations is a tensor\n",
    "\n",
    "    '''\n",
    "    def __repr__(self):\n",
    "        print (f'Transform({self.__class__.__name__})')\n",
    "        name = inspect.signature(self.__class__).parameters.keys()\n",
    "        print_clas_signature(self, name)\n",
    "        return ''\n",
    "\n",
    "    def __call__(self, x):\n",
    "        assert isinstance(x, torch.Tensor), f'must be torch.tensor not {type(x)}'\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_device(x):\n",
    "        return getattr(x, 'device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ScaleTensor(TransformBase):\n",
    "    \"\"\"\n",
    "    \\nScales given `torch.Tensor` between `low` and `high`\n",
    "\n",
    "    \\nParameters:\n",
    "    \\n`low`     : lower bound\n",
    "    \\n`high`    : upper bound\n",
    "    \\n`data_min`: max value of data\n",
    "    \\n`data_max`: min value of main data\n",
    "\n",
    "    \\nReturns:\n",
    "    \\nScaled tensor\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, low: float, high: float, data_min: float=0., data_max: float =1.):\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.data_min = data_min\n",
    "        self.data_max = data_max\n",
    "        self.ratio = (self.high-self.low) /(self.data_max-self.data_min)\n",
    "\n",
    "    def __call__(self, x) -> torch.Tensor:\n",
    "        super().__call__(x)\n",
    "        return self.ratio * x + self.low- self.data_min * self.ratio\n",
    "    \n",
    "\n",
    "class EstimateBackground(TransformBase):\n",
    "    def __init__(self, smoothing_filter_size, div_factor=1):\n",
    "        self.smoothing_filter_size = smoothing_filter_size\n",
    "        self.div_factor = div_factor\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        background = gaussian_filter(image, self.smoothing_filter_size)/self.div_factor\n",
    "        #background.clamp_min_(1.)\n",
    "        return torch.tensor(background)\n",
    "    \n",
    "def get_forward_scaling(img):\n",
    "    \n",
    "    offset = np.median(img)\n",
    "    scale = np.max(np.array(img))/3\n",
    "    return offset, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RandomCrop3D(TransformBase):\n",
    "    \"\"\"\n",
    "    Ramdomly Crops 3D tensor.\n",
    "\n",
    "    \\nThis class will generate random crop of `crop_sz`. This calss is initilize\n",
    "    with `img_sz` which should be a demension of 4 [Channel, Height, Width, Depth] and\n",
    "    a `crop_sz` dimesnion of 3 [Height, Width, Depth] of desired crop. For each crop\n",
    "    dimension `_get_slice` function will calculate random int ranging from 0 to (img_sz-crop_sz).\n",
    "    and return tuple of containing two slice intergers. If one dimension of `img_sz` matches\n",
    "    one dimension of `crop_sz` the resulting tuple will be `(None, None)` which will result\n",
    "    in not croping this particular dimension.\n",
    "\n",
    "\n",
    "    \\nParameters:\n",
    "    \\n`img_sz`     : Size of the 3D image `(C, H, W, D)`\n",
    "    \\n`crop_sz`    : Size of the 3D crop  `(H, W, D)`\n",
    "\n",
    "    \\nReturns:\n",
    "    \\nCroped 3D image of the given `crop_sz`\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, crop_sz, roi_mask):\n",
    "        assert len(crop_sz) == 3 , f'Lenth of crop_sz should be 3 not {len(crop_sz)}'\n",
    "        self.crop_sz = tuple(crop_sz)\n",
    "        self.crop_prod = crop_sz[0]*crop_sz[1]*crop_sz[2]\n",
    "        self.roi_mask = roi_mask\n",
    "\n",
    "    def __call__(self, x):\n",
    "        _, h, w, d = x.shape\n",
    "        img_sz  = tuple((h, w, d))\n",
    "        assert (img_sz) >  self.crop_sz\n",
    "        super().__call__(x)\n",
    "        slice_hwd = [self._get_slice(i, k) for i, k in zip(img_sz, self.crop_sz)]\n",
    "        while self._crop(self.roi_mask[None], *slice_hwd).sum()/self.crop_prod < 0.5:\n",
    "            slice_hwd = [self._get_slice(i, k) for i, k in zip(img_sz, self.crop_sz)]\n",
    "        return self._crop(x, *slice_hwd)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_slice(sz, crop_sz):\n",
    "        up_bound = sz-crop_sz\n",
    "        if  up_bound == 0:\n",
    "            return None, None\n",
    "        else:\n",
    "            l_bound = torch.randint(up_bound, (1,))\n",
    "        return l_bound, l_bound + crop_sz\n",
    "\n",
    "    @staticmethod\n",
    "    def _crop(x, slice_h, slice_w, slice_d):\n",
    "        return x[:, slice_h[0]:slice_h[1], slice_w[0]:slice_w[1], slice_d[0]:slice_d[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_roi_mask(img, pool_size=(10,10,10), percentile=50):\n",
    "    mp_img = skimage.measure.block_reduce(img, pool_size, np.max)\n",
    "    thresh = np.percentile(mp_img,percentile)\n",
    "    mp_img = np.where(mp_img>thresh,1,0)\n",
    "    mp_img = np.kron(mp_img, np.ones((pool_size), dtype=mp_img.dtype))\n",
    "    mp_img = mp_img[:img.shape[0], :img.shape[1], :img.shape[2]] \n",
    "    return mp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_fish.funcs.file_io import load_psf_noise_micro\n",
    "from decode_fish.engine.point_process import PointProcessUniform\n",
    "from decode_fish.funcs.plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_3d = load_tiff_image('/groups/turaga/home/speisera/share_TUM/FishSIM/sim_1/mRNAlevel_200/cell3D/strong/w1_HelaKyoto_Gapdh_2597_p01_cy3__Cell_CP_14__cell3D__1.tif')[0]\n",
    "estimate_backg = EstimateBackground(smoothing_filter_size=6, div_factor=1)\n",
    "roi_mask       = get_roi_mask(img_3d, pool_size=(10,10,10), percentile=70)\n",
    "rand_crop      = RandomCrop3D((48,48,48), roi_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probmap_generator = ScaleTensor(low=0.00000001, \n",
    "                                high=0.005, \n",
    "                                data_min = img_3d.min(), \n",
    "                                data_max = img_3d.max())\n",
    "\n",
    "ds = DecodeDataset(path = '/groups/turaga/home/speisera/share_TUM/FishSIM/sim_1/mRNAlevel_200/cell3D/strong/w1_HelaKyoto_Gapdh_2597_p01_cy3__Cell_CP_14__cell3D__1.tif',\n",
    "                   dataset_tfms =  [rand_crop], \n",
    "                   rate_transform = probmap_generator, \n",
    "                   bg_transform = estimate_backg, \n",
    "                   device='cuda:0', \n",
    "                   num_iter=100 * 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_dl = DataLoader(ds, batch_size=4, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/groups/turaga/home/speisera/anaconda3/envs/decode2_dev/lib/python3.7/site-packages/torch/cuda/__init__.py:81: UserWarning: \n",
      "    Found GPU2 NVS 510 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "x, local_rate, background = next(iter(decode_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(default_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf, noise, micro = load_psf_noise_micro(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dataloader(cfg):\n",
    "    \n",
    "    img_3d = load_tiff_image(cfg.data_path.image_path)[0]\n",
    "    estimate_backg = hydra.utils.instantiate(cfg.bg_estimation)\n",
    "    roi_mask       = get_roi_mask(img_3d, tuple(cfg.roi_mask.pool_size), percentile= cfg.roi_mask.percentile)\n",
    "    rand_crop      = RandomCrop3D((cfg.random_crop.crop_sz,cfg.random_crop.crop_sz,cfg.random_crop.crop_sz), roi_mask)\n",
    "    \n",
    "    probmap_generator = ScaleTensor(low=cfg.prob_generator.low, \n",
    "                                    high=cfg.prob_generator.high, \n",
    "                                    data_min = img_3d.min(), \n",
    "                                    data_max = img_3d.max())\n",
    "\n",
    "    ds = DecodeDataset(path = cfg.data_path.image_path,\n",
    "                       dataset_tfms =  [rand_crop], \n",
    "                       rate_transform = probmap_generator, \n",
    "                       bg_transform = estimate_backg, \n",
    "                       device='cuda:0', \n",
    "                       num_iter=cfg.dataloader.num_iter * cfg.dataloader.bs) \n",
    "\n",
    "    decode_dl = DataLoader(ds, batch_size=cfg.dataloader.bs, num_workers=0)\n",
    "    decode_dl.min_int = cfg.pointprocess.min_int\n",
    "    \n",
    "    return img_3d, decode_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_models.ipynb.\n",
      "Converted 01_psf.ipynb.\n",
      "Converted 02_microscope.ipynb.\n",
      "Converted 03_noise.ipynb.\n",
      "Converted 04_pointsource.ipynb.\n",
      "Converted 05_gmm_loss.ipynb.\n",
      "Converted 06_plotting.ipynb.\n",
      "Converted 07_file_io.ipynb.\n",
      "Converted 08_dataset.ipynb.\n",
      "Converted 09_output_trafo.ipynb.\n",
      "Converted 10_evaluation.ipynb.\n",
      "Converted 11_emitter_io.ipynb.\n",
      "Converted 12_utils.ipynb.\n",
      "Converted 13_train.ipynb.\n",
      "Converted 15_fit_psf.ipynb.\n",
      "Converted 16_visualization.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode2_dev]",
   "language": "python",
   "name": "conda-env-decode2_dev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
