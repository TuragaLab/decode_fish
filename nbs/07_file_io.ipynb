{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp funcs.file_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport -decode_fish.engine.place_psfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from decode_fish.imports import *\n",
    "from decode_fish.funcs.utils import *\n",
    "from tifffile import imread\n",
    "from decode_fish.engine.microscope import Microscope\n",
    "from decode_fish.engine.psf import LinearInterpolatedPSF\n",
    "from decode_fish.funcs.emitter_io import *\n",
    "from decode_fish.funcs.dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "from collections.abc import MutableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_df_from_hdf5(group):\n",
    "    \n",
    "    df = DF()\n",
    "    for k in group.keys():\n",
    "        df[k] = group[k][()]\n",
    "    return df\n",
    "\n",
    "def add_df_to_hdf5(parent, name, df):\n",
    "    \n",
    "    g = parent.create_group(name)\n",
    "    for k in df.keys():\n",
    "        g.create_dataset(k, data=df[k].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_psf_noise_micro(cfg):\n",
    "    \n",
    "    psf = hydra.utils.instantiate(cfg.genm.PSF)\n",
    "    noise = hydra.utils.instantiate(cfg.genm.noise)\n",
    "    micro = hydra.utils.instantiate(cfg.genm.microscope, psf=psf, noise=noise).cuda()\n",
    "\n",
    "    return micro\n",
    "\n",
    "#export\n",
    "def load_model_state(model, path):\n",
    "    \"\"\"\n",
    "    Loads the network parameters and the scaling into the model given a path.\n",
    "    \"\"\"\n",
    "    model_dict = torch.load(path)\n",
    "    model.load_state_dict(model_dict['state_dict'], strict=False)\n",
    "    model.inp_scale = model_dict['scaling'][0]\n",
    "    model.inp_offset = model_dict['scaling'][1]\n",
    "    return model\n",
    "\n",
    "def get_dataloader(cfg):\n",
    "    \n",
    "    from_records = False if cfg.data_path.image_path is None else True\n",
    "    sl = eval(cfg.data_path.image_proc.crop_sl,{'__builtins__': None},{'s_': np.s_})\n",
    "\n",
    "    if from_records:\n",
    "        if 'override' in cfg.data_path.image_proc:\n",
    "            imgs_5d = torch.cat([hydra.utils.instantiate(cfg.data_path.image_proc.override, image_path=f) for f in sorted(glob.glob(cfg.data_path.image_path))], 0)\n",
    "        else:\n",
    "            imgs_5d   = torch.cat([load_tiff_image(f)[None] for f in sorted(glob.glob(cfg.data_path.image_path))], 0)    \n",
    "    \n",
    "        if imgs_5d.ndim > 5:\n",
    "            imgs_5d = imgs_5d.view(-1, *(imgs_5d.size()[2:]))\n",
    "            \n",
    "        imgs_5d       = torch.cat([img.permute(*cfg.data_path.image_proc.swap_dim)[sl][None] for img in imgs_5d], 0) \n",
    "        roi_masks     = [get_roi_mask(img, tuple(cfg.sim.roi_mask.pool_size), percentile= cfg.sim.roi_mask.percentile) for img in imgs_5d]\n",
    "    else:\n",
    "        imgs_5d       = torch.cat([torch.empty(list(cfg.data_path.image_sim.image_shape))], 0) \n",
    "        roi_masks     = None\n",
    "        gen_bg        = [hydra.utils.instantiate(cfg.sim.bg_estimation.uniform)]\n",
    "        dataset_tfms  = []\n",
    "        \n",
    "        \n",
    "    min_shape = tuple(np.stack([v.shape for v in imgs_5d]).min(0)[-3:])\n",
    "    crop_zyx = (cfg.sim.random_crop.crop_sz, cfg.sim.random_crop.crop_sz,cfg.sim.random_crop.crop_sz)\n",
    "    if crop_zyx > min_shape:\n",
    "        crop_zyx = tuple(np.stack([min_shape, crop_zyx]).min(0))\n",
    "        print('Crop size larger than volume in at least one dimension. Crop size changed to', crop_zyx)\n",
    "        \n",
    "    if from_records:\n",
    "        if cfg.sim.bg_estimation.type == 'smoothing':\n",
    "            gen_bg        = [hydra.utils.instantiate(cfg.sim.bg_estimation.smoothing, z_size=crop_zyx[0])]\n",
    "        elif cfg.sim.bg_estimation.type == 'uniform':\n",
    "            gen_bg        = [hydra.utils.instantiate(cfg.sim.bg_estimation.uniform)]\n",
    "            \n",
    "        rand_crop = RandomCrop3D(crop_zyx, roi_masks)\n",
    "        dataset_tfms  = [rand_crop]\n",
    "    \n",
    "    if cfg.sim.bg_estimation.fractal.scale:\n",
    "        gen_bg.append(hydra.utils.instantiate(cfg.sim.bg_estimation.fractal))\n",
    "\n",
    "    probmap_generator = UniformValue(cfg.genm.prob_generator.low, cfg.genm.prob_generator.high)\n",
    "    rate_tfms = [probmap_generator]\n",
    "    \n",
    "    if cfg.genm.foci.n_foci_avg > 0:\n",
    "        rate_tfms.append(hydra.utils.instantiate(cfg.genm.foci))\n",
    "\n",
    "    ds = DecodeDataset(volumes = imgs_5d,\n",
    "                       dataset_tfms =  dataset_tfms, \n",
    "                       rate_tfms = rate_tfms, \n",
    "                       bg_tfms = gen_bg, \n",
    "                       device='cuda:0', \n",
    "                       from_records=from_records,\n",
    "                       num_iter=(cfg.training.num_iters) * cfg.training.bs) \n",
    "\n",
    "    decode_dl = DataLoader(ds, batch_size=cfg.training.bs, num_workers=0)\n",
    "    \n",
    "    return imgs_5d, decode_dl\n",
    "    \n",
    "def load_all(cfg, load_ds=True):\n",
    "    \n",
    "    path = Path(cfg.output.save_dir)\n",
    "    model = hydra.utils.instantiate(cfg.network)\n",
    "    model = load_model_state(model, path/'model.pkl')\n",
    "    post_proc = hydra.utils.instantiate(cfg.post_proc_isi)\n",
    "    micro = load_psf_noise_micro(cfg)\n",
    "    micro.load_state_dict(torch.load(path/'microscope.pkl'), strict=False)\n",
    "    if load_ds:\n",
    "        imgs_5d, decode_dl = get_dataloader(cfg)\n",
    "    else:\n",
    "        imgs_5d, decode_dl = None, None\n",
    "    \n",
    "    return model, post_proc, micro, imgs_5d, decode_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('../config/experiment/sim_int_1.yaml')\n",
    "# cfg = OmegaConf.load(default_conf)\n",
    "psf = load_psf(cfg)\n",
    "# psf.load_state_dict(torch.load(Path(cfg.output.save_dir)/'psf.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 22, 1, 512, 512])\n",
      "Crop size larger than volume in at least one dimension. Crop size changed to (1, 56, 56)\n",
      "10 volumes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/groups/turaga/home/speisera/anaconda3/envs/decode_fish_dev2/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "imgs_3d, decode_dl = get_dataloader(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_models.ipynb.\n",
      "Converted 01_psf.ipynb.\n",
      "Converted 02_microscope.ipynb.\n",
      "Converted 02b_place_psfs.ipynb.\n",
      "Converted 03_noise.ipynb.\n",
      "Converted 04_pointsource.ipynb.\n",
      "Converted 05_gmm_loss.ipynb.\n",
      "Converted 06_plotting.ipynb.\n",
      "Converted 07_file_io.ipynb.\n",
      "Converted 08_dataset.ipynb.\n",
      "Converted 09_output_trafo.ipynb.\n",
      "Converted 10_matching.ipynb.\n",
      "Converted 11_emitter_io.ipynb.\n",
      "Converted 12_utils.ipynb.\n",
      "Converted 13_train.ipynb.\n",
      "Converted 15_fit_psf.ipynb.\n",
      "Converted 16_visualization.ipynb.\n",
      "Converted 17_eval_routines.ipynb.\n",
      "Converted 18_predict_funcs.ipynb.\n",
      "Converted 19_MERFISH_routines.ipynb.\n",
      "Converted 22_MERFISH_codenet.ipynb.\n",
      "Converted 23_MERFISH_comparison.ipynb.\n",
      "Converted 24_exp_specific.ipynb.\n",
      "Converted 25_ensembling.ipynb.\n",
      "Converted 26_gen_train.ipynb.\n",
      "Converted 27_testtime_rescale.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_fish_dev2]",
   "language": "python",
   "name": "conda-env-decode_fish_dev2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
