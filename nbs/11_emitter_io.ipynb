{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp funcs.emitter_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for working with emitter DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from decode_fish.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def shift_df(df, shift=[0.,0.,0.]):\n",
    "    \n",
    "    df_corr = df.copy()\n",
    "    df_corr['x'] += shift[0]\n",
    "    df_corr['y'] += shift[1]\n",
    "    df_corr['z'] += shift[2]\n",
    "    return df_corr\n",
    "\n",
    "def percentile_filter(df, perc, key='comb_sig', return_low=True):\n",
    "    if perc >= 100:\n",
    "        return df\n",
    "    filt_val = np.percentile(df[key], perc)\n",
    "    if return_low:\n",
    "        return df[df[key] < filt_val]\n",
    "    else:\n",
    "        return df[df[key] > filt_val]\n",
    "\n",
    "\n",
    "def sig_filt(df, perc = 90, return_low=True):\n",
    "    if perc >= 100:\n",
    "        return df\n",
    "    filt_val = np.percentile(df['comb_sig'], perc)\n",
    "    if return_low:\n",
    "        return df[df['comb_sig'] < filt_val]\n",
    "    else:\n",
    "        return df[df['comb_sig'] > filt_val]\n",
    "\n",
    "#export\n",
    "def nm_to_px(df, px_size_zyx=[100.,100.,100.]):\n",
    "    \n",
    "    if df is None: return None\n",
    "    df_corr = df.copy()\n",
    "    df_corr['x'] /= px_size_zyx[2]\n",
    "    df_corr['y'] /= px_size_zyx[1]\n",
    "    df_corr['z'] /= px_size_zyx[0]\n",
    "    return df_corr\n",
    "\n",
    "#export\n",
    "def px_to_nm(df, px_size_zyx=[100.,100.,100.]):\n",
    "    \n",
    "    df_corr = df.copy()\n",
    "    df_corr['x'] *= px_size_zyx[2]\n",
    "    df_corr['y'] *= px_size_zyx[1]\n",
    "    df_corr['z'] *= px_size_zyx[0]\n",
    "    return df_corr\n",
    "\n",
    "#export\n",
    "def cat_emitter_dfs(df_list, n_frames=1):\n",
    "    \n",
    "    if isinstance(n_frames, int):\n",
    "        n_frames = [n_frames for _ in df_list]\n",
    "    cum_frames = np.cumsum(n_frames)\n",
    "    ret_df = df_list[0]\n",
    "    for df, n_f in zip(df_list[1:], cum_frames[:-1]):\n",
    "        dfc = df.copy()\n",
    "        if len(ret_df):\n",
    "            dfc['frame_idx'] += n_f\n",
    "            dfc['loc_idx'] += ret_df['loc_idx'].values[-1] + 1\n",
    "        ret_df = pd.concat([ret_df, dfc], ignore_index=True)\n",
    "    return ret_df\n",
    "\n",
    "def append_emitter_df(df1, df2):\n",
    "    \n",
    "    if not len(df1):\n",
    "        return df2\n",
    "    \n",
    "    dfc = df2.copy()\n",
    "    dfc['frame_idx'] += df1['frame_idx'].max() + 1\n",
    "    dfc['loc_idx'] += df1['loc_idx'].max() + 1    \n",
    "    \n",
    "    return pd.concat([df1, dfc], ignore_index=True)\n",
    "\n",
    "#export\n",
    "def crop_df(df, fzyx_sl=np.s_[:,:,:,:], shift=True, px_size_zyx=[1.,1.,1.], keys=['frame_idx','z','y','x']):\n",
    "    \n",
    "    px_size_zyx = list(px_size_zyx)\n",
    "    df_crop = df.copy()\n",
    "    for sl, key, px_s in zip(fzyx_sl, keys, [1] + px_size_zyx):\n",
    "        if sl.start:\n",
    "            df_crop = df_crop[df_crop[key] >= px_s*sl.start]\n",
    "        if sl.stop:\n",
    "            df_crop = df_crop[df_crop[key] < px_s*sl.stop]\n",
    "        if (shift) & (sl.start is not None):\n",
    "            df_crop[key] -= px_s*sl.start\n",
    "        \n",
    "    return df_crop\n",
    "\n",
    "def exclude_borders(df, img_size, px_size_zyx=[1.,1.,1.], border_size_zyx=[1000,400,400]):\n",
    "    \n",
    "    zm, ym, xm = img_size[-3:]\n",
    "    \n",
    "    sl = np.s_[:,border_size_zyx[0]:zm*px_size_zyx[0]-border_size_zyx[0],\n",
    "                 border_size_zyx[1]:ym*px_size_zyx[1]-border_size_zyx[1],\n",
    "                 border_size_zyx[2]:xm*px_size_zyx[2]-border_size_zyx[2]\n",
    "              ]\n",
    "    \n",
    "    return crop_df(df, sl, px_size_zyx=[1.,1.,1.], shift=False)\n",
    "\n",
    "def get_n_locs(df):\n",
    "    counts = []\n",
    "    vcounts = pred_df['frame_idx'].value_counts()\n",
    "    for i in range(pred_df['frame_idx'].max()+1):\n",
    "        if i in vcounts.index:\n",
    "            counts.append(vcounts[i])\n",
    "    return counts\n",
    "\n",
    "def sel_int_ch(res_df, codebook):\n",
    "    \n",
    "    if len(res_df):\n",
    "    \n",
    "        int_m = [f'int_{i}' for i in range(codebook.shape[1])]\n",
    "        int_s = [f'int_sig_{i}' for i in range(codebook.shape[1])]\n",
    "\n",
    "        int_arr = res_df.loc[:,int_m].values\n",
    "        int_sig = res_df.loc[:,int_s].values\n",
    "\n",
    "        int_arr_nz = int_arr[codebook[res_df['code_inds'].values].nonzero()].reshape([int_arr.shape[0], -1])\n",
    "        int_sig_nz = int_sig[codebook[res_df['code_inds'].values].nonzero()].reshape([int_sig.shape[0], -1])\n",
    "\n",
    "    #     print(int_arr.shape)\n",
    "\n",
    "        ret_df = res_df.drop(columns=int_m)\n",
    "        ret_df = ret_df.drop(columns=int_s)\n",
    "\n",
    "        ret_df[int_m[:4]] = int_arr_nz\n",
    "        ret_df[int_s[:4]] = int_sig_nz\n",
    "\n",
    "        ret_df['tot_int'] = int_arr_nz.sum(1)\n",
    "        ret_df['tot_int_sig'] = int_sig_nz.sum(1)\n",
    "\n",
    "        ret_df['int_ratio'] = ((int_arr).sum(-1) - int_arr_nz.sum(-1)) / int_arr_nz.sum(-1)\n",
    "\n",
    "        return ret_df\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return res_df\n",
    "\n",
    "def zero_int_ch(res_df, codebook):\n",
    "    \n",
    "    int_m = [f'int_{i}' for i in range(codebook.shape[1])]\n",
    "    int_s = [f'int_sig_{i}' for i in range(codebook.shape[1])]\n",
    "    \n",
    "    int_arr = res_df.loc[:,int_m].values\n",
    "    int_sig = res_df.loc[:,int_s].values\n",
    "    \n",
    "    int_arr[codebook[res_df['code_inds'].values] == 0] = 0.\n",
    "    int_sig[codebook[res_df['code_inds'].values] == 0] = 0.\n",
    "    \n",
    "    \n",
    "    ret_df = res_df.drop(columns=int_m)\n",
    "    ret_df = ret_df.drop(columns=int_s)\n",
    "    \n",
    "    ret_df[int_m] = int_arr\n",
    "    ret_df[int_s] = int_sig\n",
    "\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from decode_fish.funcs.evaluation import matching\n",
    "def get_peaks(image,threshold=500,min_distance=20):\n",
    "    \"\"\"Peak finding functions. Provides position estimate for bead stacks that are used as initialization for PSF fitting.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image: 2D array\n",
    "        Single bead recording\n",
    "    threshold: float\n",
    "        Initial threshold to identify pixel that are considered as possible peaks. \n",
    "    min_distance: float\n",
    "        Minimal distance between two peaks in pixels\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    peaks: array\n",
    "        Array of x,y peak positions\n",
    "    \"\"\"            \n",
    "    peaks = []\n",
    "    t_img = np.where(image>threshold,image,0)\n",
    "    inds = t_img.nonzero()\n",
    "    vals = t_img[inds]\n",
    "    inds_yx = [[y,x] for _,y,x in sorted(zip(vals,inds[0],inds[1]))][::-1]    \n",
    "    \n",
    "    while len(inds_yx) > 0:\n",
    "        \n",
    "        valid = True\n",
    "        yx = inds_yx[0]\n",
    "        y,x = yx\n",
    "        inds_yx.remove(yx)\n",
    "        \n",
    "        for pyx in peaks:\n",
    "            if np.sqrt((y-pyx[0])**2 + (x-pyx[1])**2) < min_distance:\n",
    "                valid = False\n",
    "                break\n",
    "        if valid:       \n",
    "            peaks.append(yx)\n",
    "            \n",
    "    peak_df = DF(np.array(peaks)[:,::-1], columns=['x','y'])\n",
    "    peak_df['frame_idx'] = 0\n",
    "    peak_df['z'] = 0\n",
    "    peak_df['loc_idx'] = np.arange(len(peak_df))\n",
    "            \n",
    "    return peak_df\n",
    "\n",
    "def remove_fids(res_df, fid_df, tolerance=1000):\n",
    "\n",
    "    res_c = DF.copy(res_df)\n",
    "    frames = res_c['frame_idx'].values + 0\n",
    "    res_c['frame_idx'] = 0\n",
    "    _, match_fid, _ = matching(fid_df, res_c, tolerance=tolerance, allow_multiple_matches=True, match_genes=False,print_res=False)\n",
    "    inds = ~res_c['loc_idx'].isin(match_fid['loc_idx_pred'])\n",
    "    res_clean = res_c.loc[inds]\n",
    "    res_clean['frame_idx'] = frames[inds]\n",
    "    \n",
    "    return res_clean\n",
    "\n",
    "#export\n",
    "def remove_doublets(pred_df, tolerance=300):\n",
    "    _, matches, _ = matching(pred_df, pred_df, tolerance=tolerance, match_genes=True, allow_multiple_matches=True, self_match=True, print_res=False)\n",
    "    return pred_df.loc[~pred_df['loc_idx'].isin(matches['loc_idx_pred'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_models.ipynb.\n",
      "Converted 01_psf.ipynb.\n",
      "Converted 02_microscope.ipynb.\n",
      "Converted 02b_place_psfs.ipynb.\n",
      "Converted 03_noise.ipynb.\n",
      "Converted 04_pointsource.ipynb.\n",
      "Converted 05_gmm_loss.ipynb.\n",
      "Converted 06_plotting.ipynb.\n",
      "Converted 07_file_io.ipynb.\n",
      "Converted 08_dataset.ipynb.\n",
      "Converted 09_output_trafo.ipynb.\n",
      "Converted 10_evaluation.ipynb.\n",
      "Converted 11_emitter_io.ipynb.\n",
      "Converted 12_utils.ipynb.\n",
      "Converted 13_train.ipynb.\n",
      "Converted 15_fit_psf.ipynb.\n",
      "Converted 16_visualization.ipynb.\n",
      "Converted 17_eval_routines.ipynb.\n",
      "Converted 18_predict_funcs.ipynb.\n",
      "Converted 19_MERFISH_routines.ipynb.\n",
      "Converted 22_MERFISH_codenet.ipynb.\n",
      "Converted 23_MERFISH_comparison.ipynb.\n",
      "Converted 24_exp_specific.ipynb.\n",
      "Converted 25_ensembling.ipynb.\n",
      "Converted 26_gen_train.ipynb.\n",
      "Converted 27_testtime_rescale.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_fish_dev2]",
   "language": "python",
   "name": "conda-env-decode_fish_dev2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
