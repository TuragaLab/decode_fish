{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp engine.gmm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from decode_fish.imports import *\n",
    "from torch import distributions as D, Tensor\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions.utils import _sum_rightmost\n",
    "from einops import rearrange\n",
    "import torch.tensor as T\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ext_log_prob(mix, x):\n",
    "    \n",
    "    x = mix._pad(x)\n",
    "    log_prob_x = mix.component_distribution.base_dist.log_prob(x) \n",
    "    log_prob_x = _sum_rightmost(log_prob_x, 1)\n",
    "    \n",
    "    log_mix_prob = torch.log_softmax(mix.mixture_distribution.logits, dim=-1)  \n",
    "    return log_prob_x, log_mix_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PointProcessGaussian(Distribution):\n",
    "    def __init__(self, logits, xyzi_mu, xyzi_sigma, int_logits=None, **kwargs):\n",
    "        \"\"\" Defines our loss function. Given logits, xyzi_mu and xyzi_sigma \n",
    "        \n",
    "        The count loss first constructs a Gaussian approximation to the predicted number of emitters by summing the mean and the variance of the Bernoulli detection probability map,\n",
    "        and then maximizes the probability of the true number of emitters under this distribution. \n",
    "        The localization loss models the distribution of sub-pixel localizations with a coordinate-wise independent Gaussian probability distribution  with a 3D standard deviation. \n",
    "        For imprecise localizations, this probability is maximized for large sigmas, for precise localizations for small sigmas. \n",
    "        The distribution of all localizations over the entire image is approximated as a weighted average of individual localization distributions, where the weights correspond to the probability of detection.\n",
    "        \n",
    "        Args:\n",
    "            logits: shape (B,1,D,H,W)\n",
    "            xyzi_mu: shape (B,4,D,H,W)\n",
    "            xyzi_sigma: shape (B,4,D,H,W)\n",
    "        \"\"\"\n",
    "        self.logits = logits\n",
    "        self.xyzi_mu = xyzi_mu\n",
    "        self.xyzi_sigma = xyzi_sigma\n",
    "        self.int_logits = int_logits\n",
    "        \n",
    "    def log_prob(self, locations, x_offset, y_offset, z_offset, intensities, n_bits, channels, min_int_sig, int_fac=1, old_loss=False):\n",
    "        \n",
    "        gauss_dim = 3 + channels\n",
    "        batch_size = self.logits.shape[0]\n",
    "        if channels == 1:\n",
    "            xyzi, s_mask = get_true_labels(batch_size, locations, x_offset, y_offset, z_offset, intensities)\n",
    "        else:\n",
    "            xyzi, s_mask = get_true_labels_mf(batch_size, n_bits, channels, \n",
    "                                              locations, x_offset, y_offset, z_offset, intensities)\n",
    "        counts = s_mask.sum(-1)\n",
    "\n",
    "        P = torch.sigmoid(self.logits) \n",
    "        count_mean = P.sum(dim=[2, 3, 4]).squeeze(-1)\n",
    "        count_var = (P - P ** 2).sum(dim=[2, 3, 4]).squeeze(-1) \n",
    "        count_dist = D.Normal(count_mean, torch.sqrt(count_var))\n",
    "        \n",
    "        mixture_probs = P / P.sum(dim=[2, 3, 4], keepdim=True)\n",
    "\n",
    "        count_prob =  count_dist.log_prob(counts) * counts\n",
    "\n",
    "        pix_inds = torch.nonzero(P,as_tuple=True)\n",
    "\n",
    "        xyzi_mu = self.xyzi_mu[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]]\n",
    "        xyzi_mu[:,:3] += torch.stack([pix_inds[4],pix_inds[3],pix_inds[2]], 1) + 0.5\n",
    "        xyzi_mu = xyzi_mu.reshape(batch_size,-1,gauss_dim)\n",
    "        xyzi_sig = self.xyzi_sigma[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]].reshape(batch_size,-1,gauss_dim)\n",
    "\n",
    "        if int_fac > 0:\n",
    "        \n",
    "            int_P = torch.sigmoid(self.int_logits)[:,:channels]\n",
    "\n",
    "            int_mean = int_P.sum(dim=[1])\n",
    "            int_var = (int_P - int_P ** 2).sum(dim=[1])\n",
    "            int_dist = D.Normal(int_mean, torch.sqrt(int_var))\n",
    "\n",
    "            int_count_prob = (P[:,0].detach() * int_dist.log_prob(torch.ones_like(int_mean).cuda() * n_bits)).sum(dim=[1,2,3])\n",
    "            int_count_prob = int_count_prob*int_fac\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            int_count_prob = 0\n",
    "\n",
    "        '''split int 2'''\n",
    "        # self.logits: probability output. batch_size * 1 * h * w * d\n",
    "        # self.int_logits: probability output for the 16 channels. batch_size * n_channels * h * w * d\n",
    "\n",
    "#         mix_logits = self.logits[pix_inds].reshape(batch_size,-1)                                                                           # batch_size * n_pixels. n_pixels = h * w * d\n",
    "#         mixture_probs = mixture_probs[pix_inds].reshape(batch_size,-1)\n",
    "#         mix_logits = F.softmax(mixture_probs, dim=-1)\n",
    "        mix = D.Categorical(mixture_probs[pix_inds].reshape(batch_size,-1))\n",
    "        mix_logits = mix.logits\n",
    "\n",
    "        int_logits = rearrange(self.int_logits[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]], '(b p) ch -> b p ch', b=batch_size)      # batch_size * n_pixels * n_channels. n_pixels = h * w * d\n",
    "\n",
    "        # mix_logits: model output probability. batch_size * n_pixels. n_pixels = h * w * d\n",
    "        # int_logits: model output probability for the 16 channels. batch_size * n_pixels * n_channels\n",
    "        # xyzi_mu: model output (mean), batch_size * n_pixel * 19 : xyz + 16 intensities\n",
    "        # xyzi_sig: model output (sigma), batch_size * n_pixel * 19 \n",
    "\n",
    "        xyz_sl = np.s_[:,:,:3]           # slice selecting xyz\n",
    "        int_sl = np.s_[:,:,3:3+channels] # slice selecting 16 intensities\n",
    "\n",
    "        dist_normal_xyz = D.Independent(D.Normal(xyzi_mu[xyz_sl], xyzi_sig[xyz_sl] + 0.00001), 1)\n",
    "        dist_normal_int = D.Independent(D.Normal(xyzi_mu[int_sl], xyzi_sig[int_sl] + min_int_sig + 0.00001), 1) # remove min_int_sig\n",
    "\n",
    "        # xyzi: ground truth position. N_gt * batch_size * 19. N_gt = maximum number of GT emitters in a batch. \n",
    "\n",
    "        xyz_inp = xyzi[xyz_sl].transpose(0, 1)[:,:,None,:]          # reshape for log_prob()\n",
    "        log_norm_prob_xyz = dist_normal_xyz.base_dist.log_prob(xyz_inp) # N_gt * batch_size * n_pixel * 3\n",
    "\n",
    "        log_cat_prob = torch.log_softmax(mix_logits, -1) # + torch.log(counts+1e-6)[:, None]        # normalized (sum to 1 over pixels) log probs for the categorical dist. of the GMM. batch_size * n_pixels\n",
    "\n",
    "        int_inp = xyzi[int_sl].transpose(0, 1)[:,:,None,:]                # reshape for log_prob()\n",
    "        log_norm_prob_int = dist_normal_int.base_dist.log_prob(int_inp)   # N_gt * batch_size * n_pixel * 16\n",
    "\n",
    "        if old_loss:\n",
    "#             Original loss computing the likelihood of a GMM with 19 dimensional Gaussians. Gives decent results. \n",
    "            log_norm_prob_xyz = _sum_rightmost(log_norm_prob_xyz, 1)         # N_gt * batch_size * n_pixel\n",
    "            total_prob = torch.logsumexp(log_norm_prob_xyz + log_cat_prob + _sum_rightmost(log_norm_prob_int, 1),-1).transpose(0, 1) # logsumexp over pixels. batch_size * N_gt\n",
    "\n",
    "#         if int_fac:\n",
    "#             # New loss. The idea is that by zeroing out the log probabilities of the empty channels the models learns to assign zero probability to them (because the probabilties are normed over the channels)\n",
    "#             # This works but the results are much worse. It seems that the chained logsumexp gives shitty gradients.\n",
    "#             log_norm_prob_xyz = _sum_rightmost(log_norm_prob_xyz, 1)         # N_gt * batch_size * n_pixel\n",
    "#             log_mix_prob_int = torch.log_softmax(int_logits, dim=-1)                             # normalized (sum to 1 over channels) log probs.  batch_size * n_pixels * n_channels\n",
    "#             log_norm_prob_int[int_inp.expand(-1,-1,log_norm_prob_int.shape[2],-1) == 0] = -1000  # Only 4 out of 16 channels contain signal. We zero out the remaining entries.\n",
    "#             total_prob_int = torch.logsumexp(log_norm_prob_int + log_mix_prob_int ,-1)           # logsumexp over pixels. N_gt * batch_size * n_pixels\n",
    "#             total_prob = torch.logsumexp(log_norm_prob_xyz + total_prob_int + log_cat_prob,-1).transpose(0, 1)\n",
    "\n",
    "#         if int_fac:\n",
    "#             log_mix_prob_int = torch.log(torch.sigmoid(int_logits)) \n",
    "\n",
    "        else:\n",
    "            log_mix_prob_int = torch.log_softmax(int_logits, dim=-1) + torch.log(T(4.))\n",
    "    #             log_norm_prob_int[int_inp.expand(-1,-1,log_norm_prob_int.shape[2],-1) == 0] = 0\n",
    "\n",
    "            rep_log_prob = torch.cat([log_norm_prob_xyz[...,None].expand(-1,-1,-1,-1,16), log_norm_prob_int[:,:,:,None]], 3) # batch_size * n_pixels  * 4 (xyzi) * n_channels\n",
    "            rep_log_prob = rep_log_prob.sum(3)\n",
    "\n",
    "    #             rep_log_prob[int_inp.expand(-1,-1,log_norm_prob_int.shape[2],-1) == 0] = -10000\n",
    "\n",
    "            total_prob = torch.logsumexp(rep_log_prob + log_cat_prob[...,None].expand(-1,-1,channels) + log_mix_prob_int,2).transpose(0, 1)\n",
    "            total_prob = torch.gather(total_prob, -1, int_inp.argsort(-1, descending=True)[:,:,0].transpose(0, 1))[...,:4]\n",
    "\n",
    "            total_prob = total_prob.sum(-1)\n",
    "\n",
    "        # s_mask: batch_size * N_gt. Binary mask to remove entries in all samples that have less then N_gt GT emitters.\n",
    "        total_prob = (total_prob * s_mask).sum(-1)\n",
    "\n",
    "        return count_prob + int_count_prob, total_prob\n",
    "    \n",
    "    def log_prob_old(self, locations, x_offset, y_offset, z_offset, intensities, n_bits, channels, min_int_sig, int_fac=1):\n",
    "        \"\"\" Creates the distributions for the count and localization loss and evaluates the log probability for the given set of localizations under those distriubtions.\n",
    "        \n",
    "            Args:\n",
    "                locations: tuple with voxel locations of inferred emitters\n",
    "                x_offset, y_offset,z_offset: continuous within pixel offsets. Has lenght of number of emitters in the whole batch.\n",
    "                intensties: brightness of emitters. Has lenght of number of emitters in the whole batch.\n",
    "                \n",
    "            Returns:\n",
    "                count_prob: count loss. Has langth of batch_size\n",
    "                spatial_prob: localizations loss. Has langth of batch_size\n",
    "        \"\"\"     \n",
    "        \n",
    "        gauss_dim = 3 + channels\n",
    "        batch_size = self.logits.shape[0]\n",
    "        if channels == 1:\n",
    "            xyzi, s_mask = get_true_labels(batch_size, locations, x_offset, y_offset, z_offset, intensities)\n",
    "        else:\n",
    "            xyzi, s_mask = get_true_labels_mf(batch_size, n_bits, channels, \n",
    "                                              locations, x_offset, y_offset, z_offset, intensities)\n",
    "        counts = s_mask.sum(-1)\n",
    "        \n",
    "        P = torch.sigmoid(self.logits) \n",
    "        count_mean = P.sum(dim=[2, 3, 4]).squeeze(-1)\n",
    "        count_var = (P - P ** 2).sum(dim=[2, 3, 4]).squeeze(-1) \n",
    "        count_dist = D.Normal(count_mean, torch.sqrt(count_var))\n",
    "                \n",
    "        count_prob =  count_dist.log_prob(counts) # * counts\n",
    "        \n",
    "        mixture_probs = P / P.sum(dim=[2, 3, 4], keepdim=True)\n",
    "        \n",
    "        pix_inds = torch.nonzero(P,as_tuple=True)\n",
    "\n",
    "        xyzi_mu = self.xyzi_mu[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]]\n",
    "        xyzi_mu[:,:3] += torch.stack([pix_inds[4],pix_inds[3],pix_inds[2]], 1) + 0.5\n",
    "        xyzi_mu = xyzi_mu.reshape(batch_size,-1,gauss_dim)\n",
    "        xyzi_sig = self.xyzi_sigma[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]].reshape(batch_size,-1,gauss_dim)\n",
    "\n",
    "#         print(xyzi_mu.shape, xyzi.shape)\n",
    "        \n",
    "        mix = D.Categorical(mixture_probs[pix_inds].reshape(batch_size,-1))\n",
    "        \n",
    "        '''base 19 dim'''\n",
    "        xyzi_sig[:,:,3:] = xyzi_sig[:,:,3:] + min_int_sig\n",
    "        comp = D.Independent(D.Normal(xyzi_mu, xyzi_sig + 0.00001), 1)\n",
    "        spatial_gmm = D.MixtureSameFamily(mix, comp)\n",
    "        spatial_prob = spatial_gmm.log_prob(xyzi.transpose(0, 1)).transpose(0,1)\n",
    "        total_prob = (spatial_prob * s_mask).sum(-1)\n",
    "        '''split int'''\n",
    "#         xyz_sl = np.s_[:,:,:3]\n",
    "#         int_sl = np.s_[:,:,3:]\n",
    "        \n",
    "#         xyzi_sig[int_sl] = xyzi_sig[int_sl] + min_int_sig\n",
    "        \n",
    "#         comp_xyz = D.Independent(D.Normal(xyzi_mu[xyz_sl], xyzi_sig[xyz_sl] + 0.00001), 1)\n",
    "#         comp_int = D.Independent(D.Normal(xyzi_mu[int_sl], xyzi_sig[int_sl] + 0.00001), 1)\n",
    "        \n",
    "#         spatial_gmm = D.MixtureSameFamily(mix, comp_xyz)\n",
    "#         int_gmm = D.MixtureSameFamily(mix, comp_int)\n",
    "        \n",
    "#         spatial_prob, log_mix_prob = ext_log_prob(spatial_gmm, xyzi[xyz_sl].transpose(0, 1))\n",
    "#         int_prob, _                = ext_log_prob(int_gmm, xyzi[int_sl].transpose(0, 1))\n",
    "\n",
    "#         total_prob = torch.logsumexp(spatial_prob + int_prob + log_mix_prob,-1).transpose(0, 1)\n",
    "#         total_prob = (total_prob * s_mask).sum(-1)\n",
    "        \n",
    "        return count_prob, total_prob\n",
    "\n",
    "def get_sample_mask(bs, locations):\n",
    "    \n",
    "    counts_ = torch.unique(locations[0], return_counts=True)[1]\n",
    "    batch_loc = torch.unique(locations[0])\n",
    "    \n",
    "    counts = torch.cuda.LongTensor(bs).fill_(0)\n",
    "    counts[batch_loc] = counts_\n",
    "    \n",
    "    max_counts = counts.max()\n",
    "    if max_counts==0: max_counts = 1 #if all 0 will return empty matrix of correct size\n",
    "    s_arr = cum_count_per_group(locations[0])\n",
    "    s_mask   = torch.cuda.FloatTensor(bs,max_counts).fill_(0)\n",
    "    s_mask[locations[0],s_arr] = 1   \n",
    "    \n",
    "    return s_mask, s_arr\n",
    "    \n",
    "def get_true_labels(bs, locations, x_os, y_os, z_os, *args):\n",
    "    \n",
    "    s_mask, s_arr = get_sample_mask(bs, locations)\n",
    "    max_counts = s_mask.shape[1]\n",
    "    \n",
    "    x =  x_os + locations[4].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    y =  y_os + locations[3].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    z =  z_os + locations[2].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    \n",
    "    gt_vars = torch.stack([x, y, z] + [item for item in args], dim=1)\n",
    "    gt_list = torch.cuda.FloatTensor(bs,max_counts,gt_vars.shape[1]).fill_(0)\n",
    "    \n",
    "    gt_list[locations[0],s_arr] = gt_vars\n",
    "    return gt_list, s_mask    \n",
    "\n",
    "def get_true_labels_mf(bs, n_bits, channels, locations, x_os, y_os, z_os, int_ch):\n",
    "    \n",
    "    b_inds = torch.cat([torch.tensor([0], device=x_os.device),((x_os[1:] - x_os[:-1]).nonzero() + 1)[:,0], \n",
    "                        torch.tensor([len(x_os)], device=x_os.device)])\n",
    "    n_gt = len(b_inds) - 1\n",
    "    \n",
    "    xyz_locs = [l[b_inds[:-1]] for l in locations]\n",
    "    x_os = x_os[b_inds[:-1]]\n",
    "    y_os = y_os[b_inds[:-1]]\n",
    "    z_os = z_os[b_inds[:-1]]\n",
    "    \n",
    "    s_mask, s_arr = get_sample_mask(bs, xyz_locs)\n",
    "    max_counts = s_mask.shape[1]\n",
    "    \n",
    "    x =  x_os + xyz_locs[4].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    y =  y_os + xyz_locs[3].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    z =  z_os + xyz_locs[2].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    \n",
    "    loc_idx = []\n",
    "    for i in range(n_gt):\n",
    "        loc_idx += [i] * (b_inds[i+1] - b_inds[i])\n",
    "    \n",
    "    intensity = torch.zeros([n_gt, channels]).to(x.device)\n",
    "    intensity[loc_idx, locations[1]] = int_ch\n",
    "    \n",
    "    gt_vars = torch.stack([x, y, z], dim=1)\n",
    "    gt_vars = torch.cat([gt_vars, intensity], dim=1)\n",
    "    gt_list = torch.cuda.FloatTensor(bs,max_counts,gt_vars.shape[1]).fill_(0)\n",
    "    \n",
    "    gt_list[xyz_locs[0],s_arr] = gt_vars\n",
    "    return gt_list, s_mask  \n",
    "\n",
    "def grp_range(counts: torch.Tensor):\n",
    "    assert counts.dim() == 1\n",
    "\n",
    "    idx = counts.cumsum(0)\n",
    "    id_arr = torch.ones(idx[-1], dtype=int, device=counts.device)\n",
    "    id_arr[0] = 0\n",
    "    id_arr[idx[:-1]] = -counts[:-1] + 1\n",
    "    return id_arr.cumsum(0)\n",
    "\n",
    "def cum_count_per_group(arr):\n",
    "    \"\"\"\n",
    "    Helper function that returns the cumulative sum per group.\n",
    "    Example:\n",
    "        [0, 0, 0, 1, 2, 2, 0] --> [0, 1, 2, 0, 0, 1, 3]\n",
    "    \"\"\"\n",
    "\n",
    "    if arr.numel() == 0:\n",
    "        return arr\n",
    "\n",
    "    _, cnt = torch.unique(arr, return_counts=True)\n",
    "    return grp_range(cnt)[np.argsort(np.argsort(arr.cpu().numpy(), kind='mergesort'), kind='mergesort')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = torch.load('../data/model_batch_output_code_int_p.pt')\n",
    "sim_vars = torch.load('../data/sim_var_code_int_p.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000, -1.9318,  0.0000,  1.5036,  1.7690,  0.0000,  0.0000],\n",
       "        device='cuda:0'),\n",
       " tensor([  0.0000, -46.1897,   0.0000, -11.6720, -11.7959,   0.0000,   0.0000],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg = PointProcessGaussian(**model_out)\n",
    "ppg.log_prob(*sim_vars[:5], n_bits=4, channels=16, min_int_sig=1.0, int_fac=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000, -1.9318,  0.0000,  1.5036,  1.7690,  0.0000,  0.0000],\n",
       "        device='cuda:0'),\n",
       " tensor([   0.0000, -141.4240,    0.0000,  -46.2780,  -47.8225,    0.0000,\n",
       "            0.0000], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg.log_prob(*sim_vars[:5], n_bits=4, channels=16, min_int_sig=1.0, int_fac=0, old_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4427, -0.6439,  0.8149,  1.5036,  1.7690,  1.2149, -1.6208],\n",
       "        device='cuda:0'),\n",
       " tensor([   0.0000, -141.4240,    0.0000,  -46.2780,  -47.8225,    0.0000,\n",
       "            0.0000], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg.log_prob(*sim_vars[:5], n_bits=4, channels=16, min_int_sig=1.0, int_fac=0, old_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4427, -0.6439,  0.8149,  1.5036,  1.7690,  1.2149, -1.6208],\n",
       "        device='cuda:0'),\n",
       " tensor([   0.0000, -141.4240,    0.0000,  -46.2780,  -47.8225,    0.0000,\n",
       "            0.0000], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg.log_prob_old(*sim_vars[:5], n_bits=4, channels=16, min_int_sig=1.0, int_fac=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg.log_prob_old(*sim_vars[:5], n_bits=4, channels=16, min_int_sig=1.0, int_fac=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations, x_offset, y_offset, z_offset, intensities = sim_vars[:5]\n",
    "n_bits=4; channels=16; min_int_sig=1.; int_fac=16\n",
    "old_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_dim = 3 + channels\n",
    "batch_size = ppg.logits.shape[0]\n",
    "if channels == 1:\n",
    "    xyzi, s_mask = get_true_labels(batch_size, locations, x_offset, y_offset, z_offset, intensities)\n",
    "else:\n",
    "    xyzi, s_mask = get_true_labels_mf(batch_size, n_bits, channels, \n",
    "                                      locations, x_offset, y_offset, z_offset, intensities)\n",
    "counts = s_mask.sum(-1)\n",
    "\n",
    "P = torch.sigmoid(ppg.logits) \n",
    "count_mean = P.sum(dim=[2, 3, 4]).squeeze(-1)\n",
    "count_var = (P - P ** 2).sum(dim=[2, 3, 4]).squeeze(-1) \n",
    "count_dist = D.Normal(count_mean, torch.sqrt(count_var))\n",
    "\n",
    "count_prob =  count_dist.log_prob(counts) # * counts\n",
    "\n",
    "pix_inds = torch.nonzero(P,as_tuple=True)\n",
    "\n",
    "xyzi_mu = ppg.xyzi_mu[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]]\n",
    "xyzi_mu[:,:3] += torch.stack([pix_inds[4],pix_inds[3],pix_inds[2]], 1) + 0.5\n",
    "xyzi_mu = xyzi_mu.reshape(batch_size,-1,gauss_dim)\n",
    "xyzi_sig = ppg.xyzi_sigma[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]].reshape(batch_size,-1,gauss_dim)\n",
    "\n",
    "int_P = torch.sigmoid(ppg.int_logits)[:,:channels]\n",
    "\n",
    "int_mean = int_P.sum(dim=[1])\n",
    "int_var = (int_P - int_P ** 2).sum(dim=[1])\n",
    "int_dist = D.Normal(int_mean, torch.sqrt(int_var))\n",
    "\n",
    "int_count_prob = (P[:,0].detach() * int_dist.log_prob(torch.ones_like(int_mean).cuda() * n_bits)).sum(dim=[1,2,3])\n",
    "\n",
    "'''split int 2'''\n",
    "# self.logits: probability output. batch_size * 1 * h * w * d\n",
    "# self.int_logits: probability output for the 16 channels. batch_size * n_channels * h * w * d\n",
    "\n",
    "mix_logits = ppg.logits[pix_inds].reshape(batch_size,-1)                                                                           # batch_size * n_pixels. n_pixels = h * w * d\n",
    "int_logits = rearrange(ppg.int_logits[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]], '(b p) ch -> b p ch', b=batch_size)      # batch_size * n_pixels * n_channels. n_pixels = h * w * d\n",
    " \n",
    "# mix_logits: model output probability. batch_size * n_pixels. n_pixels = h * w * d\n",
    "# int_logits: model output probability for the 16 channels. batch_size * n_pixels * n_channels\n",
    "# xyzi_mu: model output (mean), batch_size * n_pixel * 19 : xyz + 16 intensities\n",
    "# xyzi_sig: model output (sigma), batch_size * n_pixel * 19 \n",
    "\n",
    "xyz_sl = np.s_[:,:,:3]           # slice selecting xyz\n",
    "int_sl = np.s_[:,:,3:3+channels] # slice selecting 16 intensities\n",
    "\n",
    "dist_normal_xyz = D.Independent(D.Normal(xyzi_mu[xyz_sl], xyzi_sig[xyz_sl] + 0.00001), 1)\n",
    "dist_normal_int = D.Independent(D.Normal(xyzi_mu[int_sl], xyzi_sig[int_sl] + 0.00001 + min_int_sig), 1)\n",
    "\n",
    "# xyzi: ground truth position. N_gt * batch_size * 19. N_gt = maximum number of GT emitters in a batch. \n",
    "\n",
    "xyz_inp = xyzi[xyz_sl].transpose(0, 1)[:,:,None,:]          # reshape for log_prob()\n",
    "log_norm_prob_xyz = dist_normal_xyz.base_dist.log_prob(xyz_inp) # N_gt * batch_size * n_pixel * 3\n",
    "\n",
    "log_cat_prob = torch.log_softmax(mix_logits, -1) + torch.log(counts+1e-6)[:, None]       # normalized (sum to 1 over pixels) log probs for the categorical dist. of the GMM. batch_size * n_pixels\n",
    "\n",
    "int_inp = xyzi[int_sl].transpose(0, 1)[:,:,None,:]                # reshape for log_prob()\n",
    "log_norm_prob_int = dist_normal_int.base_dist.log_prob(int_inp)   # N_gt * batch_size * n_pixel * 16\n",
    "\n",
    "if old_loss:\n",
    "    # Original loss computing the likelihood of a GMM with 19 dimensional Gaussians. Gives decent results. \n",
    "    log_norm_prob_xyz = _sum_rightmost(log_norm_prob_xyz, 1)         # N_gt * batch_size * n_pixel\n",
    "    total_prob = torch.logsumexp(log_norm_prob_xyz + log_cat_prob + _sum_rightmost(log_norm_prob_int, 1),-1).transpose(0, 1) # logsumexp over pixels. batch_size * N_gt\n",
    "\n",
    "# else:\n",
    "# #     New loss. The idea is that by zeroing out the log probabilities of the empty channels the models learns to assign zero probability to them (because the probabilties are normed over the channels)\n",
    "# #     This works but the results are much worse. It seems that the chained logsumexp gives shitty gradients.\n",
    "#     log_norm_prob_xyz = _sum_rightmost(log_norm_prob_xyz, 1)         # N_gt * batch_size * n_pixel\n",
    "#     log_mix_prob_int = torch.log_softmax(int_logits, dim=-1)                             # normalized (sum to 1 over channels) log probs.  batch_size * n_pixels * n_channels\n",
    "#     log_norm_prob_int[int_inp.expand(-1,-1,log_norm_prob_int.shape[2],-1) == 0] = -1000  # Only 4 out of 16 channels contain signal. We zero out the remaining entries.\n",
    "#     total_prob_int = torch.logsumexp(log_norm_prob_int + log_mix_prob_int ,-1)           # logsumexp over pixels. N_gt * batch_size * n_pixels\n",
    "#     total_prob = torch.logsumexp(log_norm_prob_xyz + total_prob_int + log_cat_prob,-1).transpose(0, 1)\n",
    "    \n",
    "else:\n",
    "    log_mix_prob_int = torch.log_softmax(int_logits, dim=-1) + torch.log(T(4.))\n",
    "    \n",
    "    rep_log_prob = torch.cat([log_norm_prob_xyz[...,None].expand(-1,-1,-1,-1,16), log_norm_prob_int[:,:,:,None]], 3) # batch_size * n_pixels * 4 (xyzi) * n_channels\n",
    "    rep_log_prob = rep_log_prob.sum(3)\n",
    "    \n",
    "    total_prob = torch.logsumexp(rep_log_prob + log_cat_prob[...,None].expand(-1,-1,channels) + log_mix_prob_int,2).transpose(0, 1)\n",
    "\n",
    "    total_prob = torch.gather(total_prob, -1, int_inp.argsort(-1, descending=True)[:,:,0].transpose(0, 1))[...,:4]\n",
    "    total_prob = total_prob.sum(-1)\n",
    "    \n",
    "total_prob = (total_prob * s_mask).sum(-1)  # s_mask: batch_size * N_gt. Binary mask to remove entries in all samples that have less then N_gt GT emitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-06, 3.0000e+00, 1.0000e-06, 1.0000e+00, 1.0000e+00, 1.0000e-06,\n",
       "        1.0000e-06], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(log_cat_prob).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2304])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cat_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3816e+01,  1.0986e+00, -1.3816e+01,  9.5367e-07,  9.5367e-07,\n",
       "        -1.3816e+01, -1.3816e+01], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(counts+1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 2304, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_inp.argsort(-1, descending=True).expand(-1,-1,rep_log_prob.shape[2],-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.0000, -47.0564,   0.0000, -11.6505, -11.7778,   0.0000,   0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 1, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_inp.argsort(-1, descending=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 1, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2304, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cat_prob[...,None].expand(-1,-1,channels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7, 2304])\n",
      "torch.Size([7, 2304])\n",
      "torch.Size([3, 7, 2304])\n",
      "torch.Size([3, 7])\n"
     ]
    }
   ],
   "source": [
    "print(log_prob_xyz.shape)\n",
    "print(log_mix_prob_xyz.shape)\n",
    "print((log_prob_xyz + log_mix_prob_xyz).shape)\n",
    "print(torch.logsumexp(log_prob_xyz + log_mix_prob_xyz,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7, 2304, 16])\n",
      "torch.Size([7, 2304, 16])\n",
      "torch.Size([3, 7, 2304, 16])\n",
      "torch.Size([3, 7, 2304])\n"
     ]
    }
   ],
   "source": [
    "print(log_prob_int.shape)\n",
    "print(log_mix_prob_int_x_p.shape)\n",
    "print((log_prob_int + log_mix_prob_int_x_p).shape)\n",
    "print(torch.logsumexp(log_prob_int + log_mix_prob_int_x_p,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-30149.2031, -30149.2031, -30149.2031],\n",
       "        [-36184.4805, -36311.1953, -36365.1406],\n",
       "        [-24744.4707, -24744.4707, -24744.4707],\n",
       "        [-33971.2578, -33420.4922, -33420.4922],\n",
       "        [-34393.4453, -33823.4766, -33823.4766],\n",
       "        [-23325.5508, -23325.5508, -23325.5508],\n",
       "        [-31364.9023, -31364.9023, -31364.9023]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob_int.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob_xyz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3, 2304])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(log_prob_xyz + log_mix_prob_xyz,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2304])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(log_mix_prob_int + log_mix_prob_int_x_p,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob_xyz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, -1.8633,  0.0000,  0.4107,  0.5226,  0.0000,  0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2304, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_mix.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_fish.engine.point_process import PointProcessUniform\n",
    "point_process = PointProcessUniform(local_rate = torch.ones([2,1,48,48,48])*.001, int_conc=1.0, sim_iters=5, channels=16, n_bits=4)\n",
    "locs_3d, x_os_3d, y_os_3d, z_os_3d, ints_3d, output_shape = point_process.sample(phasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1782\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n",
      "tensor([112., 119.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "locs_3d = [l.cuda() for l in locs_3d]\n",
    "# xyzi_true, s_mask = get_true_labels(2, locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda())\n",
    "xyzi_true, s_mask = get_true_labels_mf(2, 4, 16, locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda())\n",
    "print(len(locs_3d[0]))\n",
    "print(s_mask)\n",
    "print(s_mask.sum(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-272.8184, -314.4428], device='cuda:0', grad_fn=<SubBackward0>),\n",
       " tensor([-48894.6641, -45920.2773], device='cuda:0', grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg = PointProcessGaussian(**model_out)\n",
    "gmm_loss = ppg.log_prob(locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda(), n_bits=4, channels=16, min_int_sig=0.1)\n",
    "gmm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([119, 2, 2304, 3])\n",
      "torch.Size([119, 2, 2304, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-272.8184, -314.4428], device='cuda:0', grad_fn=<SubBackward0>),\n",
       " tensor([-48895.4141, -45920.4023], device='cuda:0', grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg = PointProcessGaussian(**model_out)\n",
    "gmm_loss = ppg.log_prob(locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda(), n_bits=4, channels=16, min_int_sig=0.1)\n",
    "gmm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -6410.5063, -12788.6523], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     gmm_loss = PointProcessGaussian(**model_out).log_prob(locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_models.ipynb.\n",
      "Converted 01_psf.ipynb.\n",
      "Converted 02_microscope.ipynb.\n",
      "Converted 03_noise.ipynb.\n",
      "Converted 04_pointsource.ipynb.\n",
      "Converted 05_gmm_loss.ipynb.\n",
      "Converted 06_plotting.ipynb.\n",
      "Converted 07_file_io.ipynb.\n",
      "Converted 08_dataset.ipynb.\n",
      "Converted 09_output_trafo.ipynb.\n",
      "Converted 10_evaluation.ipynb.\n",
      "Converted 11_emitter_io.ipynb.\n",
      "Converted 12_utils.ipynb.\n",
      "Converted 13_train.ipynb.\n",
      "Converted 15_fit_psf.ipynb.\n",
      "Converted 16_visualization.ipynb.\n",
      "Converted 17_eval_routines.ipynb.\n",
      "Converted 18_predict_funcs.ipynb.\n",
      "Converted 19_MERFISH_routines.ipynb.\n",
      "Converted 20_MERFISH_visualization.ipynb.\n",
      "Converted 22_MERFISH_codenet.ipynb.\n",
      "Converted 23_MERFISH_comparison.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_fish_dev2]",
   "language": "python",
   "name": "conda-env-decode_fish_dev2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
