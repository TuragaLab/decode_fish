{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp engine.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODE Network\n",
    "\n",
    "> Definition of the classes and modules we use to build our DECODE network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from decode_fish.imports import *\n",
    "import torch.nn as nn\n",
    "import types\n",
    "from functools import partial\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def number_of_features_per_level(init_channel_number, num_levels):\n",
    "    return [init_channel_number * 2 ** k for k in range(num_levels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding):\n",
    "    \"\"\"\n",
    "    Create a list of modules with together constitute a single conv layer with non-linearity\n",
    "    and optional batchnorm/groupnorm.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size(int or tuple): size of the convolving kernel\n",
    "        order (string): order of things, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'gcr' -> groupnorm + conv + ReLU\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "            'bcr' -> batchnorm + conv + ReLU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    Return:\n",
    "        list of tuple (name, module)\n",
    "    \"\"\"\n",
    "    assert 'c' in order, \"Conv layer MUST be present\"\n",
    "    assert order[0] not in 'rle', 'Non-linearity cannot be the first operation in the layer'\n",
    "\n",
    "    modules = []\n",
    "    for i, char in enumerate(order):\n",
    "        if char == 'r':\n",
    "            modules.append(('ReLU', nn.ReLU(inplace=True)))\n",
    "        elif char == 'l':\n",
    "            modules.append(('LeakyReLU', nn.LeakyReLU(negative_slope=0.1, inplace=True)))\n",
    "        elif char == 'e':\n",
    "            modules.append(('ELU', nn.ELU(inplace=True)))\n",
    "        elif char == 'c':\n",
    "            # add learnable bias only in the absence of batchnorm/groupnorm\n",
    "            bias = not ('g' in order or 'b' in order)\n",
    "            modules.append(('conv', nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias, padding_mode='replicate')))\n",
    "        elif char == 'g':\n",
    "            is_before_conv = i < order.index('c')\n",
    "            if is_before_conv:\n",
    "                num_channels = in_channels\n",
    "            else:\n",
    "                num_channels = out_channels\n",
    "\n",
    "            # use only one group if the given number of groups is greater than the number of channels\n",
    "            if num_channels < num_groups:\n",
    "                num_groups = 1\n",
    "\n",
    "            assert num_channels % num_groups == 0, f'Expected number of channels in input to be divisible by num_groups. num_channels={num_channels}, num_groups={num_groups}'\n",
    "            modules.append(('groupnorm', nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)))\n",
    "        elif char == 'b':\n",
    "            is_before_conv = i < order.index('c')\n",
    "            if is_before_conv:\n",
    "                modules.append(('batchnorm', nn.BatchNorm3d(in_channels)))\n",
    "            else:\n",
    "                modules.append(('batchnorm', nn.BatchNorm3d(out_channels)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported layer type '{char}'. MUST be one of ['b', 'g', 'r', 'l', 'e', 'c']\")\n",
    "\n",
    "    return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SingleConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Basic convolutional module consisting of a Conv3d, non-linearity and optional batchnorm/groupnorm. The order\n",
    "    of operations can be specified via the `order` parameter\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple):\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, order='gcr', num_groups=8, padding=1):\n",
    "        super(SingleConv, self).__init__()\n",
    "\n",
    "        for name, module in create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=padding):\n",
    "            self.add_module(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DoubleConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    A module consisting of two consecutive convolution layers (e.g. BatchNorm3d+ReLU+Conv3d).\n",
    "    We use (Conv3d+ReLU+GroupNorm3d) by default.\n",
    "    This can be changed however by providing the 'order' argument, e.g. in order\n",
    "    to change to Conv3d+BatchNorm3d+ELU use order='cbe'.\n",
    "    Use padded convolutions to make sure that the output (H_out, W_out) is the same\n",
    "    as (H_in, W_in), so that you don't have to crop in the decoder path.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        encoder (bool): if True we're in the encoder path, otherwise we're in the decoder\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, encoder, kernel_size=3, order='gcr', num_groups=8, padding=1):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        if encoder:\n",
    "            # we're in the encoder path\n",
    "            conv1_in_channels = in_channels\n",
    "            conv1_out_channels = out_channels // 2\n",
    "            if conv1_out_channels < in_channels:\n",
    "                conv1_out_channels = in_channels\n",
    "            conv2_in_channels, conv2_out_channels = conv1_out_channels, out_channels\n",
    "        else:\n",
    "            # we're in the decoder path, decrease the number of channels in the 1st convolution\n",
    "            conv1_in_channels, conv1_out_channels = in_channels, out_channels\n",
    "            conv2_in_channels, conv2_out_channels = out_channels, out_channels\n",
    "\n",
    "        # conv1\n",
    "        self.add_module('SingleConv1',\n",
    "                        SingleConv(conv1_in_channels, conv1_out_channels, kernel_size, order, num_groups,\n",
    "                                   padding=padding))\n",
    "        # conv2\n",
    "        self.add_module('SingleConv2',\n",
    "                        SingleConv(conv2_in_channels, conv2_out_channels, kernel_size, order, num_groups,\n",
    "                                   padding=padding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Upsampling(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsamples a given multi-channel 3D data using either interpolation or learned transposed convolution.\n",
    "    Args:\n",
    "        transposed_conv (bool): if True uses ConvTranspose3d for upsampling, otherwise uses interpolation\n",
    "        in_channels (int): number of input channels for transposed conv\n",
    "            used only if transposed_conv is True\n",
    "        out_channels (int): number of output channels for transpose conv\n",
    "            used only if transposed_conv is True\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "            used only if transposed_conv is True\n",
    "        scale_factor (int or tuple): stride of the convolution\n",
    "            used only if transposed_conv is True\n",
    "        mode (str): algorithm used for upsampling:\n",
    "            'nearest' | 'linear' | 'bilinear' | 'trilinear' | 'area'. Default: 'nearest'\n",
    "            used only if transposed_conv is False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transposed_conv, in_channels=None, out_channels=None, kernel_size=3,\n",
    "                 scale_factor=(2, 2, 2), mode='nearest'):\n",
    "        super(Upsampling, self).__init__()\n",
    "\n",
    "        if transposed_conv:\n",
    "            # make sure that the output size reverses the MaxPool3d from the corresponding encoder\n",
    "            # (D_out = (D_in − 1) ×  stride[0] − 2 ×  padding[0] +  kernel_size[0] +  output_padding[0])\n",
    "            self.upsample = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size, stride=scale_factor,\n",
    "                                               padding=1)\n",
    "        else:\n",
    "            self.upsample = partial(self._interpolate, mode=mode)\n",
    "\n",
    "    def forward(self, encoder_features, x):\n",
    "        output_size = encoder_features.size()[2:]\n",
    "        return self.upsample(x, output_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def _interpolate(x, size, mode):\n",
    "        return F.interpolate(x, size=size, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single module from the encoder path consisting of the optional max\n",
    "    pooling layer (one may specify the MaxPool kernel_size to be different\n",
    "    than the standard (2,2,2), e.g. if the volumetric data is anisotropic\n",
    "    (make sure to use complementary scale_factor in the decoder path) followed by\n",
    "    a DoubleConv module.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
    "        apply_pooling (bool): if True use MaxPool3d before DoubleConv\n",
    "        pool_kernel_size (int or tuple): the size of the window\n",
    "        pool_type (str): pooling layer: 'max' or 'avg'\n",
    "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
    "        conv_layer_order (string): determines the order of layers\n",
    "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, apply_pooling=True,\n",
    "                 pool_kernel_size=2, pool_type='max', basic_module=DoubleConv, conv_layer_order='gcr',\n",
    "                 num_groups=8, padding=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        assert pool_type in ['max', 'avg']\n",
    "        if apply_pooling:\n",
    "            if pool_type == 'max':\n",
    "                self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)\n",
    "            else:\n",
    "                self.pooling = nn.AvgPool3d(kernel_size=pool_kernel_size)\n",
    "        else:\n",
    "            self.pooling = None\n",
    "\n",
    "        self.basic_module = basic_module(in_channels, out_channels,\n",
    "                                         encoder=True,\n",
    "                                         kernel_size=conv_kernel_size,\n",
    "                                         order=conv_layer_order,\n",
    "                                         num_groups=num_groups,\n",
    "                                         padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pooling is not None:\n",
    "            x = self.pooling(x)\n",
    "        x = self.basic_module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single module for decoder path consisting of the upsampling layer\n",
    "    (either learned ConvTranspose3d or nearest neighbor interpolation) followed by a basic module (DoubleConv or ExtResNetBlock).\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
    "        scale_factor (tuple): used as the multiplier for the image H/W/D in\n",
    "            case of nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation\n",
    "            from the corresponding encoder\n",
    "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
    "        conv_layer_order (string): determines the order of layers\n",
    "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, scale_factor=(2, 2, 2), basic_module=DoubleConv,\n",
    "                 conv_layer_order='gcr', num_groups=8, mode='nearest', padding=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        if basic_module == DoubleConv:\n",
    "            # if DoubleConv is the basic_module use interpolation for upsampling and concatenation joining\n",
    "            self.upsampling = Upsampling(transposed_conv=False, in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=conv_kernel_size, scale_factor=scale_factor, mode=mode)\n",
    "            # concat joining\n",
    "            self.joining = partial(self._joining, concat=True)\n",
    "        else:\n",
    "            # if basic_module=ExtResNetBlock use transposed convolution upsampling and summation joining\n",
    "            self.upsampling = Upsampling(transposed_conv=True, in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=conv_kernel_size, scale_factor=scale_factor, mode=mode)\n",
    "            # sum joining\n",
    "            self.joining = partial(self._joining, concat=False)\n",
    "            # adapt the number of in_channels for the ExtResNetBlock\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.basic_module = basic_module(in_channels, out_channels,\n",
    "                                         encoder=False,\n",
    "                                         kernel_size=conv_kernel_size,\n",
    "                                         order=conv_layer_order,\n",
    "                                         num_groups=num_groups,\n",
    "                                         padding=padding)\n",
    "\n",
    "    def forward(self, encoder_features, x):\n",
    "        x = self.upsampling(encoder_features=encoder_features, x=x)\n",
    "        x = self.joining(encoder_features, x)\n",
    "        x = self.basic_module(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _joining(encoder_features, x, concat):\n",
    "        if concat:\n",
    "            return torch.cat((encoder_features, x), dim=1)\n",
    "        else:\n",
    "            return encoder_features + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Abstract3DUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for standard and residual UNet.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        f_maps (int, tuple): number of feature maps at each level of the encoder; if it's an integer the number\n",
    "            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4\n",
    "        final_sigmoid (bool): if True apply element-wise nn.Sigmoid after the\n",
    "            final 1x1 convolution, otherwise apply nn.Softmax. MUST be True if nn.BCELoss (two-class) is used\n",
    "            to train the model. MUST be False if nn.CrossEntropyLoss (multi-class) is used to train the model.\n",
    "        basic_module: basic model for the encoder/decoder (DoubleConv, ExtResNetBlock, ....)\n",
    "        layer_order (string): determines the order of layers\n",
    "            in `SingleConv` module. e.g. 'crg' stands for Conv3d+ReLU+GroupNorm3d.\n",
    "            See `SingleConv` for more info\n",
    "        f_maps (int, tuple): if int: number of feature maps in the first conv layer of the encoder (default: 64);\n",
    "            if tuple: number of feature maps at each level\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        num_levels (int): number of levels in the encoder/decoder path (applied only if f_maps is an int)\n",
    "        is_segmentation (bool): if True (semantic segmentation problem) Sigmoid/Softmax normalization is applied\n",
    "            after the final convolution; if False (regression problem) the normalization layer is skipped at the end\n",
    "        testing (bool): if True (testing mode) the `final_activation` (if present, i.e. `is_segmentation=true`)\n",
    "            will be applied as the last operation during the forward pass; if False the model is in training mode\n",
    "            and the `final_activation` (even if present) won't be applied; default: False\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel in the basic_module\n",
    "        pool_kernel_size (int or tuple): the size of the window\n",
    "        conv_padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, final_sigmoid, basic_module, f_maps=64, is_2D=False, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, testing=False,\n",
    "                 conv_kernel_size=3, pool_kernel_size=2, conv_padding=1, **kwargs):\n",
    "        super(Abstract3DUNet, self).__init__()\n",
    "\n",
    "        self.testing = testing\n",
    "        if is_2D:\n",
    "            conv_kernel_size = [1, conv_kernel_size, conv_kernel_size]\n",
    "            pool_kernel_size = [1, pool_kernel_size, pool_kernel_size]\n",
    "            conv_padding = [0, conv_padding, conv_padding]\n",
    "\n",
    "        if isinstance(f_maps, int):\n",
    "            f_maps = number_of_features_per_level(f_maps, num_levels=num_levels)\n",
    "\n",
    "        # create encoder path consisting of Encoder modules. Depth of the encoder is equal to `len(f_maps)`\n",
    "        encoders = []\n",
    "        for i, out_feature_num in enumerate(f_maps):\n",
    "            if i == 0:\n",
    "                encoder = Encoder(in_channels, out_feature_num,\n",
    "                                  apply_pooling=False,  # skip pooling in the firs encoder\n",
    "                                  basic_module=basic_module,\n",
    "                                  conv_layer_order=layer_order,\n",
    "                                  conv_kernel_size=conv_kernel_size,\n",
    "                                  num_groups=num_groups,\n",
    "                                  padding=conv_padding)\n",
    "            else:\n",
    "                # TODO: adapt for anisotropy in the data, i.e. use proper pooling kernel to make the data isotropic after 1-2 pooling operations\n",
    "                encoder = Encoder(f_maps[i - 1], out_feature_num,\n",
    "                                  basic_module=basic_module,\n",
    "                                  conv_layer_order=layer_order,\n",
    "                                  conv_kernel_size=conv_kernel_size,\n",
    "                                  num_groups=num_groups,\n",
    "                                  pool_kernel_size=pool_kernel_size,\n",
    "                                  padding=conv_padding)\n",
    "\n",
    "            encoders.append(encoder)\n",
    "\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "\n",
    "        # create decoder path consisting of the Decoder modules. The length of the decoder is equal to `len(f_maps) - 1`\n",
    "        decoders = []\n",
    "        reversed_f_maps = list(reversed(f_maps))\n",
    "        for i in range(len(reversed_f_maps) - 1):\n",
    "            if basic_module == DoubleConv:\n",
    "                in_feature_num = reversed_f_maps[i] + reversed_f_maps[i + 1]\n",
    "            else:\n",
    "                in_feature_num = reversed_f_maps[i]\n",
    "\n",
    "            out_feature_num = reversed_f_maps[i + 1]\n",
    "            # TODO: if non-standard pooling was used, make sure to use correct striding for transpose conv\n",
    "            # currently strides with a constant stride: (2, 2, 2)\n",
    "            decoder = Decoder(in_feature_num, out_feature_num,\n",
    "                              basic_module=basic_module,\n",
    "                              conv_layer_order=layer_order,\n",
    "                              conv_kernel_size=conv_kernel_size,\n",
    "                              num_groups=num_groups,\n",
    "                              padding=conv_padding)\n",
    "            decoders.append(decoder)\n",
    "\n",
    "        self.decoders = nn.ModuleList(decoders)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder part\n",
    "        encoders_features = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            # reverse the encoder outputs to be aligned with the decoder\n",
    "            encoders_features.insert(0, x)\n",
    "\n",
    "        # remove the last encoder's output from the list\n",
    "        # !!remember: it's the 1st in the list\n",
    "        encoders_features = encoders_features[1:]\n",
    "\n",
    "        # decoder part\n",
    "        for decoder, encoder_features in zip(self.decoders, encoders_features):\n",
    "            # pass the output from the corresponding encoder and the output\n",
    "            # of the previous decoder\n",
    "            x = decoder(encoder_features, x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UNet3D(Abstract3DUNet):\n",
    "    \"\"\"\n",
    "    3DUnet model from\n",
    "    `\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"\n",
    "        <https://arxiv.org/pdf/1606.06650.pdf>`.\n",
    "    Uses `DoubleConv` as a basic_module and nearest neighbor upsampling in the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, final_sigmoid=True, f_maps=64, is_2D=False, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1,  **kwargs):\n",
    "        super(UNet3D, self).__init__(in_channels=in_channels, final_sigmoid=final_sigmoid,\n",
    "                                     basic_module=DoubleConv, f_maps=f_maps, is_2D=is_2D, layer_order=layer_order,\n",
    "                                     num_groups=num_groups, num_levels=num_levels, is_segmentation=is_segmentation,\n",
    "                                     conv_padding=conv_padding, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IntensityDist(nn.Module):\n",
    "    \"\"\"\n",
    "    Stores the three parameters that determine the distribution of intensities for simulator learning (and which are optimized during autoencoder learning)\n",
    "    Args:\n",
    "        int_conc, int_rate: parameters of the torch.distributions.gamma class.\n",
    "        int_loc: shift parameter. \n",
    "    \"\"\"\n",
    "    def __init__(self, int_conc, int_rate, int_loc):\n",
    "        super().__init__()\n",
    "        self.int_conc = torch.nn.Parameter(torch.tensor(float(int_conc)))\n",
    "        self.int_rate = torch.nn.Parameter(torch.tensor(float(int_rate)))\n",
    "        self.int_loc = torch.nn.Parameter(torch.tensor(float(int_loc)))  \n",
    "        \n",
    "class OutputNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the output maps from the U-net and processes them seperately, using two conv3D layers for each group (xyzi_mean, xyzi_sigma, probability, background)\n",
    "    Args:\n",
    "        f_maps: number of channels of the U-net output\n",
    "        p_offset: probability channel bias \n",
    "    \"\"\"\n",
    "    def __init__(self, f_maps=64, p_offset=-5., is_2D=False, code_inf=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.is_2D = is_2D\n",
    "        self.code_inf = code_inf\n",
    "        \n",
    "        xyzi_dim = 19 if code_inf else 4\n",
    "        bg_dim = 16 if code_inf else 1\n",
    "        \n",
    "        kernel_size = [1,3,3] if is_2D else [3,3,3]\n",
    "        padding = [0,1,1] if is_2D else [1,1,1]\n",
    "\n",
    "        self.p_out1 = nn.Conv3d(f_maps, f_maps, kernel_size=kernel_size, padding=padding)\n",
    "        self.p_out2 = nn.Conv3d(f_maps, 1, kernel_size=1, padding=0)\n",
    "        nn.init.constant_(self.p_out2.bias,p_offset)\n",
    "        \n",
    "        self.xyzi_out1 = nn.Conv3d(f_maps, f_maps, kernel_size=kernel_size, padding=padding)\n",
    "        self.xyzi_out2 = nn.Conv3d(f_maps, xyzi_dim, kernel_size=1, padding=0)\n",
    "        \n",
    "        self.xyzis_out1 = nn.Conv3d(f_maps, f_maps, kernel_size=kernel_size, padding=padding)\n",
    "        self.xyzis_out2 = nn.Conv3d(f_maps, xyzi_dim, kernel_size=1, padding=0)\n",
    "        \n",
    "        self.bg_out1 = nn.Conv3d(f_maps, f_maps, kernel_size=kernel_size, padding=padding)\n",
    "        self.bg_out2 = nn.Conv3d(f_maps, bg_dim, kernel_size=1, padding=0)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.p_out1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.p_out2.weight, mode='fan_in', nonlinearity='linear')\n",
    "        nn.init.kaiming_normal_(self.xyzi_out1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.xyzi_out2.weight, mode='fan_in', nonlinearity='linear')\n",
    "        nn.init.kaiming_normal_(self.xyzis_out1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.xyzis_out2.weight, mode='fan_in', nonlinearity='linear')\n",
    "        nn.init.kaiming_normal_(self.bg_out1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.bg_out2.weight, mode='fan_in', nonlinearity='linear')    \n",
    "        \n",
    "    def forward(self, x):\n",
    "                \n",
    "        logit    = F.elu(self.p_out1(x))\n",
    "        logit    = self.p_out2(logit)\n",
    "        logit    = torch.clamp(logit, -15., 15)\n",
    "        \n",
    "        xyzi = F.elu(self.xyzi_out1(x))\n",
    "        xyzi = self.xyzi_out2(xyzi)\n",
    "\n",
    "        xyz_mu   = torch.tanh(xyzi[:, :3])\n",
    "\n",
    "        i_mu     = F.softplus(xyzi[:, 3:])\n",
    "        xyzi_mu = torch.cat((xyz_mu, i_mu), dim=1)\n",
    "        \n",
    "        xyzis = F.elu(self.xyzis_out1(x))\n",
    "        xyzis = self.xyzis_out2(xyzis)\n",
    "        xyzi_sig = F.softplus(xyzis) + 0.01\n",
    "        \n",
    "        background = F.elu(self.bg_out1(x))\n",
    "        background = self.bg_out2(background)\n",
    "        background = F.softplus(background)\n",
    "        \n",
    "        return torch.cat([logit,xyzi_mu,xyzi_sig,background],1)\n",
    "            \n",
    "\n",
    "class UnetDecodeNoBn(nn.Module):\n",
    "    \"\"\"\n",
    "    Our DECODE network consists of a 3D U-net, and an output net module.\n",
    "    The network parameters can accessed through model.network\n",
    "    \n",
    "    We also store the parameters of the intensity distribution here for easier access. (model.int_dist)\n",
    "    \n",
    "    The forward function returns a tensor batch_size x 10 tensor to make it compatible with the monai.inferers.sliding_window_inference function.\n",
    "    \n",
    "    To get the final output dictionary one has to also apply the tensor_to_dict function.\n",
    "    \n",
    "    Args:\n",
    "        ch_in (int): number of input channels\n",
    "            Multiple input channels are currently not supported\n",
    "        depth (int): number of levels in the encoder/decoder path (applied only if f_maps is an int)\n",
    "        inp_offset, inp_scale (float): Values used for normalizing the input. \n",
    "        order (string): determines the order of layers\n",
    "            in `SingleConv` module. e.g. 'ce' stands for Conv3d+ELU.\n",
    "            See `SingleConv` for more info\n",
    "            Before the input is given to the network is is normalized using these variables.\n",
    "        f_maps (int, tuple): number of feature maps at each level of the encoder; if it's an integer the number\n",
    "            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4\n",
    "        p_offset (float):\n",
    "            bias of the probabilty channel. The negative value avoids very high rates at the start of the training which might cause memory issues.\n",
    "        int_conc, int_rate (float): parameters of the torch.distributions.gamma class.\n",
    "        int_loc (float): shift parameter. \n",
    "        \n",
    "    ToDo:\n",
    "        Support for multiple channels?\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, ch_in=1, depth=3, inp_scale=1., inp_offset=0., order='ce', f_maps=64, \n",
    "                 is_2D=False, code_inf=False, pred_z=True, p_offset=-5., int_conc=4., int_rate=1., int_loc=1.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.inp_scale = inp_scale\n",
    "        self.inp_offset = inp_offset\n",
    "        \n",
    "        self.ch_in = ch_in\n",
    "        self.is_2D = is_2D\n",
    "        self.code_inf = code_inf\n",
    "        self.pred_z = pred_z\n",
    "        \n",
    "        self.unet = UNet3D(ch_in, final_sigmoid=False, num_levels=depth, is_2D=is_2D,\n",
    "                           layer_order = order, f_maps=f_maps)\n",
    "        self.outnet = OutputNet(f_maps=f_maps, p_offset=p_offset, is_2D=is_2D, code_inf=code_inf)\n",
    "        \n",
    "        self.network = nn.ModuleList([self.unet, self.outnet])\n",
    "        self.int_dist = IntensityDist(int_conc, int_rate, int_loc)\n",
    "            \n",
    "    def forward(self, x, shuffle_ch=False):\n",
    "        \n",
    "        if shuffle_ch:\n",
    "            perm_inds = np.random.permutation(np.arange(self.ch_in))\n",
    "            x = x[:,perm_inds]\n",
    "        \n",
    "        x = (x-self.inp_offset) / self.inp_scale\n",
    "        \n",
    "        for net in self.network:\n",
    "            x = net(x)\n",
    "            \n",
    "        if shuffle_ch:\n",
    "            x[:,4:20] = x[:,4:20][:,np.argsort(perm_inds)]\n",
    "            x[:,23:39] = x[:,23:39][:,np.argsort(perm_inds)]\n",
    "            x[:,39:] = x[:,39:][:,np.argsort(perm_inds)]\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    def tensor_to_dict(self, x):\n",
    "        \n",
    "        # Limit intensity\n",
    "#         x[:,4] = x[:,4] + float(self.int_dist.int_loc.detach()) + 0.01\n",
    "        x[:,39:] = x[:,39:] * self.inp_scale \n",
    "    \n",
    "        if not self.pred_z:\n",
    "        \n",
    "            x[:,3] *= 0\n",
    "            x[:,7 + 15*self.code_inf] *= 0\n",
    "            x[:,7 + 15*self.code_inf] += 1\n",
    "            \n",
    "        if not self.code_inf:\n",
    "            ret_dict = {'logits': x[:,0:1], \n",
    "                        'xyzi_mu': x[:,1:5], \n",
    "                        'xyzi_sigma': x[:,5:9], \n",
    "                        'background': x[:,9:]}\n",
    "        else:\n",
    "            ret_dict = {'logits': x[:,0:1], \n",
    "                        'xyzi_mu': x[:,1:20], \n",
    "                        'xyzi_sigma': x[:,20:39], \n",
    "                        'background': x[:,39:]}            \n",
    "            \n",
    "        return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_fish.funcs.utils import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(16)[perm_inds] = np.arange(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1)\n",
    "inp = torch.randn([2,1,37,48,48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([2, 1, 1, 48, 48])\n",
      "xyzi_mu torch.Size([2, 19, 1, 48, 48])\n",
      "xyzi_sigma torch.Size([2, 19, 1, 48, 48])\n",
      "background torch.Size([2, 16, 1, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "model = UnetDecodeNoBn(order= 'ce', ch_in=16, f_maps=48, depth=2, is_2D=True, code_inf=True, pred_z=False)\n",
    "# output = model.tensor_to_dict(model(inp))\n",
    "output = model.tensor_to_dict(model(torch.randn([2,16,1,48,48]), shuffle_ch=True))\n",
    "\n",
    "for k in output.keys():\n",
    "    print(k, output[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4751, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['xyzi_sigma'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(output, '../data/model_batch_output_code.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = OmegaConf.load('../config/experiment/msp300_n2.yaml')\n",
    "# model = hydra.utils.instantiate(cfg.model, inp_scale=float(5), inp_offset=float(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241879"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241879"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156048"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.unet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156048"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.unet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85831"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.outnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85831"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.outnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_models.ipynb.\n",
      "Converted 01_psf.ipynb.\n",
      "Converted 02_microscope.ipynb.\n",
      "Converted 03_noise.ipynb.\n",
      "Converted 04_pointsource.ipynb.\n",
      "Converted 05_gmm_loss.ipynb.\n",
      "Converted 06_plotting.ipynb.\n",
      "Converted 07_file_io.ipynb.\n",
      "Converted 08_dataset.ipynb.\n",
      "Converted 09_output_trafo.ipynb.\n",
      "Converted 10_evaluation.ipynb.\n",
      "Converted 11_emitter_io.ipynb.\n",
      "Converted 12_utils.ipynb.\n",
      "Converted 13_train.ipynb.\n",
      "Converted 15_fit_psf.ipynb.\n",
      "Converted 16_visualization.ipynb.\n",
      "Converted 17_eval_routines.ipynb.\n",
      "Converted 18_predict_funcs.ipynb.\n",
      "Converted 19_MERFISH_routines.ipynb.\n",
      "Converted 20_MERFISH_visualization.ipynb.\n",
      "Converted 22_MERFISH_codenet.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode2_dev]",
   "language": "python",
   "name": "conda-env-decode2_dev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
