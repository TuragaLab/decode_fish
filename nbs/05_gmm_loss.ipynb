{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp engine.gmm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from decode_fish.imports import *\n",
    "from torch import distributions as D, Tensor\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions.utils import _sum_rightmost\n",
    "from einops import rearrange\n",
    "import torch.tensor as T\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ext_log_prob(mix, x):\n",
    "    \n",
    "    x = mix._pad(x)\n",
    "    log_prob_x = mix.component_distribution.base_dist.log_prob(x) \n",
    "    log_prob_x = _sum_rightmost(log_prob_x, 1)\n",
    "    \n",
    "    log_mix_prob = torch.log_softmax(mix.mixture_distribution.logits, dim=-1)  \n",
    "    return log_prob_x, log_mix_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PointProcessGaussian(Distribution):\n",
    "    def __init__(self, logits, xyzi_mu, xyzi_sigma, int_logits=None, **kwargs):\n",
    "        \"\"\" Defines our loss function. Given logits, xyzi_mu and xyzi_sigma \n",
    "        \n",
    "        The count loss first constructs a Gaussian approximation to the predicted number of emitters by summing the mean and the variance of the Bernoulli detection probability map,\n",
    "        and then maximizes the probability of the true number of emitters under this distribution. \n",
    "        The localization loss models the distribution of sub-pixel localizations with a coordinate-wise independent Gaussian probability distribution  with a 3D standard deviation. \n",
    "        For imprecise localizations, this probability is maximized for large sigmas, for precise localizations for small sigmas. \n",
    "        The distribution of all localizations over the entire image is approximated as a weighted average of individual localization distributions, where the weights correspond to the probability of detection.\n",
    "        \n",
    "        Args:\n",
    "            logits: shape (B,1,D,H,W)\n",
    "            xyzi_mu: shape (B,4,D,H,W)\n",
    "            xyzi_sigma: shape (B,4,D,H,W)\n",
    "        \"\"\"\n",
    "        self.logits = logits.cuda()\n",
    "        self.xyzi_mu = xyzi_mu.cuda()\n",
    "        self.xyzi_sigma = xyzi_sigma.cuda()\n",
    "        \n",
    "    def log_prob(self, locations, x_offset, y_offset, z_offset, intensities, codes, n_bits, channels, loss_option=0, count_mult=0, cat_logits=0):\n",
    "        \n",
    "        gauss_dim = 3 + 1\n",
    "        batch_size = self.logits.shape[0]\n",
    "        n_codes = self.logits.shape[1]\n",
    "\n",
    "        xyzi, gt_codes, s_mask = get_true_labels_mf(batch_size, locations, x_offset, y_offset, z_offset, intensities, codes.cuda())\n",
    "\n",
    "        P = torch.sigmoid(self.logits) \n",
    "\n",
    "        if loss_option == 0:\n",
    "            # Calculate count loss individually for each code\n",
    "            count_mean = P.sum(dim=[2, 3, 4]).squeeze(-1)\n",
    "            count_var = (P - P ** 2).sum(dim=[2, 3, 4]).squeeze(-1) \n",
    "            count_dist = D.Normal(count_mean, torch.sqrt(count_var))\n",
    "\n",
    "            counts = torch.zeros(count_mean.shape).cuda()\n",
    "            unique_col = [gtc.unique(return_counts=True) for gtc in gt_codes]\n",
    "            for i, ind_c in enumerate(unique_col):\n",
    "                inds, c = ind_c\n",
    "                counts[i, inds[inds>=0]] = c[inds>=0].type(torch.cuda.FloatTensor)\n",
    "\n",
    "            count_prob =  count_dist.log_prob(counts)\n",
    "            if count_mult:\n",
    "                count_prob = count_prob * counts\n",
    "            \n",
    "            count_prob = count_prob.sum(-1) \n",
    "            \n",
    "        if loss_option == 1:\n",
    "            # Calculate count loss by summing over all code channels\n",
    "            count_mean = P.sum(dim=[1, 2, 3, 4]).squeeze(-1)\n",
    "            count_var = (P - P ** 2).sum(dim=[1, 2, 3, 4]).squeeze(-1) \n",
    "            count_dist = D.Normal(count_mean, torch.sqrt(count_var))\n",
    "\n",
    "            counts = s_mask.sum(-1)\n",
    "            count_prob =  count_dist.log_prob(counts) \n",
    "            if count_mult:\n",
    "                count_prob = count_prob * counts\n",
    "\n",
    "        pix_inds = torch.nonzero(P[:,:1],as_tuple=True)\n",
    "\n",
    "        xyzi_mu = self.xyzi_mu[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]]\n",
    "        xyzi_mu[:,:3] += torch.stack([pix_inds[4],pix_inds[3],pix_inds[2]], 1) + 0.5\n",
    "        xyzi_mu = xyzi_mu.reshape(batch_size,-1,gauss_dim)\n",
    "        xyzi_sig = self.xyzi_sigma[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]].reshape(batch_size,-1,gauss_dim)\n",
    "\n",
    "        if cat_logits:\n",
    "            mixture_probs = P / P.sum(dim=[2, 3, 4], keepdim=True)\n",
    "            mix = D.Categorical(mixture_probs[torch.nonzero(P,as_tuple=True)].reshape(batch_size, n_codes, -1))\n",
    "            mix_logits = mix.logits    \n",
    "        else:\n",
    "            mix_logits = self.logits[torch.nonzero(P,as_tuple=True)].reshape(batch_size, n_codes, -1)\n",
    "\n",
    "        dist_normal_xyzi = D.Independent(D.Normal(xyzi_mu, xyzi_sig + 0.00001), 1)\n",
    "\n",
    "        xyzi_inp = xyzi.transpose(0, 1)[:,:,None,:]          # reshape for log_prob()\n",
    "        log_norm_prob_xyzi = dist_normal_xyzi.base_dist.log_prob(xyzi_inp) # N_gt * batch_size * n_pixel * 3\n",
    "\n",
    "        if loss_option == 0:\n",
    "            log_cat_prob = torch.log_softmax(mix_logits, -1) # + torch.log(counts+1e-6)[:, None]       # normalized (sum to 1 over pixels) log probs for the categorical dist. of the GMM. batch_size * n_pixels\n",
    "        else:\n",
    "            log_cat_prob = torch.log_softmax(mix_logits.view(batch_size, -1), -1).view(mix_logits.shape)\n",
    "\n",
    "        gt_codes[gt_codes<0] = 0\n",
    "        log_norm_prob_xyzi = _sum_rightmost(log_norm_prob_xyzi, 1)         # N_gt * batch_size * n_pixel\n",
    "        total_prob = torch.logsumexp(log_norm_prob_xyzi + torch.gather(log_cat_prob, 1, gt_codes[...,None].expand(-1,-1,log_cat_prob.shape[-1])).transpose(0,1),-1).transpose(0, 1)\n",
    "\n",
    "        total_prob = (total_prob * s_mask).sum(-1)  # s_mask: batch_size * N_gt. Binary mask to remove entries in all samples that have less then N_gt GT emitters.\n",
    "\n",
    "        return count_prob, total_prob\n",
    "\n",
    "def get_sample_mask(bs, locations):\n",
    "    \n",
    "    counts_ = torch.unique(locations[0], return_counts=True)[1]\n",
    "    batch_loc = torch.unique(locations[0])\n",
    "    \n",
    "    counts = torch.cuda.LongTensor(bs).fill_(0)\n",
    "    counts[batch_loc] = counts_\n",
    "    \n",
    "    max_counts = counts.max()\n",
    "    if max_counts==0: max_counts = 1 #if all 0 will return empty matrix of correct size\n",
    "    s_arr = cum_count_per_group(locations[0])\n",
    "    s_mask   = torch.cuda.FloatTensor(bs,max_counts).fill_(0)\n",
    "    s_mask[locations[0],s_arr] = 1   \n",
    "    \n",
    "    return s_mask, s_arr\n",
    "    \n",
    "def get_true_labels(bs, locations, x_os, y_os, z_os, *args):\n",
    "    \n",
    "    s_mask, s_arr = get_sample_mask(bs, locations)\n",
    "    max_counts = s_mask.shape[1]\n",
    "    \n",
    "    x =  x_os + locations[4].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    y =  y_os + locations[3].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    z =  z_os + locations[2].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    \n",
    "    gt_vars = torch.stack([x, y, z] + [item for item in args], dim=1)\n",
    "    gt_list = torch.cuda.FloatTensor(bs,max_counts,gt_vars.shape[1]).fill_(0)\n",
    "    \n",
    "    gt_list[locations[0],s_arr] = gt_vars\n",
    "    return gt_list, s_mask    \n",
    "\n",
    "def get_true_labels_mf(bs, locations, x_os, y_os, z_os, int_ch, codes):\n",
    "    \n",
    "    n_gt = len(x_os)\n",
    "    \n",
    "    s_mask, s_arr = get_sample_mask(bs, locations)\n",
    "    max_counts = s_mask.shape[1]\n",
    "\n",
    "    x =  x_os + locations[-1].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    y =  y_os + locations[-2].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    z =  z_os + locations[-3].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    \n",
    "    loc_idx = torch.arange(n_gt).repeat_interleave(4)\n",
    "    \n",
    "    intensity = int_ch.sum(-1)\n",
    "    \n",
    "    gt_vars = torch.stack([x, y, z, intensity], dim=1)\n",
    "    gt_list = torch.cuda.FloatTensor(bs,max_counts,gt_vars.shape[1]).fill_(0)\n",
    "    gt_list[locations[0],s_arr] = gt_vars\n",
    "    \n",
    "    gt_codes = torch.cuda.LongTensor(bs,max_counts).fill_(0) - 1\n",
    "    gt_codes[locations[0],s_arr] = codes\n",
    "    \n",
    "    return gt_list, gt_codes, s_mask  \n",
    "\n",
    "def grp_range(counts: torch.Tensor):\n",
    "    assert counts.dim() == 1\n",
    "\n",
    "    idx = counts.cumsum(0)\n",
    "    id_arr = torch.ones(idx[-1], dtype=int, device=counts.device)\n",
    "    id_arr[0] = 0\n",
    "    id_arr[idx[:-1]] = -counts[:-1] + 1\n",
    "    return id_arr.cumsum(0)\n",
    "\n",
    "def cum_count_per_group(arr):\n",
    "    \"\"\"\n",
    "    Helper function that returns the cumulative sum per group.\n",
    "    Example:\n",
    "        [0, 0, 0, 1, 2, 2, 0] --> [0, 1, 2, 0, 0, 1, 3]\n",
    "    \"\"\"\n",
    "\n",
    "    if arr.numel() == 0:\n",
    "        return arr\n",
    "\n",
    "    _, cnt = torch.unique(arr, return_counts=True)\n",
    "    return grp_range(cnt)[np.argsort(np.argsort(arr.cpu().numpy(), kind='mergesort'), kind='mergesort')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_out = torch.load('../data/model_batch_output_code_int_p.pt')\n",
    "# sim_vars = torch.load('../data/sim_var_code_int_p.pt')\n",
    "model_out = torch.load('../data/model_batch_output_class2.pt')\n",
    "sim_vars = torch.load('../data/sim_var_code_class2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg = PointProcessGaussian(**model_out)\n",
    "# ppg.log_prob(*sim_vars[:5], n_bits=4, channels=16, min_int_sig=1.0, int_fac=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3467, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_vars[-3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_codes = sim_vars[-1]\n",
    "unique_col = gt_codes.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations, x_offset, y_offset, z_offset, intensities, output_shape, codes = sim_vars\n",
    "batch_size = output_shape[0]\n",
    "xyzi, gt_codes, s_mask = get_true_labels_mf(batch_size, locations, x_offset, y_offset, z_offset, intensities, codes.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3467, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intensities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.4130, 15.3624, 32.3110,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [16.3938, 16.9528, 10.1640,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [10.4113, 13.2815, 20.7141,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [12.1278, 14.0488, 23.7752,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [10.2457, 32.4847, 31.8931,  ..., 18.6050,  8.7929, 22.8368]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyzi[:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppg.log_prob(*sim_vars[:5], sim_vars[-1], n_bits=4, channels=16, loss_option=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1119.2134, -1129.3894, -1136.9423, -1111.9207, -1101.9421, -1414.6497,\n",
       "         -1414.0309], device='cuda:0', grad_fn=<SumBackward1>),\n",
       " tensor([-215629.5781, -227683.1562, -210702.3438, -208619.4375, -220159.3125,\n",
       "               0.0000,       0.0000], device='cuda:0', grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg.log_prob(*sim_vars[:5], sim_vars[-1], n_bits=4, channels=16, loss_option=0, count_mult=0, cat_logits=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ -519.4117,  -507.0196,  -504.9721,  -526.9086,  -496.2515, -1097.6234,\n",
       "         -1097.0420], device='cuda:0', grad_fn=<SubBackward0>),\n",
       " tensor([-218988.3750, -231131.0312, -214165.3906, -211923.3594, -223686.4531,\n",
       "               0.0000,       0.0000], device='cuda:0', grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg.log_prob(*sim_vars[:5], sim_vars[-1], n_bits=4, channels=16, loss_option=1, count_mult=0, cat_logits=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations, x_offset, y_offset, z_offset, intensities = sim_vars[:5]\n",
    "codes = sim_vars[-1]\n",
    "n_bits=4; channels=16; min_int_sig=1.; int_fac=16\n",
    "old_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_option = 1\n",
    "count_mult = 0\n",
    "cat_logits = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_dim = 3 + 1\n",
    "batch_size = ppg.logits.shape[0]\n",
    "\n",
    "xyzi, gt_codes, s_mask = get_true_labels_mf(batch_size, locations, x_offset, y_offset, z_offset, intensities, codes.cuda())\n",
    "\n",
    "P = torch.sigmoid(ppg.logits) \n",
    "\n",
    "if loss_option == 0:\n",
    "    \n",
    "    count_mean = P.sum(dim=[2, 3, 4]).squeeze(-1)\n",
    "    count_var = (P - P ** 2).sum(dim=[2, 3, 4]).squeeze(-1) \n",
    "    count_dist = D.Normal(count_mean, torch.sqrt(count_var))\n",
    "\n",
    "    counts = torch.zeros(count_mean.shape).cuda()\n",
    "    unique_col = [gtc.unique(return_counts=True) for gtc in gt_codes]\n",
    "    for i, ind_c in enumerate(unique_col):\n",
    "        inds, c = ind_c\n",
    "        counts[i, inds[inds>=0]] = c[inds>=0].type(torch.cuda.FloatTensor)\n",
    "\n",
    "    count_prob =  count_dist.log_prob(counts).sum(-1) # * counts\n",
    "    \n",
    "if loss_option == 1:\n",
    "    \n",
    "    count_mean = P.sum(dim=[1, 2, 3, 4]).squeeze(-1)\n",
    "    count_var = (P - P ** 2).sum(dim=[1, 2, 3, 4]).squeeze(-1) \n",
    "    count_dist = D.Normal(count_mean, torch.sqrt(count_var))\n",
    "\n",
    "    counts = s_mask.sum(-1)\n",
    "    count_prob =  count_dist.log_prob(counts).sum(-1) # * counts\n",
    "    \n",
    "if count_mult:\n",
    "        \n",
    "    count_prob = count_prob * counts\n",
    "\n",
    "pix_inds = torch.nonzero(P[:,:1],as_tuple=True)\n",
    "\n",
    "xyzi_mu = ppg.xyzi_mu[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]]\n",
    "xyzi_mu[:,:3] += torch.stack([pix_inds[4],pix_inds[3],pix_inds[2]], 1) + 0.5\n",
    "xyzi_mu = xyzi_mu.reshape(batch_size,-1,gauss_dim)\n",
    "xyzi_sig = ppg.xyzi_sigma[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]].reshape(batch_size,-1,gauss_dim)\n",
    "\n",
    "if cat_logits:\n",
    "    mixture_probs = P / P.sum(dim=[2, 3, 4], keepdim=True)\n",
    "    mix = D.Categorical(mixture_probs[torch.nonzero(P,as_tuple=True)].reshape(batch_size, 141, -1))\n",
    "    mix_logits = mix.logits    \n",
    "else:\n",
    "    mix_logits = ppg.logits[torch.nonzero(P,as_tuple=True)].reshape(batch_size, 141, -1)\n",
    "\n",
    "dist_normal_xyzi = D.Independent(D.Normal(xyzi_mu, xyzi_sig + 0.00001), 1)\n",
    "\n",
    "xyzi_inp = xyzi.transpose(0, 1)[:,:,None,:]          # reshape for log_prob()\n",
    "log_norm_prob_xyzi = dist_normal_xyzi.base_dist.log_prob(xyzi_inp) # N_gt * batch_size * n_pixel * 3\n",
    "\n",
    "if loss_option == 0:\n",
    "    log_cat_prob = torch.log_softmax(mix_logits, -1) # + torch.log(counts+1e-6)[:, None]       # normalized (sum to 1 over pixels) log probs for the categorical dist. of the GMM. batch_size * n_pixels\n",
    "else:\n",
    "    log_cat_prob = torch.log_softmax(mix_logits.view(batch_size, -1), -1).view(mix_logits.shape)\n",
    "\n",
    "gt_codes[gt_codes<0] = 0\n",
    "log_norm_prob_xyzi = _sum_rightmost(log_norm_prob_xyzi, 1)         # N_gt * batch_size * n_pixel\n",
    "total_prob = torch.logsumexp(log_norm_prob_xyzi + torch.gather(log_cat_prob, 1, gt_codes[...,None].expand(-1,-1,log_cat_prob.shape[-1])).transpose(0,1),-1).transpose(0, 1)\n",
    "\n",
    "total_prob = (total_prob * s_mask).sum(-1)  # s_mask: batch_size * N_gt. Binary mask to remove entries in all samples that have less then N_gt GT emitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather(log_cat_prob, 1, gt_codes[...,None].expand(-1,-1,log_cat_prob.shape[-1])).transpose(0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cat_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_codes[2,422]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cat_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra=torch.gather(log_cat_prob, 1, gt_codes[...,None].expand(-1,-1,log_cat_prob.shape[-1])).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(log_cat_prob[2,60] - tra[2,422]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(counts+1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_inp.argsort(-1, descending=True).expand(-1,-1,rep_log_prob.shape[2],-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_inp.argsort(-1, descending=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cat_prob[...,None].expand(-1,-1,channels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_prob_xyz.shape)\n",
    "print(log_mix_prob_xyz.shape)\n",
    "print((log_prob_xyz + log_mix_prob_xyz).shape)\n",
    "print(torch.logsumexp(log_prob_xyz + log_mix_prob_xyz,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_prob_int.shape)\n",
    "print(log_mix_prob_int_x_p.shape)\n",
    "print((log_prob_int + log_mix_prob_int_x_p).shape)\n",
    "print(torch.logsumexp(log_prob_int + log_mix_prob_int_x_p,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob_int.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob_xyz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.logsumexp(log_prob_xyz + log_mix_prob_xyz,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.logsumexp(log_mix_prob_int + log_mix_prob_int_x_p,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob_xyz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_mix.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_fish.engine.point_process import PointProcessUniform\n",
    "point_process = PointProcessUniform(local_rate = torch.ones([2,1,48,48,48])*.001, int_conc=1.0, sim_iters=5, channels=16, n_bits=4)\n",
    "locs_3d, x_os_3d, y_os_3d, z_os_3d, ints_3d, output_shape = point_process.sample(phasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_3d = [l.cuda() for l in locs_3d]\n",
    "# xyzi_true, s_mask = get_true_labels(2, locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda())\n",
    "xyzi_true, s_mask = get_true_labels_mf(2, 4, 16, locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda())\n",
    "print(len(locs_3d[0]))\n",
    "print(s_mask)\n",
    "print(s_mask.sum(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg = PointProcessGaussian(**model_out)\n",
    "gmm_loss = ppg.log_prob(locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda(), n_bits=4, channels=16, min_int_sig=0.1)\n",
    "gmm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg = PointProcessGaussian(**model_out)\n",
    "gmm_loss = ppg.log_prob(locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda(), n_bits=4, channels=16, min_int_sig=0.1)\n",
    "gmm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     gmm_loss = PointProcessGaussian(**model_out).log_prob(locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_models.ipynb.\n",
      "Converted 01_psf.ipynb.\n",
      "Converted 02_microscope.ipynb.\n",
      "Converted 03_noise.ipynb.\n",
      "Converted 04_pointsource.ipynb.\n",
      "Converted 05_gmm_loss.ipynb.\n",
      "Converted 06_plotting.ipynb.\n",
      "Converted 07_file_io.ipynb.\n",
      "Converted 08_dataset.ipynb.\n",
      "Converted 09_output_trafo.ipynb.\n",
      "Converted 10_evaluation.ipynb.\n",
      "Converted 11_emitter_io.ipynb.\n",
      "Converted 12_utils.ipynb.\n",
      "Converted 13_train.ipynb.\n",
      "Converted 15_fit_psf.ipynb.\n",
      "Converted 16_visualization.ipynb.\n",
      "Converted 17_eval_routines.ipynb.\n",
      "Converted 18_predict_funcs.ipynb.\n",
      "Converted 19_MERFISH_routines.ipynb.\n",
      "Converted 20_MERFISH_visualization.ipynb.\n",
      "Converted 22_MERFISH_codenet.ipynb.\n",
      "Converted 23_MERFISH_comparison.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_fish_dev2]",
   "language": "python",
   "name": "conda-env-decode_fish_dev2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
