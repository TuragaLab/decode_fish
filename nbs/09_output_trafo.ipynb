{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp funcs.output_trafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations of sampled data and model output into emitter dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from decode_fish.imports import *\n",
    "from decode_fish.funcs.utils import *\n",
    "import torch.nn.functional as F\n",
    "from decode_fish.funcs.plotting import *\n",
    "from decode_fish.funcs.emitter_io import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# def sample_to_df(locs, x_os, y_os, z_os, ints, px_size_zyx=[100,100,100], channels=16, n_bits=4):\n",
    "    \n",
    "#     x = locs[-1] + x_os + 0.5 \n",
    "#     y = locs[-2] + y_os + 0.5 \n",
    "#     z = locs[-3] + z_os + 0.5 \n",
    "    \n",
    "#     b_inds = torch.cat([torch.tensor([0], device=x_os.device),((x_os[1:] - x_os[:-1]).nonzero() + 1)[:,0], \n",
    "#                         torch.tensor([len(x_os)], device=x_os.device)])\n",
    "#     n_gt = len(b_inds) - 1\n",
    "\n",
    "#     frame_idx = locs[0]\n",
    "#     ch_idx = locs[1]\n",
    "    \n",
    "#     loc_idx = []\n",
    "#     for i in range(n_gt):\n",
    "#         loc_idx += [i] * (b_inds[i+1] - b_inds[i])\n",
    "    \n",
    "#     df = DF({'loc_idx': loc_idx,\n",
    "#              'frame_idx': frame_idx.cpu(),\n",
    "#              'ch_idx': ch_idx.cpu(),\n",
    "#              'x': x.cpu()*px_size_zyx[2],\n",
    "#              'y': y.cpu()*px_size_zyx[1], \n",
    "#              'z': z.cpu()*px_size_zyx[0],\n",
    "#              'int': ints.cpu()}) \n",
    "    \n",
    "#     int_arr = np.zeros([n_gt, channels])\n",
    "#     int_arr[df['loc_idx'], df['ch_idx']] = ints.cpu()\n",
    "    \n",
    "#     df = df.iloc[cpu(b_inds[:-1])]\n",
    "#     for i in range(16):\n",
    "#         df[f'int_{i}'] = int_arr[:,i]\n",
    "\n",
    "#     return df\n",
    "\n",
    "def sample_to_df(locs, x_os, y_os, z_os, ints, codes, px_size_zyx=[100,100,100], channels=16, n_bits=4):\n",
    "    \n",
    "    x = locs[-1] + x_os + 0.5 \n",
    "    y = locs[-2] + y_os + 0.5 \n",
    "    z = locs[-3] + z_os + 0.5 \n",
    "    \n",
    "    b_inds = torch.cat([torch.tensor([0], device=x_os.device),((x_os[1:] - x_os[:-1]).nonzero() + 1)[:,0], \n",
    "                        torch.tensor([len(x_os)], device=x_os.device)])\n",
    "    n_gt = len(b_inds) - 1\n",
    "\n",
    "    frame_idx = locs[0]\n",
    "    ch_idx = locs[1]\n",
    "    \n",
    "    loc_idx = []\n",
    "    for i in range(n_gt):\n",
    "        loc_idx += [i] * (b_inds[i+1] - b_inds[i])\n",
    "    \n",
    "    df = DF({'loc_idx': loc_idx,\n",
    "             'frame_idx': frame_idx.cpu(),\n",
    "             'x': x.cpu()*px_size_zyx[2],\n",
    "             'y': y.cpu()*px_size_zyx[1], \n",
    "             'z': z.cpu()*px_size_zyx[0]}) \n",
    "    \n",
    "    int_arr = np.zeros([n_gt, channels])\n",
    "    int_arr[df['loc_idx'], ch_idx.cpu()] = ints.cpu()\n",
    "    \n",
    "    df = df.iloc[cpu(b_inds)[:-1]]\n",
    "    for i in range(16):\n",
    "        df[f'int_{i}'] = int_arr[:,i]\n",
    "        \n",
    "    df['code_inds'] = codes\n",
    "    df['ints'] = int_arr.sum(-1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def df_to_micro(df, px_size_zyx=[100,100,100]):\n",
    "    \n",
    "    locs = tuple([torch.tensor(df['frame_idx'],dtype=torch.int64).cuda(),\n",
    "                 torch.zeros(len(df),dtype=torch.int64).cuda(),\n",
    "                 torch.tensor((df['z']/px_size_zyx[0] + 0.5),dtype=torch.int64).cuda(),\n",
    "                 torch.tensor((df['y']/px_size_zyx[1] + 0.5),dtype=torch.int64).cuda(),\n",
    "                 torch.tensor((df['x']/px_size_zyx[2] + 0.5),dtype=torch.int64).cuda()])\n",
    "    z = (torch.tensor(df['z'],dtype=torch.float32).cuda()-locs[2]*px_size_zyx[0])/px_size_zyx[0] - 0.5\n",
    "    y = (torch.tensor(df['y'],dtype=torch.float32).cuda()-locs[3]*px_size_zyx[1])/px_size_zyx[1] - 0.5\n",
    "    x = (torch.tensor(df['x'],dtype=torch.float32).cuda()-locs[4]*px_size_zyx[2])/px_size_zyx[2] - 0.5\n",
    "    ints = torch.tensor(df['int']).cuda()\n",
    "\n",
    "    return locs, x, y, z, ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13832\n"
     ]
    }
   ],
   "source": [
    "from decode_fish.funcs.merfish_eval import *\n",
    "bench_df, code_ref, targets = get_benchmark()\n",
    "code_inds = np.stack([np.nonzero(c)[0] for c in code_ref])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/groups/turaga/home/speisera/anaconda3/envs/decode_fish_dev2/lib/python3.7/site-packages/torch/cuda/__init__.py:81: UserWarning: \n",
      "    Found GPU2 NVS 510 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "from decode_fish.engine.point_process import PointProcessUniform\n",
    "point_process = PointProcessUniform(local_rate = torch.ones([7,1,1,48,48]).cuda()*.3, int_conc=3, int_rate=1, int_loc=1, sim_iters=1, channels=16, n_bits=4, codebook=torch.tensor(code_inds), int_option=1)\n",
    "locs_3d, x_os_3d, y_os_3d, z_os_3d, ints_3d, output_shape, codes = point_process.sample(from_code_book=True, phasing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SIPostProcess(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, m1_threshold:float = 0.03, m2_threshold:float = 0.3, samp_threshold=0.1, px_size_zyx=[100,100,100], diag=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.m1_threshold = m1_threshold\n",
    "        self.m2_threshold = m2_threshold\n",
    "        self.samp_threshold = samp_threshold\n",
    "        self.diag = diag\n",
    "        self.px_size_zyx = px_size_zyx\n",
    "        \n",
    "        if not diag:\n",
    "            d1 = 0; d2 = 0\n",
    "        else:\n",
    "            d1 = 1/np.sqrt(2); d2 = 1/np.sqrt(3)\n",
    "#             d1 = 1; d2 = 1\n",
    "        self.filt = torch.FloatTensor([[[d2,d1,d2],[d1,1,d1],[d2,d1,d2]],\n",
    "                                       [[d1, 1,d1],[1, 1, 1],[d1, 1,d1]],\n",
    "                                       [[d2,d1,d2],[d1,1,d1],[d2,d1,d2]]])[None,None]\n",
    "        \n",
    "    def forward(self, logits):\n",
    "\n",
    "        device = logits.device\n",
    "        p = torch.sigmoid(logits)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            p_copy = p + 0\n",
    "\n",
    "            # probability values > threshold are regarded as possible locations\n",
    "            p_clip = torch.where(p>self.m1_threshold,p,torch.zeros_like(p))\n",
    "\n",
    "            # localize maximum values within a 3x3 patch\n",
    "            pool = F.max_pool3d(p_clip,3,1,padding=1)\n",
    "            max_mask1 = torch.eq(p, pool).float()\n",
    "\n",
    "            # Add probability values from the 4 adjacent pixels\n",
    "            conv = F.conv3d(p, self.filt.to(device) ,padding=1)\n",
    "            p_ps1 = (max_mask1 * conv)\n",
    "\n",
    "            # In order do be able to identify two fluorophores in adjacent pixels we look for probablity values > 0.5 that are not part of the first mask\n",
    "\n",
    "            p_copy *= (1-max_mask1)\n",
    "            p_clip = torch.where(p_copy>self.m2_threshold, p_copy,torch.zeros_like(p_copy))\n",
    "            max_mask2 = torch.where(p_copy>self.m2_threshold, torch.ones_like(p_copy),torch.zeros_like(p_copy))\n",
    "            p_ps2 = max_mask2*conv\n",
    "\n",
    "            # This is our final clustered probablity which we then threshold (normally > 0.7) to get our final discrete locations \n",
    "            p_ps = p_ps1 + p_ps2        \n",
    "\n",
    "            return p_ps\n",
    "        \n",
    "    def get_si_resdict(self, res_dict, p_si=None):\n",
    "        \n",
    "        if p_si is None:\n",
    "            p_si = self.forward(res_dict['logits'])\n",
    "            \n",
    "        res_dict['Probs_si'] = p_si\n",
    "        res_dict['Samples_si'] = torch.where(res_dict['Probs_si'] > self.samp_threshold, torch.ones_like(res_dict['Probs_si']), torch.zeros_like(res_dict['Probs_si']))\n",
    "        \n",
    "        return res_dict\n",
    "        \n",
    "    def get_df(self, res_dict, p_si=None, softmax=False):\n",
    "        \n",
    "        res_dict = self.get_si_resdict(res_dict, p_si)\n",
    "        \n",
    "        res_dict = {k:v.cpu() for (k,v) in res_dict.items()}\n",
    "        locations = res_dict['Samples_si'].nonzero(as_tuple=True)\n",
    "        ch0_locs = locations[0], locations[1]*0, locations[2] ,locations[3], locations[4]\n",
    "\n",
    "        pos_x, pos_y, pos_z = locations[-1] ,locations[-2], locations[-3]\n",
    "        \n",
    "        x = pos_x + res_dict['xyzi_mu'][:,[0]][ch0_locs] + 0.5 \n",
    "        y = pos_y + res_dict['xyzi_mu'][:,[1]][ch0_locs] + 0.5 \n",
    "        z = pos_z + res_dict['xyzi_mu'][:,[2]][ch0_locs]\n",
    "\n",
    "        loc_idx = torch.arange(len(x))\n",
    "        frame_idx = locations[0]\n",
    "\n",
    "        df = DF({'loc_idx': loc_idx,\n",
    "                 'frame_idx': frame_idx,\n",
    "                 'code_inds': locations[1],\n",
    "                 'x': x*self.px_size_zyx[2],\n",
    "                 'y': y*self.px_size_zyx[1], \n",
    "                 'z': z*self.px_size_zyx[0], \n",
    "                 'prob': res_dict['Probs_si'][locations], \n",
    "                 'int': res_dict['xyzi_mu'][:,[3]][ch0_locs], \n",
    "                 'int_sig': res_dict['xyzi_sigma'][:,[3]][ch0_locs], \n",
    "                 'x_sig': res_dict['xyzi_sigma'][:,[0]][ch0_locs]*self.px_size_zyx[0], \n",
    "                 'y_sig': res_dict['xyzi_sigma'][:,[1]][ch0_locs]*self.px_size_zyx[1], \n",
    "                 'z_sig': res_dict['xyzi_sigma'][:,[2]][ch0_locs]*self.px_size_zyx[2],\n",
    "                 'comb_sig': torch.sqrt(res_dict['xyzi_sigma'][:,[0]][ch0_locs]**2\n",
    "                                       +res_dict['xyzi_sigma'][:,[1]][ch0_locs]**2\n",
    "                                       +res_dict['xyzi_sigma'][:,[2]][ch0_locs]**2)})\n",
    "\n",
    "        return df\n",
    "        \n",
    "    def get_micro_inp(self, res_dict, code_inds, p_si=None, n_bits = 4, channel=0):\n",
    "\n",
    "        res_dict = self.get_si_resdict(res_dict, p_si)\n",
    "        locations = res_dict['Samples_si'].nonzero(as_tuple=True)\n",
    "\n",
    "        xyzi_ix = [locations[0],locations[2],locations[3], locations[4]]\n",
    "        x_os_3d = res_dict['xyzi_mu'][:,0][xyzi_ix]\n",
    "        y_os_3d = res_dict['xyzi_mu'][:,1][xyzi_ix]\n",
    "        z_os_3d = res_dict['xyzi_mu'][:,2][xyzi_ix]\n",
    "        ints_3d = res_dict['xyzi_mu'][:,3][xyzi_ix]\n",
    "        # output_shape  = res_dict['Samples_si'].shape\n",
    "        x_os_3d = x_os_3d.repeat_interleave(n_bits)\n",
    "        y_os_3d = y_os_3d.repeat_interleave(n_bits)\n",
    "        z_os_3d = z_os_3d.repeat_interleave(n_bits)\n",
    "        ints_3d = ints_3d.repeat_interleave(n_bits)/n_bits\n",
    "\n",
    "        locations = [locations[0].repeat_interleave(n_bits),\n",
    "                     torch.tensor(code_inds)[locations[1]].reshape(-1),\n",
    "                     locations[2].repeat_interleave(n_bits),\n",
    "                     locations[3].repeat_interleave(n_bits), \n",
    "                     locations[4].repeat_interleave(n_bits)]\n",
    "\n",
    "        output_shape  = res_dict['Samples_si'].shape\n",
    "        output_shape  = torch.Size([output_shape[0],code_inds.max()+1,output_shape[2],output_shape[3],output_shape[4]])\n",
    "\n",
    "        return locations, x_os_3d, y_os_3d, z_os_3d, ints_3d, output_shape\n",
    "# p_col = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# class ISIPostProcess(SIPostProcess):\n",
    "    \n",
    "#     def __init__(self, m1_threshold:float = 0.1, samp_threshold=0.1, px_size_zyx=[100,100,100], diag=False):\n",
    "        \n",
    "#         super().__init__(m1_threshold = m1_threshold, samp_threshold=samp_threshold, px_size_zyx=px_size_zyx, diag=diag)\n",
    "#         self.m2_threshold = None\n",
    "        \n",
    "#     def forward(self, logits):\n",
    "\n",
    "#         device = logits.device\n",
    "#         count = 0\n",
    "#         p = torch.sigmoid(logits)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "            \n",
    "#             p_ret = 0\n",
    "#             tot_mask = torch.ones_like(p)\n",
    "# #             count_arr = torch.zeros_like(p)\n",
    "            \n",
    "#             while True:\n",
    "                \n",
    "#                 count += 1\n",
    "\n",
    "#                 # probability values > threshold are regarded as possible locations\n",
    "#                 p_clip = torch.where(p>self.m1_threshold,p,torch.zeros_like(p))*tot_mask\n",
    "\n",
    "#                 # localize maximum values within a 3x3 patch\n",
    "#                 pool = F.max_pool3d(p_clip,3,1,padding=1)\n",
    "#                 max_mask1 = torch.eq(p, pool).float()\n",
    "#                 max_mask1[p==0] = 0\n",
    "                \n",
    "# #                 count_arr += max_mask1*count\n",
    "                \n",
    "#                 tot_mask *= (torch.ones_like(max_mask1) - max_mask1)\n",
    "                \n",
    "#                 # Add probability values from the adjacent pixels\n",
    "#                 conv = F.conv3d(p, self.filt.to(device).expand(-1,p.shape[1],-1,-1,-1) ,padding=1)\n",
    "#                 p_ps = max_mask1 * conv\n",
    "                \n",
    "#                 p_ret += torch.clamp_max(p_ps, 1) \n",
    "                \n",
    "#                 p_fac = 1/p_ps\n",
    "#                 p_fac[torch.isinf(p_fac)] = 0\n",
    "#                 p_fac = torch.clamp_max(p_fac, 1) \n",
    "#                 p_proc = F.conv3d(p_fac, self.filt.to(device),padding=1)*p\n",
    "\n",
    "#                 p = p - p_proc\n",
    "#                 torch.clamp_min_(p, 0)\n",
    "                \n",
    "#                 if not max_mask1.sum():\n",
    "#                     break\n",
    "            \n",
    "#             return p_ret #, count_arr\n",
    "        \n",
    "# #                 plt.figure(figsize=(20,5))\n",
    "# #                 plt.subplot(141)\n",
    "# #                 plt.imshow(cpu(p[0,0][sl[1:]]).sum(0))\n",
    "# #                 plt.subplot(142)\n",
    "# #                 plt.imshow(cpu(p_ps[0,0][sl[1:]]).sum(0))\n",
    "# #                 plt.title(cpu(p_ps[0,0][sl[1:]]).sum())\n",
    "# #                 plt.colorbar()\n",
    "# #                 plt.subplot(143)\n",
    "# #                 plt.imshow(cpu(p_proc[0,0][sl[1:]]).sum(0))\n",
    "# #                 plt.title(cpu(p_proc[0,0][sl[1:]]).sum())\n",
    "# #                 plt.colorbar()\n",
    "# #                 plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ISIPostProcess(SIPostProcess):\n",
    "    \n",
    "    def __init__(self, m1_threshold:float = 0.1, samp_threshold=0.1, px_size_zyx=[100,100,100], diag=False):\n",
    "        \n",
    "        super().__init__(m1_threshold = m1_threshold, samp_threshold=samp_threshold, px_size_zyx=px_size_zyx, diag=diag)\n",
    "        self.m2_threshold = None\n",
    "        \n",
    "    def forward(self, logits):\n",
    "\n",
    "        device = logits.device\n",
    "        p = torch.sigmoid(logits)\n",
    "        \n",
    "        batch_size = p.shape[0]\n",
    "        n_codes = p.shape[1]\n",
    "        \n",
    "        p = p.reshape(batch_size*n_codes,1,*p.shape[-3:])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            p_SI = 0\n",
    "            tot_mask = torch.ones_like(p)\n",
    "            max_mask = torch.ones_like(p)\n",
    "            \n",
    "            while max_mask.sum():\n",
    "                \n",
    "                # voxels with probability values > threshold,\n",
    "                # and which where not previously counted as locations, are canditates\n",
    "                p_cand = torch.where(p>self.m1_threshold, p, torch.zeros_like(p)) * tot_mask\n",
    "\n",
    "                # localize maximum (nonzero) values within a 3x3x3 volume\n",
    "                p_cand = F.max_pool3d(p_cand,3,1,padding=1)\n",
    "                max_mask = torch.eq(p, p_cand).float()\n",
    "                max_mask[p==0] = 0\n",
    "                \n",
    "                # Add up probability values from the adjacent pixels\n",
    "                conv = F.conv3d(p, self.filt.to(device), padding=1)\n",
    "                p_sum = max_mask * conv\n",
    "                \n",
    "                # Add the integrated probabilities to the return tensor. \n",
    "                p_SI += torch.clamp_max(p_sum, 1) \n",
    "                # Voxels that where added can not be added again\n",
    "                tot_mask *= (torch.ones_like(max_mask) - max_mask)\n",
    "                \n",
    "                # The probability mass that contributed to p_sum is removed.\n",
    "                p_fac = 1/p_sum\n",
    "                p_fac[torch.isinf(p_fac)] = 0\n",
    "                p_fac = torch.clamp_max(p_fac, 1) \n",
    "                p_proc = F.conv3d(p_fac, self.filt.to(device),padding=1)*p\n",
    "\n",
    "                p = p - p_proc\n",
    "                torch.clamp_min_(p, 0)\n",
    "            \n",
    "            return p_SI.reshape(batch_size,n_codes,*p.shape[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from decode_fish.funcs.evaluation import *\n",
    "\n",
    "post_proc1 = SIPostProcess(m1_threshold=0.03, m2_threshold=0.25, samp_threshold=0.6, px_size_zyx=[100,100,100], diag=True)\n",
    "post_proc2 = ISIPostProcess(m1_threshold=0.03, samp_threshold=0.5, px_size_zyx=[100,100,100], diag=True)\n",
    "\n",
    "# matching(px_to_nm(gt_df),  post_proc1.forward(model_out, ret='df'), tolerance=500, print_res=True)\n",
    "# _=matching(px_to_nm(gt_df),  post_proc2.forward(model_out, ret='df'), tolerance=500, print_res=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from decode_fish.funcs.utils import *\n",
    "# model_out = torch.load('../data/model_output.pt')\n",
    "# probs_inp = torch.sigmoid(model_out['logits'])[:,:,:,250:300,200:250]\n",
    "\n",
    "# model_out = torch.load('../data/model_batch_output.pt')\n",
    "# probs_inp = torch.sigmoid(model_out['logits'])\n",
    "\n",
    "# model_out = torch.load('../data/model_output_t.pt')\n",
    "# probs_inp = torch.sigmoid(model_out['logits'])[:,:,:,:,:]\n",
    "\n",
    "model_out = torch.load('../data/model_output_2d.pt')\n",
    "# probs_inp = torch.sigmoid(model_out['logits'])\n",
    "# model_out = torch.load('../data/model_batch_output_class.pt')\n",
    "# for m in model_out:\n",
    "#     model_out[m] =model_out[m].detach() \n",
    "# gt_df = torch.load('../data/gt_1.pt')\n",
    "# len(gt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54134/1113807055.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_2d' is not defined"
     ]
    }
   ],
   "source": [
    "model_out['logits'] = model_2d['logits'].expand(-1,140,-1,-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra1 = post_proc2(model_out['logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra2 = post_proc2(model_out['logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cpu(tra2)[0,0,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_proc2.get_df(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs_inp = probs_inp[:,:,10:11]\n",
    "# for k in model_out:\n",
    "#     model_out[k] = model_out[k][:,:,10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# print(len(gt_df))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(231)\n",
    "probs = cpu(probs_inp[0,0])\n",
    "probsf = probs + 0\n",
    "probsf[probsf<0.01] = 0\n",
    "im = plt.imshow(probs.sum(0))\n",
    "# plt.scatter(gt_df['x'],gt_df['y'], color='red', s=5.)\n",
    "plt.title(f'Net output {probs.sum().item():.1f} and {probsf.sum().item():.1f}'.format())\n",
    "add_colorbar(im)\n",
    "\n",
    "recs = post_proc1.get_si_resdict(model_out)\n",
    "plt.subplot(232)\n",
    "im = plt.imshow(cpu(recs['Probs_si'][0,0]).max(0))\n",
    "add_colorbar(im)\n",
    "N = cpu(recs['Probs_si'][0,0]).sum().item()\n",
    "plt.title(f'SI Probs SI {N:.1f}')\n",
    "\n",
    "plt.subplot(235)\n",
    "im = plt.imshow(cpu(recs['Samples_si'][0,0]).sum(0))\n",
    "add_colorbar(im)\n",
    "plt.title(cpu(recs['Samples_si'][0,0]).sum().item())\n",
    "\n",
    "recs = post_proc2.get_si_resdict(model_out)\n",
    "plt.subplot(233)\n",
    "im = plt.imshow(cpu(recs['Probs_si'][0,0]).max(0))\n",
    "add_colorbar(im)\n",
    "N = cpu(recs['Probs_si'][0,0]).sum().item()\n",
    "plt.title(f'ISI Probs SI {N:.1f}')\n",
    "\n",
    "plt.subplot(236)\n",
    "im = plt.imshow(cpu(recs['Samples_si'][0,0]).sum(0))\n",
    "add_colorbar(im)\n",
    "plt.title(cpu(recs['Samples_si'][0,0]).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = np.s_[:,:10,35:45,20:30]\n",
    "gt_sub = crop_df(gt_df, sl)\n",
    "p_sub = crop_df(nm_to_px(post_proc2.forward(model_out, ret='df')), sl)\n",
    "axes=plot_3d_projections(probs[sl[1:]], 'max', size=15)\n",
    "# print(probs[sl[1:]].sum(), len(gt_sub), len(p_sub))\n",
    "# axes[0].scatter(gt_sub['x'],gt_sub['y'], color='red', s=5.)\n",
    "# axes[1].scatter(gt_sub['x'],gt_sub['z'], color='red', s=5.)\n",
    "# axes[2].scatter(gt_sub['y'],gt_sub['z'], color='red', s=5.)\n",
    "\n",
    "axes[0].scatter(p_sub['x'],p_sub['y'], color='red', s=15.)\n",
    "axes[1].scatter(p_sub['x'],p_sub['z'], color='red', s=15.)\n",
    "axes[2].scatter(p_sub['y'],p_sub['z'], color='red', s=15.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in p_col:\n",
    "    plt.imshow(cpu(p[0,0][sl[1:]]).max(0))\n",
    "#     plt.title(cpu(p[0,0][sl[1:]]).sum())\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cpu(p[0,0][sl[1:]]).max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(probs.reshape(-1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_si = post_proc.spatial_integration(probs_inp)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "probs = probs_inp[0,0].detach().cpu()\n",
    "# probs[probs<0.01] = 0\n",
    "im = plt.imshow(probs.max(dim=0).values)\n",
    "plt.title(probs.sum().item())\n",
    "add_colorbar(im)\n",
    "plt.subplot(122)\n",
    "im = plt.imshow(probs_si[0,0].cpu().max(dim=0).values, vmax=1)\n",
    "add_colorbar(im)\n",
    "plt.title(probs_si[0].sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = torch.load('../data/model_output_1.pt')\n",
    "out_df = post_proc2(model_out)\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(121)\n",
    "im = plt.imshow(probs_inp[0,0].cpu().max(dim=0).values)\n",
    "add_colorbar(im)\n",
    "plt.title(len(out_df))\n",
    "plt.scatter(out_df['x']/100,out_df['y']/100, color='red', s=5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = torch.load('../data/model_batch_output.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_fish.engine.psf import LinearInterpolatedPSF\n",
    "from decode_fish.engine.noise import sCMOS\n",
    "from decode_fish.engine.point_process import PointProcessUniform\n",
    "from decode_fish.funcs.plotting import plot_3d_projections\n",
    "from decode_fish.engine.microscope import Microscope\n",
    "\n",
    "psf_state = torch.load('/groups/turaga/home/speisera/Mackebox/Artur/WorkDB/deepstorm/fishcod/simfish_psf.pkl')\n",
    "_,xs,ys,zs = psf_state['psf_volume'].shape\n",
    "psf = LinearInterpolatedPSF(fs_x=xs, fs_y=ys, fs_z=zs, upsample_factor= 1)\n",
    "psf.load_state_dict(psf_state)\n",
    "\n",
    "noise = sCMOS()\n",
    "\n",
    "micro = Microscope(parametric_psf=[psf], noise=noise, multipl=10000).cuda()\n",
    "\n",
    "point_process = PointProcessUniform(local_rate = torch.ones([1,1,48,48,48]).cuda()*.0001, min_int = 0.5)\n",
    "locs_3d, x_os_3d, y_os_3d, z_os_3d, ints_3d, output_shape = point_process.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsim = micro(locs_3d, x_os_3d, y_os_3d, z_os_3d, ints_3d, output_shape)\n",
    "xrec = micro(locs_mod, x_os_mod, y_os_mod, z_os_mod, ints_mod, output_shape_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_projections(xsim[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_projections(xrec[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_models.ipynb.\n",
      "Converted 01_psf.ipynb.\n",
      "Converted 02_microscope.ipynb.\n",
      "Converted 03_noise.ipynb.\n",
      "Converted 04_pointsource.ipynb.\n",
      "Converted 05_gmm_loss.ipynb.\n",
      "Converted 06_plotting.ipynb.\n",
      "Converted 07_file_io.ipynb.\n",
      "Converted 08_dataset.ipynb.\n",
      "Converted 09_output_trafo.ipynb.\n",
      "Converted 10_evaluation.ipynb.\n",
      "Converted 11_emitter_io.ipynb.\n",
      "Converted 12_utils.ipynb.\n",
      "Converted 13_train.ipynb.\n",
      "Converted 15_fit_psf.ipynb.\n",
      "Converted 16_visualization.ipynb.\n",
      "Converted 17_eval_routines.ipynb.\n",
      "Converted 18_predict_funcs.ipynb.\n",
      "Converted 19_MERFISH_routines.ipynb.\n",
      "Converted 20_MERFISH_visualization.ipynb.\n",
      "Converted 22_MERFISH_codenet.ipynb.\n",
      "Converted 23_MERFISH_comparison.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_fish_dev2]",
   "language": "python",
   "name": "conda-env-decode_fish_dev2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
