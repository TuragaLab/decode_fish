{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp funcs.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from decode_fish.imports import *\n",
    "from typing import Union\n",
    "import skimage.measure\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import inspect\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from decode_fish.funcs.utils import *\n",
    "from perlin_numpy import generate_fractal_noise_3d, generate_perlin_noise_3d\n",
    "from torch.nn import functional as F\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DecodeDataset:\n",
    "\n",
    "    def __init__(self, volumes: list,\n",
    "                 dataset_tfms: list, \n",
    "                 rate_tfms: list, \n",
    "                 bg_tfms: list, \n",
    "                 num_iter: int = 5000, \n",
    "                 device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        Basic Dataset\n",
    "\n",
    "        Args:\n",
    "            path (str): [image_path]\n",
    "            dataset_tfms (list): transformation specific to dataset\n",
    "            rate_tfms ([type]): transformation for rate generation\n",
    "            bg_transform ([type]): background transformation\n",
    "            num_iter (int, optional):define lenth of dataset. Defaults to 5000.\n",
    "            device (str, optional): device cpu or gpu]. Defaults to 'cpu'.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.volumes = volumes\n",
    "        self.dataset_tfms = dataset_tfms\n",
    "        self.num_iter = num_iter\n",
    "        self.rate_tfms = rate_tfms\n",
    "        self.bg_tfms = bg_tfms\n",
    "        self.device = device\n",
    "        \n",
    "        print(f'{len(self.volumes)} volumes')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_iter\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        i = random.randint(0,len(self.volumes)-1)\n",
    "        x = self.volumes[i] # Adding dimension here to get to 4.\n",
    "        x = self._compose(x, self.dataset_tfms, ind = i).to(self.device)\n",
    "        local_rate = self._compose(x, self.rate_tfms, ind = i)\n",
    "        background = self._compose(x, self.bg_tfms)\n",
    "        return x, local_rate, background\n",
    "#         return x.to(self.device), local_rate.to(self.device), background.to(self.device)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        print (f'{self.__class__.__name__} Summary:')\n",
    "        print (f'Dataset tfms: {len(self.dataset_tfms)}')\n",
    "        for i in self.dataset_tfms:\n",
    "            print (f'\\n-->')\n",
    "            f\"{i}\"\n",
    "        return ''\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compose(x, list_func, **kwargs):\n",
    "#         if not list_func: list_func.append(lambda x: x)\n",
    "        if not list_func: \n",
    "            return x\n",
    "        for func in list_func:\n",
    "            x = func(x, **kwargs)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_class_signature(self, nms):\n",
    "    \"print class signature\"\n",
    "    mod = inspect.currentframe().f_back.f_locals\n",
    "    for n in nms:\n",
    "        print(f'{n}: {getattr(self,n)}')\n",
    "#export\n",
    "class TransformBase:\n",
    "    '''\n",
    "    All transformations optionally must be inherited from this class for nice\n",
    "    representations and checks if input to given transformations is a tensor\n",
    "\n",
    "    '''\n",
    "    def __repr__(self):\n",
    "        print (f'Transform({self.__class__.__name__})')\n",
    "        name = inspect.signature(self.__class__).parameters.keys()\n",
    "        print_class_signature(self, name)\n",
    "        return ''\n",
    "\n",
    "    def __call__(self, x, **kwargs):\n",
    "        assert isinstance(x, torch.Tensor), f'must be torch.tensor not {type(x)}'\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_device(x):\n",
    "        return getattr(x, 'device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ScaleTensor(TransformBase):\n",
    "    \"\"\"\n",
    "    \\nScales given `torch.Tensor` between `low` and `high`\n",
    "\n",
    "    \\nParameters:\n",
    "    \\n`low`     : lower bound\n",
    "    \\n`high`    : upper bound\n",
    "    \\n`data_min`: max value of data\n",
    "    \\n`data_max`: min value of main data\n",
    "\n",
    "    \\nReturns:\n",
    "    \\nScaled tensor\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, low: float=0., high: float=1.):\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def __call__(self, x, **kwargs) -> torch.Tensor:\n",
    "        super().__call__(x)\n",
    "        ratio = (self.high-self.low) / (x.max()-x.min() + 1)\n",
    "        return ratio * (x - x.min() + 1) + self.low\n",
    "    \n",
    "class ShuffleChannel(TransformBase):\n",
    "\n",
    "    def __call__(self, x, **kwargs) -> torch.Tensor:\n",
    "        super().__call__(x)\n",
    "\n",
    "        return x[torch.randperm(len(x))]\n",
    "    \n",
    "class RandScale(TransformBase):\n",
    "\n",
    "    def __init__(self, low: float, high: float):\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def __call__(self, x, **kwargs) -> torch.Tensor:\n",
    "        super().__call__(x)\n",
    "        ratio = torch.distributions.Uniform(self.low, self.high).sample()\n",
    "        return ratio * x\n",
    "    \n",
    "class UniformValue(TransformBase):\n",
    "    def __init__(self, min_val=0., max_val=0.):\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "    \n",
    "    def __call__(self, image, **kwargs):\n",
    "        rand_val = torch.distributions.Uniform(self.min_val, self.max_val).sample([1]).to(image.device)\n",
    "        return torch.ones_like(image[:1])*rand_val\n",
    "    \n",
    "def get_forward_scaling(img):\n",
    "    \n",
    "    offset = np.median(img)\n",
    "    scale = np.max(np.array(img))/3\n",
    "    return offset, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RandomCrop3D(TransformBase):\n",
    "    \"\"\"\n",
    "    Ramdomly Crops 3D tensor.\n",
    "\n",
    "    \\nThis class will generate random crop of `crop_sz`. This class is initialized\n",
    "    with `img_sz` which should be a demension of 4 [Channel, Height, Width, Depth] and\n",
    "    a `crop_sz` dimesnion of 3 [Height, Width, Depth] of desired crop. For each crop\n",
    "    dimension `_get_slice` function will calculate random int ranging from 0 to (img_sz-crop_sz).\n",
    "    and return tuple of containing two slice intergers. If one dimension of `img_sz` matches\n",
    "    one dimension of `crop_sz` the resulting tuple will be `(None, None)` which will result\n",
    "    in not croping this particular dimension.\n",
    "\n",
    "\n",
    "    \\nParameters:\n",
    "    \\n`crop_sz`    : Size of the 3D crop  `(H, W, D)`\n",
    "\n",
    "    \\nReturns:\n",
    "    \\nCroped 3D image of the given `crop_sz`\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, crop_sz, roi_masks):\n",
    "        assert len(crop_sz) == 3 , f'Lenth of crop_sz should be 3 not {len(crop_sz)}'\n",
    "        self.crop_sz = tuple(crop_sz)\n",
    "        self.crop_prod = crop_sz[0]*crop_sz[1]*crop_sz[2]\n",
    "        self.roi_masks = roi_masks\n",
    "\n",
    "    def __call__(self, x, **kwargs):\n",
    "        ch, h, w, d = x.shape\n",
    "        img_sz  = tuple((h, w, d))\n",
    "        assert (img_sz) >=  self.crop_sz\n",
    "        super().__call__(x, **kwargs)\n",
    "        slice_hwd = [self._get_slice(i, k) for i, k in zip(img_sz, self.crop_sz)]\n",
    "        if 'ind' in kwargs:\n",
    "            while self._crop(self.roi_masks[kwargs['ind']][None], *slice_hwd).sum()/self.crop_prod < 0.5:\n",
    "                slice_hwd = [self._get_slice(i, k) for i, k in zip(img_sz, self.crop_sz)]\n",
    "        return self._crop(x, *slice_hwd)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_slice(sz, crop_sz):\n",
    "        up_bound = sz-crop_sz\n",
    "        if  up_bound == 0:\n",
    "            return None, None\n",
    "        else:\n",
    "            l_bound = torch.randint(up_bound, (1,))\n",
    "        return l_bound, l_bound + crop_sz\n",
    "\n",
    "    @staticmethod\n",
    "    def _crop(x, slice_h, slice_w, slice_d):\n",
    "        return x[:, slice_h[0]:slice_h[1], slice_w[0]:slice_w[1], slice_d[0]:slice_d[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AddFoci(TransformBase):\n",
    "    \n",
    "    def __init__(self, n_foci_avg: float, rad_range: tuple, n_mol_range: tuple, px_size_zyx: tuple=(100,100,100), mode='gaussian'):\n",
    "        \n",
    "        self.n_foci_avg = n_foci_avg\n",
    "        self.rad_range = rad_range\n",
    "        self.n_mol_range = n_mol_range\n",
    "        self.px_size_zyx = px_size_zyx\n",
    "        self.mode = mode\n",
    "        \n",
    "    def binary_sphere(self, shape, radius, position):\n",
    "        grid = [slice(-x0, dim - x0) for x0, dim in zip(position, shape)]\n",
    "        position = np.ogrid[grid]\n",
    "        arr = np.zeros(shape, dtype=float)\n",
    "        for x_i, semisize in zip(position, radius):\n",
    "            arr += (x_i / semisize) ** 2\n",
    "\n",
    "        return arr <= 1.0\n",
    "    \n",
    "    def gaussian_sphere(self, shape, radius, position):\n",
    "        grid = [slice(-x0, dim - x0) for x0, dim in zip(position, shape)]\n",
    "        position = np.ogrid[grid]\n",
    "        arr = np.exp(-position[0]**2 / (2 * (radius[0] ** 2))) * np.exp(-position[1]**2 / (2 * (radius[1] ** 2))) * np.exp(-position[2]**2 / (2 * (radius[2] ** 2))) / (2 * np.pi * (radius[0] * radius[1] * radius[2]))\n",
    "        return arr\n",
    "    \n",
    "    def __call__(self, x, **kwargs) -> torch.Tensor:\n",
    "        super().__call__(x)\n",
    "        \n",
    "        prob = self.n_foci_avg/torch.numel(x[0])\n",
    "        locations = torch.distributions.Bernoulli(torch.ones_like(x)*prob).sample()\n",
    "        xwf = x + 0\n",
    "        \n",
    "        for inds in torch.nonzero(locations, as_tuple=False):\n",
    "            rad = torch.distributions.Uniform(*self.rad_range).sample().item()\n",
    "            rads = [rad/pxs for pxs in self.px_size_zyx]\n",
    "            n_mol = torch.distributions.Uniform(*self.n_mol_range).sample().item()\n",
    "            center = inds[-3:].cpu().numpy()\n",
    "            size = list(x.shape[-3:])\n",
    "            if 'gaus' in self.mode:\n",
    "                foci = torch.tensor(self.gaussian_sphere(list(x.shape[-3:]), rads, center))\n",
    "            if 'bin' in self.mode:\n",
    "                foci = torch.tensor(self.binary_sphere(list(x.shape[-3:]), rads, center))\n",
    "            foci = foci * (n_mol / foci.sum())\n",
    "            xwf[inds[0]] += foci.to(x.device)\n",
    "        \n",
    "        return xwf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class torch_gaussian_filter(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing on a\n",
    "    1d, 2d or 3d tensor. Filtering is performed seperately for each channel\n",
    "    in the input using a depthwise convolution.\n",
    "    Arguments:\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "        dim (int, optional): The number of dimensions of the data.\n",
    "            Default value is 2 (spatial).\n",
    "    \"\"\"\n",
    "    def __init__(self,  kernel_size, sigma, dim=2):\n",
    "        super(torch_gaussian_filter, self).__init__()\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size] * dim\n",
    "        if isinstance(sigma, numbers.Number):\n",
    "            sigma = [sigma] * dim\n",
    "\n",
    "        # The gaussian kernel is the product of the\n",
    "        # gaussian function of each dimension.\n",
    "        kernel = 1\n",
    "        self.kernel_size = kernel_size\n",
    "        meshgrids = torch.meshgrid(\n",
    "            [\n",
    "                torch.arange(size, dtype=torch.float32)\n",
    "                for size in kernel_size\n",
    "            ]\n",
    "        )\n",
    "        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
    "            mean = (size - 1) / 2\n",
    "            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
    "                      torch.exp(-((mgrid - mean) / std) ** 2 / 2)\n",
    "\n",
    "        # Make sure sum of values in gaussian kernel equals 1.\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "\n",
    "        # Reshape to depthwise convolutional weight\n",
    "        kernel = kernel.view(1, 1, *kernel.size())\n",
    "        kernel = kernel.repeat(1, *[1] * (kernel.dim() - 1))\n",
    "\n",
    "        self.conv = torch.nn.Conv3d(1, 1, kernel_size=kernel_size, padding=[k//2 for k in kernel_size], padding_mode='replicate', bias=0)\n",
    "        self.conv.weight = torch.nn.Parameter(kernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply gaussian filter to input.\n",
    "        Arguments:\n",
    "            x (torch.Tensor): Input to apply gaussian filter on.\n",
    "        Returns:\n",
    "            filtered (torch.Tensor): Filtered output.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            shape = x.shape\n",
    "            out = self.conv(x.reshape(-1,1,*shape[-3:])).reshape(shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_uneven(x):\n",
    "    return int(2*np.ceil(x/2)-1)\n",
    "\n",
    "class GaussianSmoothing(TransformBase):\n",
    "    def __init__(self, smoothing_filter_size, div_factor=1, device='cuda', z_size=48):\n",
    "        self.smoothing_filter_size = smoothing_filter_size\n",
    "        self.div_factor = div_factor\n",
    "        self.gaussian_filter = torch_gaussian_filter(kernel_size=[get_uneven(np.min([z_size, smoothing_filter_size])), \n",
    "                                                                  smoothing_filter_size*2-1, \n",
    "                                                                  smoothing_filter_size*2-1], \n",
    "                                                     sigma=smoothing_filter_size, dim=3).to(device)\n",
    "    \n",
    "    def __call__(self, image, **kwargs):\n",
    "        background = self.gaussian_filter(image[None])/self.div_factor\n",
    "        # Scipy version, to slow\n",
    "        # background = gaussian_filter(image, self.smoothing_filter_size)/self.div_factor  \n",
    "        # background.clamp_min_(1.)\n",
    "        return background[0]\n",
    "    \n",
    "class AddPerlinNoise(TransformBase):\n",
    "    def __init__(self, shape, res, octaves, scale=1, persistence=0.5, lacunarity=2):\n",
    "        self.shape = shape\n",
    "        self.res = res\n",
    "        self.scale = scale\n",
    "        self.octaves = octaves\n",
    "        self.persistence = persistence\n",
    "        self.lacunarity = lacunarity\n",
    "    \n",
    "    def __call__(self, image, **kwargs):\n",
    "        \n",
    "        assert all(i <= self.shape for i in image.shape[-3:])\n",
    "        shape = [self.shape,self.shape,self.shape]\n",
    "        res = self.res\n",
    "        if image.shape[-3] == 1:\n",
    "            shape[0] = 4\n",
    "            res[0] = 1\n",
    "            \n",
    "        bs = len(image)\n",
    "        fractal_noise = [generate_fractal_noise_3d_torch(shape=shape, \n",
    "                                                  res=res, octaves=self.octaves, persistence=self.persistence, device=image.device) for _ in range(bs)]\n",
    "        fractal_noise = torch.stack(fractal_noise)\n",
    "        fractal_noise = fractal_noise[:,:image.shape[-3],:image.shape[-2],:image.shape[-1]]\n",
    "        return image + self.scale*fractal_noise\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = generate_fractal_noise_3d(\n",
    "#     (48, 48, 48), (4, 8, 8), 2, tileable=(False, False, False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_roi_mask(img, pool_size=(10,10,10), percentile=50):\n",
    "    img = img.mean(0)\n",
    "    mp_img = skimage.measure.block_reduce(img, pool_size, np.max)\n",
    "    thresh = np.percentile(mp_img,percentile)\n",
    "    mp_img = np.where(mp_img>thresh,1,0)\n",
    "    mp_img = np.kron(mp_img, np.ones((pool_size), dtype=mp_img.dtype))\n",
    "    mp_img = mp_img[:img.shape[0], :img.shape[1], :img.shape[2]] \n",
    "    return mp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/groups/turaga/home/speisera/anaconda3/envs/decode2_dev/lib/python3.7/site-packages/torch/cuda/__init__.py:81: UserWarning: \n",
      "    Found GPU2 NVS 510 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "from decode_fish.funcs.file_io import load_psf_noise_micro\n",
    "from decode_fish.engine.point_process import PointProcessUniform\n",
    "from decode_fish.funcs.plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = '/groups/turaga/home/speisera/share_TUM/FishSIM/sim_density_fac1_1/mRNAlevel_500/random/NR/w1_HelaKyoto_Gapdh_2597_p01_cy3__Cell_CP_10__random__1.tif'\n",
    "pdir = '/groups/turaga/home/speisera/share_TUM/FishSIM/sim_density_fac1_1/mRNAlevel_500/random/NR/w1*.tif'\n",
    "pdir = '/groups/turaga/home/speisera/share_TUM/FishSIM/sim_foci_fac1_1/mRNAlevel_500/foci/strong/w1*.tif'\n",
    "pdir = '/groups/turaga/home/speisera/Mackebox/Artur/WorkDB/deepstorm//datasets/CodFish/MERFISH/starfish_scaled/img_stack.tiff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_3d        = [load_tiff_image(f) for f in glob.glob(pdir)]\n",
    "smoothing      = GaussianSmoothing(smoothing_filter_size=5, div_factor=1, z_size=33)\n",
    "roi_masks      = [get_roi_mask(img, pool_size=(10,10,10), percentile=70) for img in imgs_3d]\n",
    "rand_crop      = RandomCrop3D((1,48,48), roi_masks)\n",
    "shuffle_ch     = ShuffleChannel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probmap_generator = UniformValue(0.0001, 0.0005)\n",
    "\n",
    "focifier = AddFoci(n_foci_avg=5, \n",
    "                   rad_range=(100,500), \n",
    "                   n_mol_range=(5.,30.), \n",
    "                   px_size_zyx =(100,100,100),\n",
    "                   mode='bin')\n",
    "\n",
    "fractal_noise = AddPerlinNoise(48, [3,6,6], 3, scale=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 volumes\n"
     ]
    }
   ],
   "source": [
    "ds = DecodeDataset(volumes = imgs_3d,\n",
    "                   dataset_tfms =  [shuffle_ch, rand_crop], \n",
    "                   rate_tfms = [probmap_generator, focifier], \n",
    "                   bg_tfms = [smoothing],#,fractal_noise], \n",
    "                   device='cuda:0', \n",
    "                   num_iter=100 * 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_dl = DataLoader(ds, batch_size=10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.78 GiB total capacity; 7.81 GiB already allocated; 327.75 MiB free; 9.06 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2e37c35ac5d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print(x.mean())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/decode2_dev/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/decode2_dev/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/decode2_dev/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/decode2_dev/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/decode2_dev/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/decode2_dev/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.78 GiB total capacity; 7.81 GiB already allocated; 327.75 MiB free; 9.06 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x, local_rate, background = next(iter(decode_dl))\n",
    "#     print(x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_3d_projections(generate_fractal_noise_3d(shape=(48,48,48), res=(3,6,6), octaves=3, tileable=(False, False, False), persistence=.75, lacunarity=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, local_rate, background = next(iter(decode_dl))\n",
    "plot_3d_projections(local_rate[2,0], proj_func=np.max)\n",
    "plot_3d_projections(background[2,0], proj_func=np.max)\n",
    "# plot_3d_projections(x[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode2_dev]",
   "language": "python",
   "name": "conda-env-decode2_dev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
