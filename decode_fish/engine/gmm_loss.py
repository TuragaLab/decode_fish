# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/05_gmm_loss.ipynb (unless otherwise specified).

__all__ = ['ext_log_prob', 'PointProcessGaussian', 'get_sample_mask', 'get_true_labels', 'get_true_labels_mf',
           'grp_range', 'cum_count_per_group']

# Cell
from ..imports import *
from torch import distributions as D, Tensor
from torch.distributions import Distribution
from torch.distributions.utils import _sum_rightmost
from einops import rearrange
import torch.tensor as T

# Cell
def ext_log_prob(mix, x):

    x = mix._pad(x)
    log_prob_x = mix.component_distribution.base_dist.log_prob(x)
    log_prob_x = _sum_rightmost(log_prob_x, 1)

    log_mix_prob = torch.log_softmax(mix.mixture_distribution.logits, dim=-1)
    return log_prob_x, log_mix_prob

# Cell
class PointProcessGaussian(Distribution):
    def __init__(self, logits, xyzi_mu, xyzi_sigma, int_logits=None, **kwargs):
        """ Defines our loss function. Given logits, xyzi_mu and xyzi_sigma

        The count loss first constructs a Gaussian approximation to the predicted number of emitters by summing the mean and the variance of the Bernoulli detection probability map,
        and then maximizes the probability of the true number of emitters under this distribution.
        The localization loss models the distribution of sub-pixel localizations with a coordinate-wise independent Gaussian probability distribution  with a 3D standard deviation.
        For imprecise localizations, this probability is maximized for large sigmas, for precise localizations for small sigmas.
        The distribution of all localizations over the entire image is approximated as a weighted average of individual localization distributions, where the weights correspond to the probability of detection.

        Args:
            logits: shape (B,1,D,H,W)
            xyzi_mu: shape (B,4,D,H,W)
            xyzi_sigma: shape (B,4,D,H,W)
        """
        self.logits = logits
        self.xyzi_mu = xyzi_mu
        self.xyzi_sigma = xyzi_sigma
        self.int_logits = int_logits

    def log_prob(self, locations, x_offset, y_offset, z_offset, intensities, n_bits, channels, min_int_sig, int_fac=1):

        gauss_dim = 3 + channels
        batch_size = self.logits.shape[0]
        if channels == 1:
            xyzi, s_mask = get_true_labels(batch_size, locations, x_offset, y_offset, z_offset, intensities)
        else:
            xyzi, s_mask = get_true_labels_mf(batch_size, n_bits, channels,
                                              locations, x_offset, y_offset, z_offset, intensities)
        counts = s_mask.sum(-1)

        P = torch.sigmoid(self.logits)
        count_mean = P.sum(dim=[2, 3, 4]).squeeze(-1)
        count_var = (P - P ** 2).sum(dim=[2, 3, 4]).squeeze(-1)
        count_dist = D.Normal(count_mean, torch.sqrt(count_var))

        count_prob =  count_dist.log_prob(counts) # * counts

        pix_inds = torch.nonzero(P,as_tuple=True)

        xyzi_mu = self.xyzi_mu[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]]
        xyzi_mu[:,:3] += torch.stack([pix_inds[4],pix_inds[3],pix_inds[2]], 1) + 0.5
        xyzi_mu = xyzi_mu.reshape(batch_size,-1,gauss_dim)
        xyzi_sig = self.xyzi_sigma[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]].reshape(batch_size,-1,gauss_dim)
        int_P = torch.sigmoid(self.int_logits)[:,:channels]

        int_mean = int_P.sum(dim=[1])
        int_var = (int_P - int_P ** 2).sum(dim=[1])
        int_dist = D.Normal(int_mean, torch.sqrt(int_var))

        int_count_prob = (P[:,0].detach() * int_dist.log_prob(torch.ones_like(int_mean).cuda() * n_bits)).sum(dim=[1,2,3])

        mix_logits = self.logits[pix_inds].reshape(batch_size,-1)
        int_logits = rearrange(self.int_logits[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]], '(b p) ch -> b p ch', b=batch_size)

        '''w4'''
        comp_xyz = D.Independent(D.Normal(xyzi_mu, xyzi_sig + 0.00001), 1)

        xyzi = xyzi.transpose(0, 1)[:,:,None,:]
        log_prob_xyzi = comp_xyz.base_dist.log_prob(xyzi)

        log_cat_prob = torch.log_softmax(mix_logits, -1)
        log_mix_prob_int = torch.log_softmax(int_logits, dim=-1) + torch.log(T(4)).cuda()
        log_mix_prob_xyzi = torch.cat([torch.zeros_like(log_mix_prob_int[...,:3]),log_mix_prob_int], -1)
        ''' '''

        int_bin = torch.where(xyzi != 0, torch.ones_like(xyzi), torch.zeros_like(xyzi))
        log_prob_xyzi[int_bin.expand(-1,-1,log_prob_xyzi.shape[2],-1) == 0] = -10000

        w2_inp = log_prob_xyzi + log_mix_prob_xyzi

        total_prob_int = torch.logsumexp(w2_inp ,-1)
        total_prob_xyz = torch.logsumexp(total_prob_int + log_cat_prob,-1).transpose(0, 1)

        total_prob = ((total_prob_xyz) * s_mask).sum(-1)

        '''split int 2'''
#         xyz_sl = np.s_[:,:,:3]
#         int_sl = np.s_[:,:,3:3+channels]

#         comp_xyz = D.Independent(D.Normal(xyzi_mu[xyz_sl], xyzi_sig[xyz_sl] + 0.00001), 1)
#         comp_int = D.Independent(D.Normal(xyzi_mu[int_sl], xyzi_sig[int_sl] + min_int_sig), 1)

#         xyz = xyzi[xyz_sl].transpose(0, 1)[:,:,None,:]
#         log_prob_xyz = comp_xyz.base_dist.log_prob(xyz)
#         log_prob_xyz = _sum_rightmost(log_prob_xyz, 1)

#         log_mix_prob_xyz = torch.log_softmax(mix_logits, -1)

#         ''' '''
#         int_ch = xyzi[int_sl].transpose(0, 1)[:,:,None,:]
#         int_bin = torch.where(int_ch > 0, torch.ones_like(int_ch), torch.zeros_like(int_ch))-
#         log_prob_int = comp_int.base_dist.log_prob(int_ch)

#         if int_fac:

#             log_mix_prob_int = torch.log_softmax(int_logits, dim=-1) # + torch.log(4*torch.ones(1)).cuda()

#     #         w1_inp = torch.gather((log_prob_int + log_mix_prob_int), -1, int_bin.argsort(-1, descending=True).expand(-1,-1,log_prob_int.shape[2],-1))[...,:4]

#             log_prob_int[int_bin.expand(-1,-1,log_prob_int.shape[2],-1) == 0] = -100
#             w2_inp = log_prob_int + log_mix_prob_int

#             total_prob_int = torch.logsumexp(w2_inp ,-1)
#             total_prob_xyz = torch.logsumexp(log_prob_xyz + log_mix_prob_xyz + total_prob_int,-1).transpose(0, 1)
#         else:
#             total_prob_xyz = torch.logsumexp(log_prob_xyz + log_mix_prob_xyz + _sum_rightmost(log_prob_int, 1),-1).transpose(0, 1) # old loss new format
#             int_count_prob = 0

#         total_prob = ((total_prob_xyz) * s_mask).sum(-1)

        return count_prob + 0.0*int_count_prob, total_prob

    def log_prob_old(self, locations, x_offset, y_offset, z_offset, intensities, n_bits, channels, min_int_sig, int_fac=1):
        """ Creates the distributions for the count and localization loss and evaluates the log probability for the given set of localizations under those distriubtions.

            Args:
                locations: tuple with voxel locations of inferred emitters
                x_offset, y_offset,z_offset: continuous within pixel offsets. Has lenght of number of emitters in the whole batch.
                intensties: brightness of emitters. Has lenght of number of emitters in the whole batch.

            Returns:
                count_prob: count loss. Has langth of batch_size
                spatial_prob: localizations loss. Has langth of batch_size
        """

        gauss_dim = 3 + channels
        batch_size = self.logits.shape[0]
        if channels == 1:
            xyzi, s_mask = get_true_labels(batch_size, locations, x_offset, y_offset, z_offset, intensities)
        else:
            xyzi, s_mask = get_true_labels_mf(batch_size, n_bits, channels,
                                              locations, x_offset, y_offset, z_offset, intensities)
        counts = s_mask.sum(-1)

        P = torch.sigmoid(self.logits)
        count_mean = P.sum(dim=[2, 3, 4]).squeeze(-1)
        count_var = (P - P ** 2).sum(dim=[2, 3, 4]).squeeze(-1)
        count_dist = D.Normal(count_mean, torch.sqrt(count_var))

        count_prob =  count_dist.log_prob(counts) # * counts

        mixture_probs = P / P.sum(dim=[2, 3, 4], keepdim=True)

        pix_inds = torch.nonzero(P,as_tuple=True)

        xyzi_mu = self.xyzi_mu[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]]
        xyzi_mu[:,:3] += torch.stack([pix_inds[4],pix_inds[3],pix_inds[2]], 1) + 0.5
        xyzi_mu = xyzi_mu.reshape(batch_size,-1,gauss_dim)
        xyzi_sig = self.xyzi_sigma[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]].reshape(batch_size,-1,gauss_dim)

#         print(xyzi_mu.shape, xyzi.shape)

        mix = D.Categorical(mixture_probs[pix_inds].reshape(batch_size,-1))

        '''base 19 dim'''
#         xyzi_sig[:,:,3:] = xyzi_sig[:,:,3:] + min_int_sig
#         comp = D.Independent(D.Normal(xyzi_mu, xyzi_sig + 0.00001), 1)
#         spatial_gmm = D.MixtureSameFamily(mix, comp)
#         spatial_prob = spatial_gmm.log_prob(xyzi.transpose(0, 1)).transpose(0,1)
#         total_prob = (spatial_prob * s_mask).sum(-1)
        '''split int'''
        xyz_sl = np.s_[:,:,:3]
        int_sl = np.s_[:,:,3:]

        xyzi_sig[int_sl] = xyzi_sig[int_sl] + min_int_sig

        comp_xyz = D.Independent(D.Normal(xyzi_mu[xyz_sl], xyzi_sig[xyz_sl] + 0.00001), 1)
        comp_int = D.Independent(D.Normal(xyzi_mu[int_sl], xyzi_sig[int_sl] + 0.00001), 1)

        spatial_gmm = D.MixtureSameFamily(mix, comp_xyz)
        int_gmm = D.MixtureSameFamily(mix, comp_int)

        spatial_prob, log_mix_prob = ext_log_prob(spatial_gmm, xyzi[xyz_sl].transpose(0, 1))
        int_prob, _                = ext_log_prob(int_gmm, xyzi[int_sl].transpose(0, 1))

        total_prob = torch.logsumexp(spatial_prob + 0*int_prob + log_mix_prob,-1).transpose(0, 1)
        total_prob = (total_prob * s_mask).sum(-1)

        return count_prob, total_prob

def get_sample_mask(bs, locations):

    counts_ = torch.unique(locations[0], return_counts=True)[1]
    batch_loc = torch.unique(locations[0])

    counts = torch.cuda.LongTensor(bs).fill_(0)
    counts[batch_loc] = counts_

    max_counts = counts.max()
    if max_counts==0: max_counts = 1 #if all 0 will return empty matrix of correct size
    s_arr = cum_count_per_group(locations[0])
    s_mask   = torch.cuda.FloatTensor(bs,max_counts).fill_(0)
    s_mask[locations[0],s_arr] = 1

    return s_mask, s_arr

def get_true_labels(bs, locations, x_os, y_os, z_os, *args):

    s_mask, s_arr = get_sample_mask(bs, locations)
    max_counts = s_mask.shape[1]

    x =  x_os + locations[4].type(torch.cuda.FloatTensor) + 0.5
    y =  y_os + locations[3].type(torch.cuda.FloatTensor) + 0.5
    z =  z_os + locations[2].type(torch.cuda.FloatTensor) + 0.5

    gt_vars = torch.stack([x, y, z] + [item for item in args], dim=1)
    gt_list = torch.cuda.FloatTensor(bs,max_counts,gt_vars.shape[1]).fill_(0)

    gt_list[locations[0],s_arr] = gt_vars
    return gt_list, s_mask

def get_true_labels_mf(bs, n_bits, channels, locations, x_os, y_os, z_os, int_ch):

    b_inds = torch.cat([torch.tensor([0], device=x_os.device),((x_os[1:] - x_os[:-1]).nonzero() + 1)[:,0],
                        torch.tensor([len(x_os)], device=x_os.device)])
    n_gt = len(b_inds) - 1

    xyz_locs = [l[b_inds[:-1]] for l in locations]
    x_os = x_os[b_inds[:-1]]
    y_os = y_os[b_inds[:-1]]
    z_os = z_os[b_inds[:-1]]

    s_mask, s_arr = get_sample_mask(bs, xyz_locs)
    max_counts = s_mask.shape[1]

    x =  x_os + xyz_locs[4].type(torch.cuda.FloatTensor) + 0.5
    y =  y_os + xyz_locs[3].type(torch.cuda.FloatTensor) + 0.5
    z =  z_os + xyz_locs[2].type(torch.cuda.FloatTensor) + 0.5

    loc_idx = []
    for i in range(n_gt):
        loc_idx += [i] * (b_inds[i+1] - b_inds[i])

    intensity = torch.zeros([n_gt, channels]).to(x.device)
    intensity[loc_idx, locations[1]] = int_ch

    gt_vars = torch.stack([x, y, z], dim=1)
    gt_vars = torch.cat([gt_vars, intensity], dim=1)
    gt_list = torch.cuda.FloatTensor(bs,max_counts,gt_vars.shape[1]).fill_(0)

    gt_list[xyz_locs[0],s_arr] = gt_vars
    return gt_list, s_mask

def grp_range(counts: torch.Tensor):
    assert counts.dim() == 1

    idx = counts.cumsum(0)
    id_arr = torch.ones(idx[-1], dtype=int, device=counts.device)
    id_arr[0] = 0
    id_arr[idx[:-1]] = -counts[:-1] + 1
    return id_arr.cumsum(0)

def cum_count_per_group(arr):
    """
    Helper function that returns the cumulative sum per group.
    Example:
        [0, 0, 0, 1, 2, 2, 0] --> [0, 1, 2, 0, 0, 1, 3]
    """

    if arr.numel() == 0:
        return arr

    _, cnt = torch.unique(arr, return_counts=True)
    return grp_range(cnt)[np.argsort(np.argsort(arr.cpu().numpy(), kind='mergesort'), kind='mergesort')]