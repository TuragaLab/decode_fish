{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp engine.gmm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from decode_fish.imports import *\n",
    "from torch import distributions as D, Tensor\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions.utils import _sum_rightmost\n",
    "from einops import rearrange\n",
    "import torch.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ext_log_prob(mix, x):\n",
    "    \n",
    "    x = mix._pad(x)\n",
    "    log_prob_x = mix.component_distribution.base_dist.log_prob(x) \n",
    "    log_prob_x = _sum_rightmost(log_prob_x, 1)\n",
    "    \n",
    "    log_mix_prob = torch.log_softmax(mix.mixture_distribution.logits, dim=-1)  \n",
    "    return log_prob_x, log_mix_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PointProcessGaussian(Distribution):\n",
    "    def __init__(self, logits, xyzi_mu, xyzi_sigma, int_logits=None, **kwargs):\n",
    "        \"\"\" Defines our loss function. Given logits, xyzi_mu and xyzi_sigma \n",
    "        \n",
    "        The count loss first constructs a Gaussian approximation to the predicted number of emitters by summing the mean and the variance of the Bernoulli detection probability map,\n",
    "        and then maximizes the probability of the true number of emitters under this distribution. \n",
    "        The localization loss models the distribution of sub-pixel localizations with a coordinate-wise independent Gaussian probability distribution  with a 3D standard deviation. \n",
    "        For imprecise localizations, this probability is maximized for large sigmas, for precise localizations for small sigmas. \n",
    "        The distribution of all localizations over the entire image is approximated as a weighted average of individual localization distributions, where the weights correspond to the probability of detection.\n",
    "        \n",
    "        Args:\n",
    "            logits: shape (B,1,D,H,W)\n",
    "            xyzi_mu: shape (B,4,D,H,W)\n",
    "            xyzi_sigma: shape (B,4,D,H,W)\n",
    "        \"\"\"\n",
    "        self.logits = logits\n",
    "        self.xyzi_mu = xyzi_mu\n",
    "        self.xyzi_sigma = xyzi_sigma\n",
    "        self.int_logits = int_logits\n",
    "        \n",
    "    def log_prob(self, locations, x_offset, y_offset, z_offset, intensities, n_bits, channels, min_int_sig, int_fac=1, old_loss=False):\n",
    "        \n",
    "        gauss_dim = 3 + channels\n",
    "        batch_size = self.logits.shape[0]\n",
    "        if channels == 1:\n",
    "            xyzi, s_mask = get_true_labels(batch_size, locations, x_offset, y_offset, z_offset, intensities)\n",
    "        else:\n",
    "            xyzi, s_mask = get_true_labels_mf(batch_size, n_bits, channels, \n",
    "                                              locations, x_offset, y_offset, z_offset, intensities)\n",
    "        counts = s_mask.sum(-1)\n",
    "\n",
    "        P = torch.sigmoid(self.logits) \n",
    "        count_mean = P.sum(dim=[2, 3, 4]).squeeze(-1)\n",
    "        count_var = (P - P ** 2).sum(dim=[2, 3, 4]).squeeze(-1) \n",
    "        count_dist = D.Normal(count_mean, torch.sqrt(count_var))\n",
    "\n",
    "        count_prob =  count_dist.log_prob(counts) # * counts\n",
    "\n",
    "        pix_inds = torch.nonzero(P,as_tuple=True)\n",
    "\n",
    "        xyzi_mu = self.xyzi_mu[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]]\n",
    "        xyzi_mu[:,:3] += torch.stack([pix_inds[4],pix_inds[3],pix_inds[2]], 1) + 0.5\n",
    "        xyzi_mu = xyzi_mu.reshape(batch_size,-1,gauss_dim)\n",
    "        xyzi_sig = self.xyzi_sigma[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]].reshape(batch_size,-1,gauss_dim)\n",
    "\n",
    "        int_P = torch.sigmoid(self.int_logits)[:,:channels]\n",
    "\n",
    "        int_mean = int_P.sum(dim=[1])\n",
    "        int_var = (int_P - int_P ** 2).sum(dim=[1])\n",
    "        int_dist = D.Normal(int_mean, torch.sqrt(int_var))\n",
    "\n",
    "        int_count_prob = (P[:,0].detach() * int_dist.log_prob(torch.ones_like(int_mean).cuda() * n_bits)).sum(dim=[1,2,3])\n",
    "\n",
    "        '''split int 2'''\n",
    "        # self.logits: probability output. batch_size * 1 * h * w * d\n",
    "        # self.int_logits: probability output for the 16 channels. batch_size * n_channels * h * w * d\n",
    "\n",
    "        mix_logits = self.logits[pix_inds].reshape(batch_size,-1)                                                                           # batch_size * n_pixels. n_pixels = h * w * d\n",
    "        int_logits = rearrange(self.int_logits[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]], '(b p) ch -> b p ch', b=batch_size)      # batch_size * n_pixels * n_channels. n_pixels = h * w * d\n",
    "\n",
    "        # mix_logits: model output probability. batch_size * n_pixels. n_pixels = h * w * d\n",
    "        # int_logits: model output probability for the 16 channels. batch_size * n_pixels * n_channels\n",
    "        # xyzi_mu: model output (mean), batch_size * n_pixel * 19 : xyz + 16 intensities\n",
    "        # xyzi_sig: model output (sigma), batch_size * n_pixel * 19 \n",
    "\n",
    "        xyz_sl = np.s_[:,:,:3]           # slice selecting xyz\n",
    "        int_sl = np.s_[:,:,3:3+channels] # slice selecting 16 intensities\n",
    "\n",
    "        dist_normal_xyz = D.Independent(D.Normal(xyzi_mu[xyz_sl], xyzi_sig[xyz_sl] + 0.00001), 1)\n",
    "        dist_normal_int = D.Independent(D.Normal(xyzi_mu[int_sl], xyzi_sig[int_sl] + 0.00001), 1) # remove min_int_sig\n",
    "\n",
    "        # xyzi: ground truth position. N_gt * batch_size * 19. N_gt = maximum number of GT emitters in a batch. \n",
    "\n",
    "        xyz_inp = xyzi[xyz_sl].transpose(0, 1)[:,:,None,:]          # reshape for log_prob()\n",
    "        log_norm_prob_xyz = dist_normal_xyz.base_dist.log_prob(xyz_inp) # N_gt * batch_size * n_pixel * 3\n",
    "\n",
    "        log_cat_prob = torch.log_softmax(mix_logits, -1)        # normalized (sum to 1 over pixels) log probs for the categorical dist. of the GMM. batch_size * n_pixels\n",
    "\n",
    "        int_inp = xyzi[int_sl].transpose(0, 1)[:,:,None,:]                # reshape for log_prob()\n",
    "        log_norm_prob_int = dist_normal_int.base_dist.log_prob(int_inp)   # N_gt * batch_size * n_pixel * 16\n",
    "\n",
    "        if old_loss:\n",
    "#             Original loss computing the likelihood of a GMM with 19 dimensional Gaussians. Gives decent results. \n",
    "            log_norm_prob_xyz = _sum_rightmost(log_norm_prob_xyz, 1)         # N_gt * batch_size * n_pixel\n",
    "            total_prob = torch.logsumexp(log_norm_prob_xyz + log_cat_prob + _sum_rightmost(log_norm_prob_int, 1),-1).transpose(0, 1) # logsumexp over pixels. batch_size * N_gt\n",
    "\n",
    "#         if int_fac:\n",
    "#             # New loss. The idea is that by zeroing out the log probabilities of the empty channels the models learns to assign zero probability to them (because the probabilties are normed over the channels)\n",
    "#             # This works but the results are much worse. It seems that the chained logsumexp gives shitty gradients.\n",
    "#             log_norm_prob_xyz = _sum_rightmost(log_norm_prob_xyz, 1)         # N_gt * batch_size * n_pixel\n",
    "#             log_mix_prob_int = torch.log_softmax(int_logits, dim=-1)                             # normalized (sum to 1 over channels) log probs.  batch_size * n_pixels * n_channels\n",
    "#             log_norm_prob_int[int_inp.expand(-1,-1,log_norm_prob_int.shape[2],-1) == 0] = -1000  # Only 4 out of 16 channels contain signal. We zero out the remaining entries.\n",
    "#             total_prob_int = torch.logsumexp(log_norm_prob_int + log_mix_prob_int ,-1)           # logsumexp over pixels. N_gt * batch_size * n_pixels\n",
    "#             total_prob = torch.logsumexp(log_norm_prob_xyz + total_prob_int + log_cat_prob,-1).transpose(0, 1)\n",
    "\n",
    "#         if int_fac:\n",
    "#             log_mix_prob_int = torch.log(torch.sigmoid(int_logits)) \n",
    "\n",
    "        else:\n",
    "            log_mix_prob_int = torch.log_softmax(int_logits, dim=-1) \n",
    "    #             log_norm_prob_int[int_inp.expand(-1,-1,log_norm_prob_int.shape[2],-1) == 0] = 0\n",
    "\n",
    "            rep_log_prob = torch.cat([log_norm_prob_xyz[...,None].expand(-1,-1,-1,-1,16), log_norm_prob_int[:,:,:,None]], 3) # batch_size * n_pixels  * 4 (xyzi) * n_channels\n",
    "            rep_log_prob = rep_log_prob.sum(3)\n",
    "\n",
    "    #             rep_log_prob[int_inp.expand(-1,-1,log_norm_prob_int.shape[2],-1) == 0] = -10000\n",
    "\n",
    "            total_prob = torch.logsumexp(rep_log_prob + log_cat_prob[...,None].expand(-1,-1,channels) + log_mix_prob_int,2).transpose(0, 1)\n",
    "            total_prob = torch.gather(total_prob, -1, int_inp.argsort(-1, descending=True)[:,:,0].transpose(0, 1))[...,:4]\n",
    "\n",
    "            total_prob = total_prob.sum(-1)\n",
    "\n",
    "        # s_mask: batch_size * N_gt. Binary mask to remove entries in all samples that have less then N_gt GT emitters.\n",
    "        total_prob = (total_prob * s_mask).sum(-1)\n",
    "\n",
    "        return count_prob + int_fac*int_count_prob, total_prob\n",
    "    \n",
    "    def log_prob_old(self, locations, x_offset, y_offset, z_offset, intensities, n_bits, channels, min_int_sig, int_fac=1):\n",
    "        \"\"\" Creates the distributions for the count and localization loss and evaluates the log probability for the given set of localizations under those distriubtions.\n",
    "        \n",
    "            Args:\n",
    "                locations: tuple with voxel locations of inferred emitters\n",
    "                x_offset, y_offset,z_offset: continuous within pixel offsets. Has lenght of number of emitters in the whole batch.\n",
    "                intensties: brightness of emitters. Has lenght of number of emitters in the whole batch.\n",
    "                \n",
    "            Returns:\n",
    "                count_prob: count loss. Has langth of batch_size\n",
    "                spatial_prob: localizations loss. Has langth of batch_size\n",
    "        \"\"\"     \n",
    "        \n",
    "        gauss_dim = 3 + channels\n",
    "        batch_size = self.logits.shape[0]\n",
    "        if channels == 1:\n",
    "            xyzi, s_mask = get_true_labels(batch_size, locations, x_offset, y_offset, z_offset, intensities)\n",
    "        else:\n",
    "            xyzi, s_mask = get_true_labels_mf(batch_size, n_bits, channels, \n",
    "                                              locations, x_offset, y_offset, z_offset, intensities)\n",
    "        counts = s_mask.sum(-1)\n",
    "        \n",
    "        P = torch.sigmoid(self.logits) \n",
    "        count_mean = P.sum(dim=[2, 3, 4]).squeeze(-1)\n",
    "        count_var = (P - P ** 2).sum(dim=[2, 3, 4]).squeeze(-1) \n",
    "        count_dist = D.Normal(count_mean, torch.sqrt(count_var))\n",
    "                \n",
    "        count_prob =  count_dist.log_prob(counts) # * counts\n",
    "        \n",
    "        mixture_probs = P / P.sum(dim=[2, 3, 4], keepdim=True)\n",
    "        \n",
    "        pix_inds = torch.nonzero(P,as_tuple=True)\n",
    "\n",
    "        xyzi_mu = self.xyzi_mu[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]]\n",
    "        xyzi_mu[:,:3] += torch.stack([pix_inds[4],pix_inds[3],pix_inds[2]], 1) + 0.5\n",
    "        xyzi_mu = xyzi_mu.reshape(batch_size,-1,gauss_dim)\n",
    "        xyzi_sig = self.xyzi_sigma[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]].reshape(batch_size,-1,gauss_dim)\n",
    "\n",
    "#         print(xyzi_mu.shape, xyzi.shape)\n",
    "        \n",
    "        mix = D.Categorical(mixture_probs[pix_inds].reshape(batch_size,-1))\n",
    "        \n",
    "        '''base 19 dim'''\n",
    "#         xyzi_sig[:,:,3:] = xyzi_sig[:,:,3:] + min_int_sig\n",
    "#         comp = D.Independent(D.Normal(xyzi_mu, xyzi_sig + 0.00001), 1)\n",
    "#         spatial_gmm = D.MixtureSameFamily(mix, comp)\n",
    "#         spatial_prob = spatial_gmm.log_prob(xyzi.transpose(0, 1)).transpose(0,1)\n",
    "#         total_prob = (spatial_prob * s_mask).sum(-1)\n",
    "        '''split int'''\n",
    "        xyz_sl = np.s_[:,:,:3]\n",
    "        int_sl = np.s_[:,:,3:]\n",
    "        \n",
    "        xyzi_sig[int_sl] = xyzi_sig[int_sl] + min_int_sig\n",
    "        \n",
    "        comp_xyz = D.Independent(D.Normal(xyzi_mu[xyz_sl], xyzi_sig[xyz_sl] + 0.00001), 1)\n",
    "        comp_int = D.Independent(D.Normal(xyzi_mu[int_sl], xyzi_sig[int_sl] + 0.00001), 1)\n",
    "        \n",
    "        spatial_gmm = D.MixtureSameFamily(mix, comp_xyz)\n",
    "        int_gmm = D.MixtureSameFamily(mix, comp_int)\n",
    "        \n",
    "        spatial_prob, log_mix_prob = ext_log_prob(spatial_gmm, xyzi[xyz_sl].transpose(0, 1))\n",
    "        int_prob, _                = ext_log_prob(int_gmm, xyzi[int_sl].transpose(0, 1))\n",
    "\n",
    "        total_prob = torch.logsumexp(spatial_prob + int_prob + log_mix_prob,-1).transpose(0, 1)\n",
    "        total_prob = (total_prob * s_mask).sum(-1)\n",
    "        return count_prob, total_prob\n",
    "\n",
    "def get_sample_mask(bs, locations):\n",
    "    \n",
    "    counts_ = torch.unique(locations[0], return_counts=True)[1]\n",
    "    batch_loc = torch.unique(locations[0])\n",
    "    \n",
    "    counts = torch.cuda.LongTensor(bs).fill_(0)\n",
    "    counts[batch_loc] = counts_\n",
    "    \n",
    "    max_counts = counts.max()\n",
    "    if max_counts==0: max_counts = 1 #if all 0 will return empty matrix of correct size\n",
    "    s_arr = cum_count_per_group(locations[0])\n",
    "    s_mask   = torch.cuda.FloatTensor(bs,max_counts).fill_(0)\n",
    "    s_mask[locations[0],s_arr] = 1   \n",
    "    \n",
    "    return s_mask, s_arr\n",
    "    \n",
    "def get_true_labels(bs, locations, x_os, y_os, z_os, *args):\n",
    "    \n",
    "    s_mask, s_arr = get_sample_mask(bs, locations)\n",
    "    max_counts = s_mask.shape[1]\n",
    "    \n",
    "    x =  x_os + locations[4].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    y =  y_os + locations[3].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    z =  z_os + locations[2].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    \n",
    "    gt_vars = torch.stack([x, y, z] + [item for item in args], dim=1)\n",
    "    gt_list = torch.cuda.FloatTensor(bs,max_counts,gt_vars.shape[1]).fill_(0)\n",
    "    \n",
    "    gt_list[locations[0],s_arr] = gt_vars\n",
    "    return gt_list, s_mask    \n",
    "\n",
    "def get_true_labels_mf(bs, n_bits, channels, locations, x_os, y_os, z_os, int_ch):\n",
    "    \n",
    "    b_inds = torch.cat([torch.tensor([0], device=x_os.device),((x_os[1:] - x_os[:-1]).nonzero() + 1)[:,0], \n",
    "                        torch.tensor([len(x_os)], device=x_os.device)])\n",
    "    n_gt = len(b_inds) - 1\n",
    "    \n",
    "    xyz_locs = [l[b_inds[:-1]] for l in locations]\n",
    "    x_os = x_os[b_inds[:-1]]\n",
    "    y_os = y_os[b_inds[:-1]]\n",
    "    z_os = z_os[b_inds[:-1]]\n",
    "    \n",
    "    s_mask, s_arr = get_sample_mask(bs, xyz_locs)\n",
    "    max_counts = s_mask.shape[1]\n",
    "    \n",
    "    x =  x_os + xyz_locs[4].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    y =  y_os + xyz_locs[3].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    z =  z_os + xyz_locs[2].type(torch.cuda.FloatTensor) + 0.5 \n",
    "    \n",
    "    loc_idx = []\n",
    "    for i in range(n_gt):\n",
    "        loc_idx += [i] * (b_inds[i+1] - b_inds[i])\n",
    "    \n",
    "    intensity = torch.zeros([n_gt, channels]).to(x.device)\n",
    "    intensity[loc_idx, locations[1]] = int_ch\n",
    "    \n",
    "    gt_vars = torch.stack([x, y, z], dim=1)\n",
    "    gt_vars = torch.cat([gt_vars, intensity], dim=1)\n",
    "    gt_list = torch.cuda.FloatTensor(bs,max_counts,gt_vars.shape[1]).fill_(0)\n",
    "    \n",
    "    gt_list[xyz_locs[0],s_arr] = gt_vars\n",
    "    return gt_list, s_mask  \n",
    "\n",
    "def grp_range(counts: torch.Tensor):\n",
    "    assert counts.dim() == 1\n",
    "\n",
    "    idx = counts.cumsum(0)\n",
    "    id_arr = torch.ones(idx[-1], dtype=int, device=counts.device)\n",
    "    id_arr[0] = 0\n",
    "    id_arr[idx[:-1]] = -counts[:-1] + 1\n",
    "    return id_arr.cumsum(0)\n",
    "\n",
    "def cum_count_per_group(arr):\n",
    "    \"\"\"\n",
    "    Helper function that returns the cumulative sum per group.\n",
    "    Example:\n",
    "        [0, 0, 0, 1, 2, 2, 0] --> [0, 1, 2, 0, 0, 1, 3]\n",
    "    \"\"\"\n",
    "\n",
    "    if arr.numel() == 0:\n",
    "        return arr\n",
    "\n",
    "    _, cnt = torch.unique(arr, return_counts=True)\n",
    "    return grp_range(cnt)[np.argsort(np.argsort(arr.cpu().numpy(), kind='mergesort'), kind='mergesort')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = torch.load('../data/model_batch_output_code_int_p.pt')\n",
    "sim_vars = torch.load('../data/sim_var_code_int_p.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4427, -0.6439,  0.8149,  1.5036,  1.7690,  1.2149, -1.6208],\n",
       "        device='cuda:0'),\n",
       " tensor([  0.0000, -76.8753,   0.0000, -17.1957, -17.3230,   0.0000,   0.0000],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg = PointProcessGaussian(**model_out)\n",
    "ppg.log_prob(*sim_vars[:5], n_bits=4, channels=16, min_int_sig=1.0, int_fac=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -5.7647,  -8.6536,  -9.5782,  ..., -11.5490, -10.3813,  -6.6444],\n",
      "        [-14.1734, -18.1229, -17.6393,  ..., -17.0651, -17.1649, -13.8440],\n",
      "        [ -4.3772,  -7.3647,  -6.6920,  ...,  -7.2645,  -3.1791,  -2.5484],\n",
      "        ...,\n",
      "        [-16.8773, -20.3177, -20.5776,  ..., -21.1145, -21.4433, -18.3202],\n",
      "        [ -3.8027,  -6.7228,  -6.2081,  ...,  -6.1619,  -6.7072,  -4.2943],\n",
      "        [ -6.7797, -10.4279, -10.5400,  ..., -13.3366,  -5.6428,  -4.0053]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4427, -0.6439,  0.8149,  1.5036,  1.7690,  1.2149, -1.6208],\n",
       "        device='cuda:0'),\n",
       " tensor([   0.0000, -146.1235,    0.0000,  -46.2733,  -47.8197,    0.0000,\n",
       "            0.0000], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg.log_prob(*sim_vars[:5], n_bits=4, channels=16, min_int_sig=1.0, int_fac=0, old_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -5.5903,  -8.4780,  -9.4026,  ..., -11.3734, -10.2057,  -6.4693],\n",
      "        [ -9.8069, -13.7562, -13.2726,  ..., -12.6984, -12.7982,  -9.4775],\n",
      "        [ -4.3763,  -7.3635,  -6.6908,  ...,  -7.2632,  -3.1791,  -2.5495],\n",
      "        ...,\n",
      "        [ -7.5141, -10.9540, -11.2139,  ..., -11.7508, -12.0796,  -8.9566],\n",
      "        [ -3.8025,  -6.7223,  -6.2076,  ...,  -6.1614,  -6.7067,  -4.2940],\n",
      "        [ -6.5509, -10.1974, -10.3095,  ..., -13.1061,  -5.4175,  -3.8014]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4427, -0.6439,  0.8149,  1.5036,  1.7690,  1.2149, -1.6208],\n",
       "        device='cuda:0'),\n",
       " tensor([   0.0000, -141.4240,    0.0000,  -46.2780,  -47.8225,    0.0000,\n",
       "            0.0000], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg.log_prob_old(*sim_vars[:5], n_bits=4, channels=16, min_int_sig=1.0, int_fac=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4427, -0.6439,  0.8149,  1.5036,  1.7690,  1.2149, -1.6208],\n",
       "        device='cuda:0'),\n",
       " tensor([   0.0000, -141.4240,    0.0000,  -46.2780,  -47.8225,    0.0000,\n",
       "            0.0000], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg.log_prob_old(*sim_vars[:5], n_bits=4, channels=16, min_int_sig=1.0, int_fac=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations, x_offset, y_offset, z_offset, intensities = sim_vars[:5]\n",
    "n_bits=4; channels=16; min_int_sig=1.; int_fac=16\n",
    "old_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_dim = 3 + channels\n",
    "batch_size = ppg.logits.shape[0]\n",
    "if channels == 1:\n",
    "    xyzi, s_mask = get_true_labels(batch_size, locations, x_offset, y_offset, z_offset, intensities)\n",
    "else:\n",
    "    xyzi, s_mask = get_true_labels_mf(batch_size, n_bits, channels, \n",
    "                                      locations, x_offset, y_offset, z_offset, intensities)\n",
    "counts = s_mask.sum(-1)\n",
    "\n",
    "P = torch.sigmoid(ppg.logits) \n",
    "count_mean = P.sum(dim=[2, 3, 4]).squeeze(-1)\n",
    "count_var = (P - P ** 2).sum(dim=[2, 3, 4]).squeeze(-1) \n",
    "count_dist = D.Normal(count_mean, torch.sqrt(count_var))\n",
    "\n",
    "count_prob =  count_dist.log_prob(counts) # * counts\n",
    "\n",
    "pix_inds = torch.nonzero(P,as_tuple=True)\n",
    "\n",
    "xyzi_mu = ppg.xyzi_mu[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]]\n",
    "xyzi_mu[:,:3] += torch.stack([pix_inds[4],pix_inds[3],pix_inds[2]], 1) + 0.5\n",
    "xyzi_mu = xyzi_mu.reshape(batch_size,-1,gauss_dim)\n",
    "xyzi_sig = ppg.xyzi_sigma[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]].reshape(batch_size,-1,gauss_dim)\n",
    "\n",
    "int_P = torch.sigmoid(ppg.int_logits)[:,:channels]\n",
    "\n",
    "int_mean = int_P.sum(dim=[1])\n",
    "int_var = (int_P - int_P ** 2).sum(dim=[1])\n",
    "int_dist = D.Normal(int_mean, torch.sqrt(int_var))\n",
    "\n",
    "int_count_prob = (P[:,0].detach() * int_dist.log_prob(torch.ones_like(int_mean).cuda() * n_bits)).sum(dim=[1,2,3])\n",
    "\n",
    "'''split int 2'''\n",
    "# self.logits: probability output. batch_size * 1 * h * w * d\n",
    "# self.int_logits: probability output for the 16 channels. batch_size * n_channels * h * w * d\n",
    "\n",
    "mix_logits = ppg.logits[pix_inds].reshape(batch_size,-1)                                                                           # batch_size * n_pixels. n_pixels = h * w * d\n",
    "int_logits = rearrange(ppg.int_logits[pix_inds[0],:,pix_inds[2],pix_inds[3],pix_inds[4]], '(b p) ch -> b p ch', b=batch_size)      # batch_size * n_pixels * n_channels. n_pixels = h * w * d\n",
    " \n",
    "# mix_logits: model output probability. batch_size * n_pixels. n_pixels = h * w * d\n",
    "# int_logits: model output probability for the 16 channels. batch_size * n_pixels * n_channels\n",
    "# xyzi_mu: model output (mean), batch_size * n_pixel * 19 : xyz + 16 intensities\n",
    "# xyzi_sig: model output (sigma), batch_size * n_pixel * 19 \n",
    "\n",
    "xyz_sl = np.s_[:,:,:3]           # slice selecting xyz\n",
    "int_sl = np.s_[:,:,3:3+channels] # slice selecting 16 intensities\n",
    "\n",
    "dist_normal_xyz = D.Independent(D.Normal(xyzi_mu[xyz_sl], xyzi_sig[xyz_sl] + 0.00001), 1)\n",
    "dist_normal_int = D.Independent(D.Normal(xyzi_mu[int_sl], xyzi_sig[int_sl] + 0.00001 + min_int_sig), 1)\n",
    "\n",
    "# xyzi: ground truth position. N_gt * batch_size * 19. N_gt = maximum number of GT emitters in a batch. \n",
    "\n",
    "xyz_inp = xyzi[xyz_sl].transpose(0, 1)[:,:,None,:]          # reshape for log_prob()\n",
    "log_norm_prob_xyz = dist_normal_xyz.base_dist.log_prob(xyz_inp) # N_gt * batch_size * n_pixel * 3\n",
    "\n",
    "log_cat_prob = torch.log_softmax(mix_logits, -1)        # normalized (sum to 1 over pixels) log probs for the categorical dist. of the GMM. batch_size * n_pixels\n",
    "\n",
    "int_inp = xyzi[int_sl].transpose(0, 1)[:,:,None,:]                # reshape for log_prob()\n",
    "log_norm_prob_int = dist_normal_int.base_dist.log_prob(int_inp)   # N_gt * batch_size * n_pixel * 16\n",
    "\n",
    "if old_loss:\n",
    "    # Original loss computing the likelihood of a GMM with 19 dimensional Gaussians. Gives decent results. \n",
    "    log_norm_prob_xyz = _sum_rightmost(log_norm_prob_xyz, 1)         # N_gt * batch_size * n_pixel\n",
    "    total_prob = torch.logsumexp(log_norm_prob_xyz + log_cat_prob + _sum_rightmost(log_norm_prob_int, 1),-1).transpose(0, 1) # logsumexp over pixels. batch_size * N_gt\n",
    "\n",
    "# else:\n",
    "# #     New loss. The idea is that by zeroing out the log probabilities of the empty channels the models learns to assign zero probability to them (because the probabilties are normed over the channels)\n",
    "# #     This works but the results are much worse. It seems that the chained logsumexp gives shitty gradients.\n",
    "#     log_norm_prob_xyz = _sum_rightmost(log_norm_prob_xyz, 1)         # N_gt * batch_size * n_pixel\n",
    "#     log_mix_prob_int = torch.log_softmax(int_logits, dim=-1)                             # normalized (sum to 1 over channels) log probs.  batch_size * n_pixels * n_channels\n",
    "#     log_norm_prob_int[int_inp.expand(-1,-1,log_norm_prob_int.shape[2],-1) == 0] = -1000  # Only 4 out of 16 channels contain signal. We zero out the remaining entries.\n",
    "#     total_prob_int = torch.logsumexp(log_norm_prob_int + log_mix_prob_int ,-1)           # logsumexp over pixels. N_gt * batch_size * n_pixels\n",
    "#     total_prob = torch.logsumexp(log_norm_prob_xyz + total_prob_int + log_cat_prob,-1).transpose(0, 1)\n",
    "    \n",
    "else:\n",
    "    log_mix_prob_int = torch.log_softmax(int_logits, dim=-1) \n",
    "    log_mix_prob_int = torch.log(torch.sigmoid(int_logits)) \n",
    "    \n",
    "    log_norm_prob_int[int_inp.expand(-1,-1,log_norm_prob_int.shape[2],-1) == 0] = 0\n",
    "    \n",
    "    rep_log_prob = torch.cat([log_norm_prob_xyz[...,None].expand(-1,-1,-1,-1,16), log_norm_prob_int[:,:,:,None]], 3) # batch_size * n_pixels * 4 (xyzi) * n_channels\n",
    "    rep_log_prob = rep_log_prob.sum(3)\n",
    "    \n",
    "#     rep_log_prob[int_inp.expand(-1,-1,log_norm_prob_int.shape[2],-1) == 0] = -1000\n",
    "    total_prob = torch.logsumexp(rep_log_prob + log_cat_prob[...,None].expand(-1,-1,channels) + log_mix_prob_int,2).transpose(0, 1)\n",
    "\n",
    "#     w1_inp = torch.gather(rep_log_prob, -1, int_inp.argsort(-1, descending=True).expand(-1,-1,rep_log_prob.shape[2],-1))[...,:4]\n",
    "\n",
    "    total_prob = torch.logsumexp(rep_log_prob + log_cat_prob[...,None].expand(-1,-1,channels) + log_mix_prob_int,2).transpose(0, 1)\n",
    "#     total_prob = torch.gather(total_prob, -1, int_inp.argsort(-1, descending=True)[:,:,0].transpose(0, 1))[...,:4]\n",
    "    total_prob = total_prob.sum(-1)\n",
    "    \n",
    "total_prob = (total_prob * s_mask).sum(-1)  # s_mask: batch_size * N_gt. Binary mask to remove entries in all samples that have less then N_gt GT emitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.8192,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.7124],\n",
       "          [-1.6630,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.6464],\n",
       "          [-1.6635,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.6536],\n",
       "          ...,\n",
       "          [-1.6408,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.6647],\n",
       "          [-1.6430,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.6665],\n",
       "          [-1.7757,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.7521]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.0556,  0.0000,  0.0000,  ...,  0.0000, -1.9843, -2.1808],\n",
       "          [-2.5055,  0.0000,  0.0000,  ...,  0.0000, -1.8087, -1.9584],\n",
       "          [-2.5084,  0.0000,  0.0000,  ...,  0.0000, -1.7894, -1.9482],\n",
       "          ...,\n",
       "          [-2.3431,  0.0000,  0.0000,  ...,  0.0000, -1.7289, -1.9265],\n",
       "          [-2.3455,  0.0000,  0.0000,  ...,  0.0000, -1.7334, -1.9222],\n",
       "          [-2.4979,  0.0000,  0.0000,  ...,  0.0000, -1.7453, -1.9603]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.8561, -1.7789],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.8317, -1.7779],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.8151, -1.7735],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.8931, -1.8525],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.8995, -1.8501],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.9013, -1.9259]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.7331],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.7137],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.7123],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.7771],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.7757],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.8588]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_norm_prob_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 2304, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_inp.argsort(-1, descending=True).expand(-1,-1,rep_log_prob.shape[2],-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.0000, -204.9517,    0.0000,  -30.4709,  -28.2120,    0.0000,\n",
       "           0.0000], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 1, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_inp.argsort(-1, descending=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 1, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2304, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cat_prob[...,None].expand(-1,-1,channels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7, 2304])\n",
      "torch.Size([7, 2304])\n",
      "torch.Size([3, 7, 2304])\n",
      "torch.Size([3, 7])\n"
     ]
    }
   ],
   "source": [
    "print(log_prob_xyz.shape)\n",
    "print(log_mix_prob_xyz.shape)\n",
    "print((log_prob_xyz + log_mix_prob_xyz).shape)\n",
    "print(torch.logsumexp(log_prob_xyz + log_mix_prob_xyz,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7, 2304, 16])\n",
      "torch.Size([7, 2304, 16])\n",
      "torch.Size([3, 7, 2304, 16])\n",
      "torch.Size([3, 7, 2304])\n"
     ]
    }
   ],
   "source": [
    "print(log_prob_int.shape)\n",
    "print(log_mix_prob_int_x_p.shape)\n",
    "print((log_prob_int + log_mix_prob_int_x_p).shape)\n",
    "print(torch.logsumexp(log_prob_int + log_mix_prob_int_x_p,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-30149.2031, -30149.2031, -30149.2031],\n",
       "        [-36184.4805, -36311.1953, -36365.1406],\n",
       "        [-24744.4707, -24744.4707, -24744.4707],\n",
       "        [-33971.2578, -33420.4922, -33420.4922],\n",
       "        [-34393.4453, -33823.4766, -33823.4766],\n",
       "        [-23325.5508, -23325.5508, -23325.5508],\n",
       "        [-31364.9023, -31364.9023, -31364.9023]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob_int.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob_xyz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3, 2304])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(log_prob_xyz + log_mix_prob_xyz,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2304])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(log_mix_prob_int + log_mix_prob_int_x_p,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob_xyz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, -1.8633,  0.0000,  0.4107,  0.5226,  0.0000,  0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2304, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_mix.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_fish.engine.point_process import PointProcessUniform\n",
    "point_process = PointProcessUniform(local_rate = torch.ones([2,1,48,48,48])*.001, int_conc=1.0, sim_iters=5, channels=16, n_bits=4)\n",
    "locs_3d, x_os_3d, y_os_3d, z_os_3d, ints_3d, output_shape = point_process.sample(phasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1782\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n",
      "tensor([112., 119.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "locs_3d = [l.cuda() for l in locs_3d]\n",
    "# xyzi_true, s_mask = get_true_labels(2, locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda())\n",
    "xyzi_true, s_mask = get_true_labels_mf(2, 4, 16, locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda())\n",
    "print(len(locs_3d[0]))\n",
    "print(s_mask)\n",
    "print(s_mask.sum(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-272.8184, -314.4428], device='cuda:0', grad_fn=<SubBackward0>),\n",
       " tensor([-48894.6641, -45920.2773], device='cuda:0', grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg = PointProcessGaussian(**model_out)\n",
    "gmm_loss = ppg.log_prob(locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda(), n_bits=4, channels=16, min_int_sig=0.1)\n",
    "gmm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([119, 2, 2304, 3])\n",
      "torch.Size([119, 2, 2304, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-272.8184, -314.4428], device='cuda:0', grad_fn=<SubBackward0>),\n",
       " tensor([-48895.4141, -45920.4023], device='cuda:0', grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg = PointProcessGaussian(**model_out)\n",
    "gmm_loss = ppg.log_prob(locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda(), n_bits=4, channels=16, min_int_sig=0.1)\n",
    "gmm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -6410.5063, -12788.6523], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     gmm_loss = PointProcessGaussian(**model_out).log_prob(locs_3d, x_os_3d.cuda(), y_os_3d.cuda(), z_os_3d.cuda(), ints_3d.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_models.ipynb.\n",
      "Converted 01_psf.ipynb.\n",
      "Converted 02_microscope.ipynb.\n",
      "Converted 03_noise.ipynb.\n",
      "Converted 04_pointsource.ipynb.\n",
      "Converted 05_gmm_loss.ipynb.\n",
      "Converted 06_plotting.ipynb.\n",
      "Converted 07_file_io.ipynb.\n",
      "Converted 08_dataset.ipynb.\n",
      "Converted 09_output_trafo.ipynb.\n",
      "Converted 10_evaluation.ipynb.\n",
      "Converted 11_emitter_io.ipynb.\n",
      "Converted 12_utils.ipynb.\n",
      "Converted 13_train.ipynb.\n",
      "Converted 15_fit_psf.ipynb.\n",
      "Converted 16_visualization.ipynb.\n",
      "Converted 17_eval_routines.ipynb.\n",
      "Converted 18_predict_funcs.ipynb.\n",
      "Converted 19_MERFISH_routines.ipynb.\n",
      "Converted 20_MERFISH_visualization.ipynb.\n",
      "Converted 22_MERFISH_codenet.ipynb.\n",
      "Converted 23_MERFISH_comparison.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_fish_dev2]",
   "language": "python",
   "name": "conda-env-decode_fish_dev2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
