# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/13_train.ipynb (unless otherwise specified).

__all__ = ['eval_logger', 'load_from_eval_dict', 'save_train_state', 'train']

# Cell
from ..imports import *
from .evaluation import *
from .file_io import *
from .emitter_io import *
from .utils import *
from .dataset import *
from .output_trafo import *
from .plotting import *
import torch.nn.functional as F
from torch.optim import AdamW
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.data import DataLoader
from ..engine.microscope import Microscope
from ..engine.model import UnetDecodeNoBn
import shutil
from torch.utils.tensorboard import SummaryWriter
from ..engine.point_process import PointProcessUniform
from ..engine.gmm_loss import PointProcessGaussian
import wandb
# from decode_fish.funcs.visualization vimport get_simulation_statistics

# Cell
def eval_logger(writer, pred_df, target_df, iteration, data_str='Sim. '):

    perf_dict,_,shift = matching(target_df, pred_df, print_res=False)
    if 'Inp' in data_str:
        pred_corr = shift_df(pred_df, shift)
        perf_dict, _, _ = matching(target_df, pred_corr, print_res=False)

    writer.add_scalar(data_str +'Metrics/eff_3d', perf_dict['eff_3d'], iteration)
    writer.add_scalar(data_str +'Metrics/jaccard', perf_dict['jaccard'], iteration)
    writer.add_scalar(data_str +'Metrics/rmse_vol', perf_dict['rmse_vol'], iteration)

    writer.add_scalar(data_str +'Metrics/precision', perf_dict['precision'], iteration)
    writer.add_scalar(data_str +'Metrics/recall', perf_dict['recall'], iteration)
    writer.add_scalar(data_str +'Metrics/rmse_x', perf_dict['rmse_x'], iteration)
    writer.add_scalar(data_str +'Metrics/rmse_y', perf_dict['rmse_y'], iteration)
    writer.add_scalar(data_str +'Metrics/rmse_z', perf_dict['rmse_z'], iteration)

def load_from_eval_dict(eval_dict):

    eval_img = load_tiff_image(eval_dict['image_path'])
    eval_img = crop_vol(eval_img, eval_dict['crop_sl'])
    if eval_dict['txt_path'] is not None:
        eval_df = simfish_to_df(eval_dict['txt_path'], px_size=eval_dict['px_size'])
        eval_df = crop_df(eval_df, eval_dict['crop_sl'], px_size=eval_dict['px_size'])
    else:
        eval_df = None
    return eval_img, eval_df

def save_train_state(save_dir, model, microscope, optim_net, psf, optim_psf):

        torch.save({'state_dict':model.state_dict(), 'scaling':[model.unet.inp_scale, model.unet.inp_offset]}, save_dir/'model.pkl')
        torch.save(microscope.state_dict(), save_dir/'microscope.pkl')
        torch.save(optim_net.state_dict(), save_dir/'opt_net.pkl')
        torch.save(psf.state_dict(), save_dir/'psf.pkl' )
        torch.save(optim_psf.state_dict(), save_dir/'opt_psf.pkl')

# Cell
def train(model,
         dl,
         num_iter_sl,
         num_iter_ae,
         optim_net,
         optim_psf,
         sched_net,
         sched_psf,
         microscope,
         psf,
         post_proc,
         min_int,
         log_interval,
         bl_loss_scale = 0.01,
         cnt_loss_scale=1,
         grad_clip=0.01,
         save_dir=None,
         log_dir=None,
         eval_dict=None,
         log_figs=True):

    """
    Training loop for autoencoder learning. Alternates between a simulator training step to train the inference network
    and an autoencoder step to train the PSF (and microscope) parameters.

    Args:
        model (torch.nn.Module): DECODE 3D UNet.
        num_iter_sl (int): Number of training iterations for pure sl learning(batches).
        num_iter_sl (int): Total number of training iterations (batches).
        dl  (torch.utils.data.dataloader.DataLoader): Dataloader that returns a random sub volume from the real volume, an estiamted emitter density and background.
        optim_net  (torch.optim.Optimizer): Optimizer for the network parameters.
        optim_psf  (torch.optim.Optimizer): Optimizer for the PSF parameters.
        sched_net  (torch.optim.lr_scheduler): LR scheduler for the network parameters.
        sched_psf  (torch.optim.lr_scheduler): LR scheduler for the PSF parameters.
        min_int  (float): Minimal fraction of the max intensity used when sampling emitters.
        microscope (torch.nn.Module): Microscope class that transforms emitter locations into simulated images.
        log_interval  (int): Number of iterations between performance evaluations.
        save_dir  (str, PosixPath): Output path where the trained model is stored.
        log_dir  (str, PosixPath, optional): Output path where log files for Tensorboard are stored.
        psf (torch.nn.Module): Parametric PSF.
        bl_loss_scale  (float): The background loss gets scaled by this factor when added to the GMM loss.
        grad_clip  (float): Gradient clipping threshold.
        eval_dict  (dict, optional): Dictionary with evaluation parameters

    """

    save_dir = Path(save_dir)
    if log_dir is not None:
        if os.path.isdir(log_dir):
            shutil.rmtree(log_dir)
    writer = SummaryWriter(log_dir)

    if eval_dict is not None:
        eval_img, eval_df = load_from_eval_dict(eval_dict)

    model.cuda().train()
    torch.save(psf.state_dict(), str(save_dir) + '/psf_init.pkl' )

    for batch_idx in range(num_iter_sl + num_iter_ae):

        x, local_rate, background = next(iter(dl))

        optim_net.zero_grad()

        # Get supervised loss
        sim_vars = PointProcessUniform(local_rate, min_int=min_int, sim_iters=5).sample()  # sim_vars = locs_sl, x_os_sl, y_os_sl, z_os_sl, ints_sl, output_shape
        xsim = microscope(*sim_vars)
        xsim_noise = microscope.noise(xsim, background).sample()

        out_sim = model(xsim_noise)
        count_prob, spatial_prob = PointProcessGaussian(**out_sim).log_prob(*sim_vars[:-1])
        gmm_loss = -(spatial_prob + cnt_loss_scale*count_prob).mean()

        background_loss = F.mse_loss(out_sim['background'], background) * bl_loss_scale

        loss = gmm_loss + background_loss

        # Update network parameters
        loss.backward()

        if grad_clip:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip, norm_type=2)

        optim_net.step()
        if sched_net:
            sched_net.step()

        if batch_idx > num_iter_sl:

            optim_psf.zero_grad()
            # Get autoencoder loss
            out_inp = model(x)
            ae_img = microscope(*post_proc(out_inp, ret='micro'))
            log_p_x_given_z = - microscope.noise(ae_img,out_inp['background']).log_prob(x).mean()

#             Update PSF parameters
            log_p_x_given_z.backward()

            if grad_clip:
                torch.nn.utils.clip_grad_norm_(optim_psf.param_groups[0]['params'], max_norm=grad_clip, norm_type=2)

            optim_psf.step()
            if sched_psf:
                sched_psf.step()

        # Logging
        if writer is not None and batch_idx % 10 == 0:
            writer.add_scalar('SL Losses/gmm_loss', gmm_loss.detach().cpu(), batch_idx)
            writer.add_scalar('SL Losses/count_loss', (-count_prob.mean()).detach().cpu(), batch_idx)
            writer.add_scalar('SL Losses/bg_loss', background_loss.detach().cpu(), batch_idx)
            if batch_idx > num_iter_sl:
                writer.add_scalar('AE Losses/p_x_given_z', log_p_x_given_z.detach().cpu(), batch_idx)
                writer.add_scalar('AE Losses/sum(psf)', psf.psf_volume[0].sum().detach().cpu(), batch_idx)

        if batch_idx % log_interval == 0:
            print(batch_idx)
            if writer is not None:
                with torch.no_grad():
                    pred_df = post_proc(out_sim, ret='df')
                    target_df = sample_to_df(*sim_vars[:-1])
                    eval_logger(writer, pred_df, target_df, batch_idx, data_str='Sim. ')
                    writer.add_scalar('Sim. Metrics/prob_fac', torch.sigmoid(out_sim['logits']).sum().item()/len(target_df), batch_idx)
                    writer.add_scalar('Sim. Metrics/n_em_fac', len(pred_df)/len(target_df), batch_idx)

                    if log_figs:
                        sl_fig = sl_plot(x, xsim_noise, pred_df, target_df, background, out_sim)
                        plt.show()
#                         writer.add_figure('SL summary', sl_fig, batch_idx)

                    if eval_dict is not None:
                        res_eval = model(eval_img[None].cuda())
                        ae_img = microscope(*post_proc(res_eval, ret='micro'))
                        pred_eval_df = post_proc(res_eval, ret='df')
                        if eval_df is not None:
                            eval_logger(writer, pred_eval_df, eval_df, batch_idx, data_str='Inp. ')

                        if log_figs:
                            eval_fig = gt_plot(eval_img, pred_eval_df, eval_df, eval_dict['px_size'],ae_img[0]+res_eval['background'][0])
                            plt.show()
#                             writer.add_figure('GT', eval_fig, batch_idx)

            # storing
            if batch_idx > 0 and abs(num_iter_sl - batch_idx)<log_interval:
                Path.mkdir(save_dir/'sl_save', exist_ok=True)
                save_train_state(save_dir/'sl_save', model, microscope, optim_net, psf, optim_psf)

            save_train_state(save_dir, model, microscope, optim_net, psf, optim_psf)